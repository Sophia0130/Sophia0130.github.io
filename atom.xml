<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>绿小蕤</title>
  <icon>https://www.gravatar.com/avatar/e4d7a8bd1cb84fb3b4123916b4ea2f6b</icon>
  <subtitle>好逸恶劳,贪生怕死</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-06-24T07:58:37.548Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>绿小蕤</name>
    <email>528036346@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业（四）</title>
    <link href="http://yoursite.com/2018/06/24/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/24/Deep-Learning-吴恩达-作业（四）/</id>
    <published>2018-06-24T07:46:03.000Z</published>
    <updated>2018-06-24T07:58:37.548Z</updated>
    
    <content type="html"><![CDATA[<h2 id="initialization"><a href="#initialization" class="headerlink" title="initialization"></a>initialization</h2><p>三种不同的参数W、b的初始化方式，对于多次迭代后cost function下降效果的影响</p><a id="more"></a><p><img src="http://p8ge6t5tt.bkt.clouddn.com/initization%E4%BD%9C%E4%B8%9A%E5%9B%9B.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;initialization&quot;&gt;&lt;a href=&quot;#initialization&quot; class=&quot;headerlink&quot; title=&quot;initialization&quot;&gt;&lt;/a&gt;initialization&lt;/h2&gt;&lt;p&gt;三种不同的参数W、b的初始化方式，对于多次迭代后cost function下降效果的影响&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>深度学习和强化学习</title>
    <link href="http://yoursite.com/2018/06/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/06/21/深度学习和强化学习/</id>
    <published>2018-06-21T07:06:39.000Z</published>
    <updated>2018-06-21T07:07:12.516Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络是个模型，深度学习是一种方法。深度学习可以理解成用深度神经网络来进行机器学习，可以有效解决层数多的网络不好学习的问题。</p><a id="more"></a><h2 id="深度学习（Deep-Learning）"><a href="#深度学习（Deep-Learning）" class="headerlink" title="深度学习（Deep Learning）"></a>深度学习（Deep Learning）</h2><p><strong>DNN （深度神经网络）</strong>将原始信号（例如RGB像素值）<strong>直接</strong>输入DNN，而不需要创建任何域特定的输入功能。通过多层神经元，DNN可以“自动”通过每一层产生适当的特征，最后提供一个非常好的预测。这极大地消除了寻找”特征工程”的麻烦。</p><p>DNN也演变成许多不同的网络拓扑结构，CNN（卷积神经网络），RNN（递归神经网络），LSTM（长期短期记忆），GAN（生成敌对网络），转移学习（Transfer Learning），注意模型（attention model）。</p><p>深度学习模型：<a href="https://www.zhihu.com/question/38679133" target="_blank" rel="noopener">https://www.zhihu.com/question/38679133</a></p><h2 id="强化学习（Reinforcement-Learning）"><a href="#强化学习（Reinforcement-Learning）" class="headerlink" title="强化学习（Reinforcement Learning）"></a>强化学习（Reinforcement Learning）</h2><p>强化学习有四个要素，环境模型、Agent（学习者）、回报函数、策略。其实就是一种<strong>奖惩机制</strong>，奖励那些适应性好的学习，惩罚那些适应性不好的学习。强化学习学习得非常快，因为每一个新的反馈都会影响随后的决定。</p><h2 id="深度学习与强化学习的区别"><a href="#深度学习与强化学习的区别" class="headerlink" title="深度学习与强化学习的区别"></a>深度学习与强化学习的区别</h2><p>深度学习的学习过程是静态的，强化学习的学习过程是动态的。静态与动态的区别在于是否会与环境进行交互，深度学习是给什么样本就学什么，而强化学习是要和环境进行交互，再通过环境给出的奖惩来学习。<strong>深度学习解决的更多是感知问题，强化学习解决的主要是决策问题</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;神经网络&quot;&gt;&lt;a href=&quot;#神经网络&quot; class=&quot;headerlink&quot; title=&quot;神经网络&quot;&gt;&lt;/a&gt;神经网络&lt;/h2&gt;&lt;p&gt;神经网络是个模型，深度学习是一种方法。深度学习可以理解成用深度神经网络来进行机器学习，可以有效解决层数多的网络不好学习的问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业（三）</title>
    <link href="http://yoursite.com/2018/06/20/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/20/Deep-Learning-吴恩达-作业（三）/</id>
    <published>2018-06-20T12:54:48.000Z</published>
    <updated>2018-06-21T13:55:21.231Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hdf5文件"><a href="#hdf5文件" class="headerlink" title="hdf5文件"></a>hdf5文件</h2><p>PS：第一次接触这种文件格式，可以参考下面两篇博文快速知道其存储形式 ~</p><p><a href="https://blog.csdn.net/yudf2010/article/details/50353292" target="_blank" rel="noopener">https://blog.csdn.net/yudf2010/article/details/50353292</a></p><p><a href="http://docs.h5py.org/en/latest/quick.html#quick" target="_blank" rel="noopener">http://docs.h5py.org/en/latest/quick.html#quick</a></p><p>h5py文件是存放两类对象的容器</p><ul><li>数据集(dataset)：像数组类的数据集合，和numpy的数组差不多。</li></ul><ul><li>组(group)：像文件夹一样的容器，类似python中的 dict，有键(key)和值(value)。group中可以存放dataset或者其他的group。</li></ul><a id="more"></a><h3 id="1-查看h5文件中的key"><a href="#1-查看h5文件中的key" class="headerlink" title="1. 查看h5文件中的key"></a>1. 查看h5文件中的key</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#h5文件的读取  </span></span><br><span class="line">f_train = h5py.File(<span class="string">'D:/机器学习/深度学习神经网络/作业三/datasets/train_catvnoncat.h5'</span>,<span class="string">'r'</span>)   <span class="comment">#打开h5文件  </span></span><br><span class="line"><span class="comment"># 可以查看所有的主键  </span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> f_train.keys():  </span><br><span class="line">    print(f_train[key].name)   <span class="comment">#名字</span></span><br><span class="line">    print(f_train[key].shape)  <span class="comment">#大小</span></span><br><span class="line">    print(f_train[key].value)  <span class="comment">#数据</span></span><br></pre></td></tr></table></figure><h3 id="2-key的打印结果"><a href="#2-key的打印结果" class="headerlink" title="2. key的打印结果"></a>2. key的打印结果</h3><p>train_catvnoncat.h5 和 test_catvnoncat.h5 中key的打印结果一样，train中样本209，test中样本50</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">/list_classes</span><br><span class="line">(2,)</span><br><span class="line">[b&apos;non-cat&apos; b&apos;cat&apos;]</span><br><span class="line">/train_set_x</span><br><span class="line">(209, 64, 64, 3)</span><br><span class="line">[[[[ 17  31  56]</span><br><span class="line">   [ 22  33  59]</span><br><span class="line">   [ 25  35  62]</span><br><span class="line">   ...</span><br><span class="line">   [  0   0   0]</span><br><span class="line">   [  0   0   0]</span><br><span class="line">   [  0   0   0]]]]</span><br><span class="line">/train_set_y</span><br><span class="line">(209,)</span><br><span class="line">[0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0</span><br><span class="line"> 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1</span><br><span class="line"> 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0</span><br><span class="line"> 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0</span><br><span class="line"> 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]</span><br></pre></td></tr></table></figure><h3 id="3-查看key的数据类型"><a href="#3-查看key的数据类型" class="headerlink" title="3. 查看key的数据类型"></a>3. 查看key的数据类型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f_train = h5py.File(<span class="string">'D:/机器学习/深度学习神经网络/作业三/datasets/train_catvnoncat.h5'</span>,<span class="string">'r'</span>)   </span><br><span class="line">train_x_orig = f_train[<span class="string">'train_set_x'</span>]</span><br><span class="line">train_y = f_train[<span class="string">'train_set_y'</span>]</span><br><span class="line">classes = f_train[<span class="string">'list_classes'</span>]</span><br><span class="line">print(train_x_orig.dtype)</span><br><span class="line">print(train_y.dtype)</span><br></pre></td></tr></table></figure><ul><li>list_classes ：uint8</li><li>train_set_x ：int64 应该是64* 64* 3大小的图片</li><li>train_set_y ：|S7 好像是Matlab里的v7.3格式，不知道啊 ~</li></ul><p>PS：下面是两个两个编程作业~</p><h2 id="两层的神经网络"><a href="#两层的神经网络" class="headerlink" title="两层的神经网络"></a>两层的神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###########两层的神经网络 LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID 识别是否有猫</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment">###########加载数据</span></span><br><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data() <span class="comment">#dnn_app_utils中的函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将三个通道合为一列，标准化至0~1</span></span><br><span class="line"><span class="comment"># Reshape the training and test examples</span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T  <span class="comment"># The "-1" makes reshape flatten the remaining dimensions </span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x's shape: "</span> + str(train_x.shape)) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x's shape: "</span> + str(test_x.shape))</span><br><span class="line"><span class="comment">###########加载数据</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###########两层的神经网络参数训练 LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID</span></span><br><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># num_px * num_px * 3</span></span><br><span class="line">n_h = <span class="number">7</span>         <span class="comment"># 隐藏层神经元个数</span></span><br><span class="line">n_y = <span class="number">1</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- If set to True, this will print the cost every 100 iterations </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary containing W1, W2, b1, and b2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                              <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                           <span class="comment"># number of examples</span></span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary, by calling one of the functions you'd previously implemented</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y) <span class="comment">#初始化参数</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get W1, b1, W2 and b2 from the dictionary parameters.</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: "X, W1, b1, W2, b2". Output: "A1, cache1, A2, cache2".</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, <span class="string">'relu'</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, <span class="string">'sigmoid'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initializing backward propagation</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Backward propagation. Inputs: "dA2, cache2, cache1". Outputs: "dA1, dW2, db2; also dA0 (not used), dW1, db1".</span></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, <span class="string">'sigmoid'</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, <span class="string">'relu'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2</span></span><br><span class="line">        grads[<span class="string">'dW1'</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve W1, b1, W2, b2 from parameters</span></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>: <span class="comment">#每迭代100次记录cost的值</span></span><br><span class="line">            costs.append(cost)</span><br><span class="line">       </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"><span class="comment">###########两层的神经网络</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算样本准确率</span></span><br><span class="line">predictions_train = predict(train_x, train_y, parameters) <span class="comment">#训练集准确率为1</span></span><br><span class="line">predictions_test = predict(test_x, test_y, parameters) <span class="comment">#测试集准确率为0.72</span></span><br></pre></td></tr></table></figure><p>训练过程中cost function的值的下降</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn3.JPG" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn4.JPG" alt=""></p><h2 id="四层的神经网络"><a href="#四层的神经网络" class="headerlink" title="四层的神经网络"></a>四层的神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###########四层的神经网络 LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID 识别是否有猫</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment">###########加载数据</span></span><br><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data() <span class="comment">#dnn_app_utils中的函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将三个通道合为一列，标准化至0~1</span></span><br><span class="line"><span class="comment"># Reshape the training and test examples</span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T  <span class="comment"># The "-1" makes reshape flatten the remaining dimensions </span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span></span><br><span class="line"><span class="comment">#print ("train_x's shape: " + str(train_x.shape)) </span></span><br><span class="line"><span class="comment">#print ("test_x's shape: " + str(test_x.shape))</span></span><br><span class="line"><span class="comment">###########加载数据</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###########四层神经网络参数训练，每层的神经元个数 LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID</span></span><br><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>] <span class="comment">#  4-layer model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span><span class="comment">#lr was 0.009</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    costs = []                         <span class="comment"># keep track of cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Parameters initialization. (≈ 1 line of code)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        AL, caches =  L_model_forward(X, parameters)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cost.</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line"></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">                </span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show() <span class="comment">#这里要注意的一个问题是：程序会一直show这张图而不往下执行</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"><span class="comment">###########四层神经网络</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算样本准确率</span></span><br><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="keyword">True</span>)</span><br><span class="line">predictions_train = predict(train_x, train_y, parameters) <span class="comment">#训练集准确率为0.98</span></span><br><span class="line">predictions_test = predict(test_x, test_y, parameters) <span class="comment">#测试集准确率为0.8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#打印分类错误的图片</span></span><br><span class="line">print_mislabeled_images(classes, test_x, test_y, pred_test)</span><br><span class="line"></span><br><span class="line"><span class="string">''' 测试自己的图片</span></span><br><span class="line"><span class="string">my_image = "cat1.jpg" </span></span><br><span class="line"><span class="string">my_label_y = [1] # the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">fname = "images/" + my_image</span></span><br><span class="line"><span class="string">image = np.array(ndimage.imread(fname, flatten=False))</span></span><br><span class="line"><span class="string">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))</span></span><br><span class="line"><span class="string">my_image = my_image/255.</span></span><br><span class="line"><span class="string">my_predicted_image = predict(my_image, my_label_y, parameters)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">plt.imshow(image)</span></span><br><span class="line"><span class="string">print ("y = " + str(np.squeeze(my_predicted_image)) + ", your L-layer model predicts a \""</span></span><br><span class="line"><span class="string">       + classes[int(np.squeeze(my_predicted_image)),].decode("utf-8") +  "\" picture.")</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>训练过程中cost function的值的下降</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn5.JPG" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn6.JPG" alt=""></p><p>看了一下资源消耗，用的是CPU和内存，可能用不同的框架，用的资源不一样。可以将数据从硬盘读进内存，输入CPU计算。或者将数据从硬盘读入显存，用GPU计算。</p><h2 id="待解决的问题"><a href="#待解决的问题" class="headerlink" title="待解决的问题"></a>待解决的问题</h2><p>我现在最大的疑惑就是，怎么把训练好的网络参数储存起来，以什么格式储存？后面要检测图片中是否有猫的时候，直接将参数取来用，不需要从头再计算一遍参数。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;hdf5文件&quot;&gt;&lt;a href=&quot;#hdf5文件&quot; class=&quot;headerlink&quot; title=&quot;hdf5文件&quot;&gt;&lt;/a&gt;hdf5文件&lt;/h2&gt;&lt;p&gt;PS：第一次接触这种文件格式，可以参考下面两篇博文快速知道其存储形式 ~&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yudf2010/article/details/50353292&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/yudf2010/article/details/50353292&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://docs.h5py.org/en/latest/quick.html#quick&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://docs.h5py.org/en/latest/quick.html#quick&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;h5py文件是存放两类对象的容器&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据集(dataset)：像数组类的数据集合，和numpy的数组差不多。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;组(group)：像文件夹一样的容器，类似python中的 dict，有键(key)和值(value)。group中可以存放dataset或者其他的group。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning 吴恩达 作业（二）</title>
    <link href="http://yoursite.com/2018/06/20/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/20/Deep-Learning-吴恩达-作业（二）/</id>
    <published>2018-06-20T12:01:53.000Z</published>
    <updated>2018-06-20T12:55:40.497Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Building-your-Deep-Neural-Network-Step-by-Step"><a href="#Building-your-Deep-Neural-Network-Step-by-Step" class="headerlink" title="Building your Deep Neural Network: Step by Step"></a>Building your Deep Neural Network: Step by Step</h2><p>L层神经网络：前L-1层的激励函数是ReLU，输出层激励函数是sigmod</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Building your Deep Neural Network: Step by Step</span></span><br><span class="line"><span class="comment">#前L-1层的激励函数是ReLU,输出层激励函数是sigmod</span></span><br><span class="line"><span class="comment">#参数存储需要看清楚</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v4 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化每一层的参数 W、b</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network 每一层神经元的个数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parameters = &#123;&#125;                <span class="comment"># dict类型数据</span></span><br><span class="line">    L = len(layer_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):          <span class="comment"># 0就是输入层</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l<span class="number">-1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 两种激励函数,计算当前层的A,返回cache：A_prev, W, b, Z</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span><br><span class="line"><span class="string">             stored for computing the backward pass efficiently    </span></span><br><span class="line"><span class="string">             dict数据类型</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        <span class="comment">##linear_cache (A_prev, W, b)</span></span><br><span class="line">        <span class="comment">##activation_cache Z</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        Z, linear_cache = np.dot(W,A_prev)+b,(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        Z, linear_cache = np.dot(W,A_prev)+b,(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># L层的前向传播 输出AL(y-hat) caches(A_prev、W、b、Z)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- last post-activation value   最后输出的y-hat </span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []     <span class="comment">#存储A_prev、W、b、Z</span></span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="number">2</span>                  <span class="comment"># number of layers in the neural network //表示整数除</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU]*(L-1). Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L): <span class="comment">#for执行1~L-1</span></span><br><span class="line">        A_prev = A </span><br><span class="line">        A, cache = linear_activation_forward(A,  parameters[<span class="string">'W'</span> + str(l)],  parameters[<span class="string">'b'</span> + str(l)], <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement LINEAR -&gt; SIGMOID. Add "cache" to the "caches" list.</span></span><br><span class="line">    AL, cache = linear_activation_forward(A,  parameters[<span class="string">'W'</span> + str(L)],  parameters[<span class="string">'b'</span> + str(L)], <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># cost function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function defined by equation (7).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from aL and y.</span></span><br><span class="line">    cost = -np.sum(np.multiply(Y,np.log(AL))+ np.multiply(<span class="number">1</span>-Y,np.log(<span class="number">1</span>-AL)))/m</span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据dZ 计算 dA_prev, dW, db</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dW = np.dot(dZ,A_prev.T)/m</span><br><span class="line">    db = np.sum(dZ,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)/m <span class="comment">#同一行的相加 </span></span><br><span class="line">    dA_prev = np.dot(W.T,dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据dA计算dZ,dA_prev, dW, db</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></span><br><span class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    <span class="comment">#linear_cache：A_prev, W, b</span></span><br><span class="line">    <span class="comment">#activation_cache：Z</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#dZ=dA*g`(Z) 不同激励函数的导数不同</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache) <span class="comment">#dZ</span></span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache) <span class="comment">#根据dZ计算参数梯度 dA_prev, dW, db</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># L层反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ... </span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>] <span class="comment">#样本个数</span></span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment">#初始化起始dAL,第L层的dA</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "dAL, current_cache". Outputs: "grads["dAL-1"], grads["dWL"], grads["dbL"]</span></span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]  <span class="comment">#第l层的 A_prev，W，b，Z 放在caches[l-1]</span></span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class="string">"sigmoid"</span>) <span class="comment">#计算L层的参数,其对应的激励函数sigmoid</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop from l=L-2 to l=0</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)): </span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class="line">        <span class="comment"># Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l + <span class="number">2</span>)], current_cache, activation = <span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter. Use a for loop.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)]- learning_rate*grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)]- learning_rate*grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>PS：我做作业的时候，遇到最大的麻烦就是几个参数的上标，还有对应那一层的 cache</p><h3 id="1-下面是我整理的，程序对应的框图："><a href="#1-下面是我整理的，程序对应的框图：" class="headerlink" title="1.下面是我整理的，程序对应的框图："></a>1.下面是我整理的，程序对应的框图：</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn1.png" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn2.png" alt=""></p><h3 id="2-参数整理"><a href="#2-参数整理" class="headerlink" title="2.参数整理"></a>2.参数整理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">parameters  字典类型</span><br><span class="line">parameters[&apos;W&apos; + str(l)]</span><br><span class="line">parameters[&apos;b&apos; + str(l)]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">grads　字典类型 </span><br><span class="line">grads[&quot;dA&quot; + str(l)]  前一层的dA_prev</span><br><span class="line">grads[&quot;dW&quot; + str(l)] </span><br><span class="line">grads[&quot;db&quot; + str(l)]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cache 存储当前层的 A_prev，W，b，Z </span><br><span class="line">linear_cache ：A_prev, W, b</span><br><span class="line">activation_cache： Z</span><br><span class="line">cache = (linear_cache, activation_cache)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">caches 存储所有的 A_prev, W, b, Z (caches[l]对应l+1层)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Building-your-Deep-Neural-Network-Step-by-Step&quot;&gt;&lt;a href=&quot;#Building-your-Deep-Neural-Network-Step-by-Step&quot; class=&quot;headerlink&quot; title=&quot;Building your Deep Neural Network: Step by Step&quot;&gt;&lt;/a&gt;Building your Deep Neural Network: Step by Step&lt;/h2&gt;&lt;p&gt;L层神经网络：前L-1层的激励函数是ReLU，输出层激励函数是sigmod&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习，python" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%8Cpython/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning 吴恩达 作业（一）</title>
    <link href="http://yoursite.com/2018/06/15/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/15/Deep-Learning-吴恩达-作业（一）/</id>
    <published>2018-06-15T06:11:03.000Z</published>
    <updated>2018-06-20T12:55:50.752Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Planar-data-classification-with-one-hidden-layer"><a href="#Planar-data-classification-with-one-hidden-layer" class="headerlink" title="Planar data classification with one hidden layer"></a>Planar data classification with one hidden layer</h2><p>不废话直接上代码，看神人的代码真的是惊叹！！！膜拜、模仿   不想多说，看代码</p><a id="more"></a><p>单隐层神经网络：输入用tanh激励函数，输出用sigmoid激励函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="comment"># import sklearn.linear_model</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span>  sigmoid, load_planar_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载随机的平面点，画出样本的散点图</span></span><br><span class="line">X, Y = load_planar_dataset()</span><br><span class="line"><span class="string">''' # 画出样本的散点图  作业上画图的函数我这边都会报错</span></span><br><span class="line"><span class="string">blue=['b']*200</span></span><br><span class="line"><span class="string">color=['r']*200</span></span><br><span class="line"><span class="string">color.extend(blue) #append添加单个元素 extend添加多个元素</span></span><br><span class="line"><span class="string">plt.scatter(X[0, :], X[1, :], c=color, s=40) #scatter散点图</span></span><br><span class="line"><span class="string">plt.show()</span></span><br><span class="line"><span class="string">print(np.shape(X))</span></span><br><span class="line"><span class="string">print(np.shape(Y))</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">#1. 逻辑回归</span></span><br><span class="line"><span class="string">lg = sklearn.linear_model.LogisticRegression()</span></span><br><span class="line"><span class="string">lg.fit(X.T, Y.ravel().T)  #训练 #.ravel()将多维数组降位一维</span></span><br><span class="line"><span class="string">LG_predictions = lg.predict(X.T) #将训练数据作为测试数据放入预测结果，与正确标签比对</span></span><br><span class="line"><span class="string">print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LG_predictions) + np.dot(1-Y,1-LG_predictions))/float(Y.size)*100) +'% ') #47%</span></span><br><span class="line"><span class="string">plot_decision_boundary(lambda x: lg.predict(x), X, Y)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#2. 贝叶斯岭回归</span></span><br><span class="line"><span class="string">br = sklearn.linear_model.BayesianRidge()</span></span><br><span class="line"><span class="string">br.fit(X.T, Y.ravel().T) </span></span><br><span class="line"><span class="string">BR_predictions = br.predict(X.T)</span></span><br><span class="line"><span class="string">print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,BR_predictions) + np.dot(1-Y,1-BR_predictions))/float(Y.size)*100) + '% ') #51%</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3. deeplearning</span></span><br><span class="line"><span class="comment"># Defining the neural network structure</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    n_x -- the size of the input layer   输入层</span></span><br><span class="line"><span class="string">    n_h -- the size of the hidden layer  隐藏层</span></span><br><span class="line"><span class="string">    n_y -- the size of the output layer  输出层</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer 输入样本的特征个数</span></span><br><span class="line">    n_h = <span class="number">4</span>          <span class="comment"># 隐藏层的节点个数</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer 输出结果</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the model's parameters 初始化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># np.random.seed(2) </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span>  </span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h,n_x))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y,n_h))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Loop 前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m) X经过转置</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters (output of initialization function)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    Z1 = np.dot(W1,X)+b1    <span class="comment">#.dot矩阵乘法 .multiply对应元素相乘</span></span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2,A1)+b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算cost function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost given in equation (13)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost given equation (13)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    logprobs = np.multiply(Y,np.log(A2))+np.multiply(<span class="number">1</span>-Y,np.log(<span class="number">1</span>-A2))  <span class="comment">#一个样本的loss function</span></span><br><span class="line">    cost = -np.sum(logprobs)/m  <span class="comment">#所有样本的cost function</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect.  #将维度为1的去掉 </span></span><br><span class="line">                                <span class="comment"># E.g., turns [[17]] into 17 </span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float)) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播 计算参数梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation using the instructions above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing our parameters </span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".</span></span><br><span class="line"><span class="string">    X -- input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary "parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary "cache".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span></span><br><span class="line">    dZ2 = A2-Y</span><br><span class="line">    dW2 = np.dot(dZ2,A1.T)/m</span><br><span class="line">    db2 = np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)/m   <span class="comment">#按行相加，并且保持其二维特性</span></span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T,dZ2),<span class="number">1</span>-pow(A1,<span class="number">2</span>)) </span><br><span class="line">    dW1 = np.dot(dZ1,X.T)/m</span><br><span class="line">    db1 = np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)/m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新参数 进行新一轮正向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Updates parameters using the gradient descent update rule given above</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary "grads"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line">    <span class="comment">## END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = W1-learning_rate*dW1</span><br><span class="line">    b1 = b1-learning_rate*db1</span><br><span class="line">    W2 = W2-learning_rate*dW2</span><br><span class="line">    b2 = b2-learning_rate*db2</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 组合前面的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    num_iterations -- Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    !--用到的变量--!</span></span><br><span class="line"><span class="string">    待求参数parameters = &#123;"W1": W1,"b1": b1, "W2": W2, "b2": b2&#125; 先初始化，然后更新</span></span><br><span class="line"><span class="string">    中间变量cache = &#123;"Z1": Z1,"A1": A1, "Z2": Z2,"A2": A2&#125;</span></span><br><span class="line"><span class="string">    代价函数cost</span></span><br><span class="line"><span class="string">    梯度grads = &#123;"dW1": dW1, "db1": db1,"dW2": dW2, "db2": db2&#125; </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#np.random.seed(3)</span></span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>] <span class="comment">#一个样本 输入特征个数</span></span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>] <span class="comment">#一个样本 输出结果</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 5 lines of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y) <span class="comment">#初始化参数</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations): <span class="comment">#梯度下降迭代次数</span></span><br><span class="line">         </span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="comment"># Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost)) <span class="comment">#每迭代1000次 输出cost function</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = np.round(A2) <span class="comment">#四舍五入</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a model with a n_h-dimensional hidden layer</span></span><br><span class="line">n_h = <span class="number">1</span></span><br><span class="line">parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">10000</span>)</span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line">Acc=float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'隐藏层节点：%d'</span>%n_h+<span class="string">'  '</span>+<span class="string">'准确率： %0.3f'</span> %Acc+<span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><p>改变隐藏层的节点数，得到的结果</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dl%E4%BD%9C%E4%B8%9A3.JPG" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dl%E4%BD%9C%E4%B8%9A1.JPG" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dl%E4%BD%9C%E4%B8%9A2.JPG" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Planar-data-classification-with-one-hidden-layer&quot;&gt;&lt;a href=&quot;#Planar-data-classification-with-one-hidden-layer&quot; class=&quot;headerlink&quot; title=&quot;Planar data classification with one hidden layer&quot;&gt;&lt;/a&gt;Planar data classification with one hidden layer&lt;/h2&gt;&lt;p&gt;不废话直接上代码，看神人的代码真的是惊叹！！！膜拜、模仿   不想多说，看代码&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习，python" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%8Cpython/"/>
    
  </entry>
  
  <entry>
    <title>anaconda，conda，pip的关系</title>
    <link href="http://yoursite.com/2018/06/13/anaconda%EF%BC%8Cconda%EF%BC%8Cpip%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
    <id>http://yoursite.com/2018/06/13/anaconda，conda，pip的关系/</id>
    <published>2018-06-13T01:31:32.000Z</published>
    <updated>2018-06-13T01:38:12.210Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Anaconda：是一个<strong>python发行版</strong>。软件发行版是在系统上提前编译和配置好的软件包集合， 装好了后就可以直接用。因为包含了大量的科学包，Anaconda 的下载文件比较大（约 515 MB），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用Miniconda这个较小的发行版(仅包含conda和 Python)​</li><li>Conda：是一个通用的包管理器，当初设计来<strong>管理任何语言的包</strong></li><li>pip ：python包的软件包管理器</li></ul><p>Conda 和 pip的区别：pip可以允许你在<strong>任何环境</strong>中安装python包，而conda允许你在<strong>conda环境中安装任何语言包</strong>（包括c语言或者python）</p><p>PS；包管理器是自动化软件安装，更新，卸载的一种工具</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Anaconda：是一个&lt;strong&gt;python发行版&lt;/strong&gt;。软件发行版是在系统上提前编译和配置好的软件包集合， 装好了后就可以直接用。因为包含了大量的科学包，Anaconda 的下载文件比较大（约 515 MB），如果只需要某些包，或者需要节省
      
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="http://yoursite.com/2018/06/11/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>http://yoursite.com/2018/06/11/正则表达式/</id>
    <published>2018-06-11T08:31:41.000Z</published>
    <updated>2018-06-11T08:35:37.805Z</updated>
    
    <content type="html"><![CDATA[<p>PS：暂时没有深入学习的需求，但是老是看到，所有就想了解一下。<strong>正则表达式 (regular expression) 描述了一种字符串匹配的模式(pattern)</strong>。</p><p><a href="http://deerchao.net/tutorials/regex/regex.htm#mission" target="_blank" rel="noopener">http://deerchao.net/tutorials/regex/regex.htm#mission</a></p><p>字符可能是字母，数字，标点符号，空格，换行符，汉字等等。</p><p>字符串是0个或更多个字符的序列。文本也就是字符串。</p><p><strong>字符串匹配正则表达式</strong>，是指这个字符串里有一部分（或几部分）<strong>能满足表达式给出的条件</strong>。</p><p><strong>正则表达式就是记录文本规则的代码，用来进行文本匹配的工具。</strong></p><p>（比如，你可以编写一个正则表达式，用来查找所有以0开头，后面跟着2-3个数字，然后是一个连字号“-”，最后是7或8位数字的字符串像 010-12345678或0376-7654321)。</p><p>re 模块使 Python 语言拥有全部的正则表达式功能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;PS：暂时没有深入学习的需求，但是老是看到，所有就想了解一下。&lt;strong&gt;正则表达式 (regular expression) 描述了一种字符串匹配的模式(pattern)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://deerchao.net/tu
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2018/06/11/Titanic-Machine-Learning-from-Disaster/"/>
    <id>http://yoursite.com/2018/06/11/Titanic-Machine-Learning-from-Disaster/</id>
    <published>2018-06-11T06:36:57.500Z</published>
    <updated>2018-06-20T12:19:49.572Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>opencv Haar特征+Adaboost算法 物体检测</title>
    <link href="http://yoursite.com/2018/06/07/opencv-Haar%E7%89%B9%E5%BE%81-Adaboost%E7%AE%97%E6%B3%95-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2018/06/07/opencv-Haar特征-Adaboost算法-物体检测/</id>
    <published>2018-06-07T09:45:56.000Z</published>
    <updated>2018-06-07T09:48:26.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Haar"><a href="#一、Haar" class="headerlink" title="一、Haar"></a>一、Haar</h2><ol><li>什么是Haar特征</li><li>如何计算Haar特征</li><li>积分图快速计算Haar特征</li><li>Haar特征值标准化</li></ol><a id="more"></a><h2 id="二、级联分类器"><a href="#二、级联分类器" class="headerlink" title="二、级联分类器"></a>二、级联分类器</h2><ol><li>级联分类器结构：弱分类器线性组合成为强分类器，强分类器用决策树的形式级联</li><li>存储分类器的XML中的参数</li></ol><p>强弱分类器<img src="http://p8ge6t5tt.bkt.clouddn.com/xml1.JPG" alt=""></p><p>Haar特征<img src="http://p8ge6t5tt.bkt.clouddn.com/xml2.JPG" alt=""></p><h2 id="三、利用并查集合并窗口和丢弃零散分布的误检窗口"><a href="#三、利用并查集合并窗口和丢弃零散分布的误检窗口" class="headerlink" title="三、利用并查集合并窗口和丢弃零散分布的误检窗口"></a>三、利用并查集合并窗口和丢弃零散分布的误检窗口</h2><h2 id="四、AdaBoost-训练算法"><a href="#四、AdaBoost-训练算法" class="headerlink" title="四、AdaBoost 训练算法"></a>四、AdaBoost 训练算法</h2><ol><li><p>查准率 Precision查全率 Recall，precision和recall都越高越好</p></li><li><p>命中率 hitRate，度量检测器对正样本的通过能力，越接近1越好  </p><p>虚警率 falseAlarm，度量检测器对负样本的通过能力，越接近0越好</p></li></ol><h3 id="AdaBoost-步骤"><a href="#AdaBoost-步骤" class="headerlink" title="AdaBoost 步骤"></a>AdaBoost 步骤</h3><ol><li>寻找TP和FP作为训练样本</li><li>计算每个Haar特征在当前权重下的Best split threshold+leftvalue+rightvalue，组成了一个个弱分类器</li><li>通过WSE寻找最优的弱分类器</li><li>更新权重</li><li>按照minHitRate估计stageThreshold</li><li>重复上述1-5步骤，直到falseAlarmRate到达要求，或弱分类器数量足够，停止循环，输出stage</li><li>进入下一个stage训练</li></ol><h2 id="四、opencv-训练级联分类器"><a href="#四、opencv-训练级联分类器" class="headerlink" title="四、opencv 训练级联分类器"></a>四、opencv 训练级联分类器</h2><ol><li>收集正、负样本，正样本固定大小、负样本任意大小</li><li>生成正、负样本描述文件 .txt</li><li>对正样品归一化处理，负样本不处理</li><li>生成正样本特征文件 .vec</li><li>样本训练级联分类器</li></ol><h2 id="五、HAAR与LBP区别"><a href="#五、HAAR与LBP区别" class="headerlink" title="五、HAAR与LBP区别"></a>五、HAAR与LBP区别</h2><ol><li>HAAR特征是浮点数计算，LBP特征是整数计算；</li><li>LBP训练需要的样本数量比HAAR大 </li><li>LBP的速度一般比HAAR快 </li><li>同样的样本HAAR训练出来的检测结果要比LBP准确</li><li>扩大LBP的样本数据可达到HAAR的训练效果</li></ol><h2 id="六、opencv-提供的模型和工具"><a href="#六、opencv-提供的模型和工具" class="headerlink" title="六、opencv 提供的模型和工具"></a>六、opencv 提供的模型和工具</h2><p>1.opencv 自带的检测模型</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/opencv%E8%87%AA%E5%B8%A6%E7%9A%84xml.JPG" alt="img"></p><p>2.opencv 自带的AdaBoost分类器</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/opencvtrain.JPG" alt="img"></p><p>opencv_annotation        用来在一张大图中标定一个或多个需要检测的目标<br>opencv_createsamples  用来制作positive sample的vec<br>opencv_traincascade     用来训练得到需要的cascade.xml</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、Haar&quot;&gt;&lt;a href=&quot;#一、Haar&quot; class=&quot;headerlink&quot; title=&quot;一、Haar&quot;&gt;&lt;/a&gt;一、Haar&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;什么是Haar特征&lt;/li&gt;
&lt;li&gt;如何计算Haar特征&lt;/li&gt;
&lt;li&gt;积分图快速计算Haar特征&lt;/li&gt;
&lt;li&gt;Haar特征值标准化&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
      <category term="c++" scheme="http://yoursite.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法回顾目录</title>
    <link href="http://yoursite.com/2018/06/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%9B%9E%E9%A1%BE%E7%9B%AE%E5%BD%95/"/>
    <id>http://yoursite.com/2018/06/05/机器学习算法回顾目录/</id>
    <published>2018-06-05T07:32:56.000Z</published>
    <updated>2018-06-05T07:33:50.112Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、回归算法"><a href="#一、回归算法" class="headerlink" title="一、回归算法"></a>一、回归算法</h2><ol><li>线性回归：预测连续值</li><li>逻辑回归：预测离散值或分类</li></ol><h2 id="二、决策树-Decision-Tree-DT"><a href="#二、决策树-Decision-Tree-DT" class="headerlink" title="二、决策树 Decision Tree (DT)"></a>二、决策树 Decision Tree (DT)</h2><p>根据一个准则，选取最优划分属性 (最优划分属性：决策树的分支节点的类别尽可能纯，即尽可能属于同一类别)</p><ol><li>ID3：根据信息增益选取最优划分属性</li><li>C4.5：根据信息增益和增益率选取最优划分属性</li><li>CART (classification and regression tree) ：根据基尼指数选取最优划分属性</li></ol><h2 id="三、集成学习-ensemble-learning"><a href="#三、集成学习-ensemble-learning" class="headerlink" title="三、集成学习 (ensemble learning)"></a>三、集成学习 (ensemble learning)</h2><ol><li>Boosting：个体学习器间有强依赖关系，必须串行生成的序列化方法 (训练集基于上一轮结果进行调整)</li><li>Bagging：个体学习器间没有强依赖关系，可同时生成的并行化方法 (训练集互不相关</li></ol><h2 id="集成学习之-Boosting"><a href="#集成学习之-Boosting" class="headerlink" title="集成学习之 Boosting"></a>集成学习之 Boosting</h2><ol><li><p><strong>弱学习器提升为强学习器的过程称为Boosting</strong>，Boosting产生一系列的学习器，后产生的学习器的训练集取决于之前的产生的学习器，之前被误判的示例在之后占据较大的概率。</p></li><li><p>不同的boosting实现，主要区别：</p><p>(1) 弱学习算法本身不同</p><p>(2) 在每一轮学习之前，改变训练数据的权值分布的方法不同</p><p>(3) 将一组弱分类器组合成一个强分类器的方法不同</p></li><li><p>Boosting 算法总结</p><ul><li><p>AdaBoost</p><p>AdaBoost 算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法的学习方法</p></li></ul></li></ol><ul><li><p>GBDT 梯度提升决策树 </p><p>GBDT = Boosting Decision Tree 提升决策树 (DT)+ Gradient Boosting 梯度提升(GB) </p><p>是一种迭代的决策树算法</p></li></ul><ul><li><p>XGBoost </p><p>XGBoost是在GBDT的基础上对Boosting算法进行的改进，内部决策树使用的是回归树。</p></li></ul><h2 id="集成学习之Bagging"><a href="#集成学习之Bagging" class="headerlink" title="集成学习之Bagging"></a>集成学习之Bagging</h2><p>Bagging是基于bootstrap sampling也称为自助采样法法，是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间。</p><ul><li><p>Random Forest 随机森林 (RF) 就是基于Bagging，进一步在训练过程中引入随机属性。随机森林简单来说就是用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。</p><p>​</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、回归算法&quot;&gt;&lt;a href=&quot;#一、回归算法&quot; class=&quot;headerlink&quot; title=&quot;一、回归算法&quot;&gt;&lt;/a&gt;一、回归算法&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;线性回归：预测连续值&lt;/li&gt;
&lt;li&gt;逻辑回归：预测离散值或分类&lt;/li&gt;
&lt;/ol&gt;
&lt;h2
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Cygwin 安装</title>
    <link href="http://yoursite.com/2018/06/01/Cygwin-%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2018/06/01/Cygwin-安装/</id>
    <published>2018-06-01T09:12:21.000Z</published>
    <updated>2018-06-01T09:17:06.917Z</updated>
    
    <content type="html"><![CDATA[<p>PS：一篇我自己都搞不大明白的博文，咋办呢 … 得慢慢开始学 Linux 了</p><h2 id="Cygwin的安装"><a href="#Cygwin的安装" class="headerlink" title="Cygwin的安装"></a>Cygwin的安装</h2><ul><li>Cygwin 用于各种版本的Microsoft Windows上，运行类UNIX系统。Cygwin的主要目的是通过重新编译，将POSIX系统（例如Linux、BSD，以及其他Unix系统）上的软件移植到Windows上。                     官网下载 <a href="http://www.cygwin.com/" target="_blank" rel="noopener">http://www.cygwin.com/</a></li></ul><p><a href="https://jingyan.baidu.com/article/6b97984d83dfe51ca2b0bf0e.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/6b97984d83dfe51ca2b0bf0e.html</a></p><ul><li>安装vim 之前只安装了gcc， 后面又重新进去安装，在之前那步就把选项给勾了。                        <a href="https://blog.csdn.net/u012247418/article/details/79719679" target="_blank" rel="noopener">https://blog.csdn.net/u012247418/article/details/79719679</a></li></ul><ul><li>helloworld 验证 (难道就我觉得用起来这么不方便的吗)</li></ul><p><a href="https://jingyan.baidu.com/article/a948d6512fb5d70a2ccd2e6f.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/a948d6512fb5d70a2ccd2e6f.html</a></p><a id="more"></a><h2 id="Cygwin-、虚拟机-、Win10-Linux子系统区别"><a href="#Cygwin-、虚拟机-、Win10-Linux子系统区别" class="headerlink" title="Cygwin 、虚拟机 、Win10 Linux子系统区别"></a>Cygwin 、虚拟机 、Win10 Linux子系统区别</h2><p>(1) Cygwin 和虚拟机的区别</p><p>Cygwin 是一个 POSIX 兼容层，就是将linux平台上的系统调用转换成了windows API，使 Linux 的软件能够运行，应该可以称得上一种 Linux 发行版，而虚拟机则是虚拟了一个仿真的计算机环境，让Linux 认为虚拟机是一台机器，从而在上面安装 Linux</p><p><strong>Cygwin并<em>不能直接运行</em>  Linux 中运行的程序，必须在Cygwin中<em>重新编译</em> 该程序源码后才能让该程序在windows中运行</strong>。</p><p><a href="http://www.169it.com/tech-qa-linux/article-8333186796219879155.html" target="_blank" rel="noopener">http://www.169it.com/tech-qa-linux/article-8333186796219879155.html</a></p><p>(2) Win10下的Linux子系统</p><p><a href="http://www.cnblogs.com/micro-chen/p/5437316.html" target="_blank" rel="noopener">http://www.cnblogs.com/micro-chen/p/5437316.html</a></p><p><a href="https://www.jianshu.com/p/bc38ed12da1d" target="_blank" rel="noopener">https://www.jianshu.com/p/bc38ed12da1d</a></p><p>PS：下面是遇到的乱七八糟的总结，因为对OS实在不熟悉，遇到很多英文，真的搞不清楚这些是个啥</p><hr><h2 id="1-Linux-——-redhat、suse、debain、ubuntu、fedora版本区别"><a href="#1-Linux-——-redhat、suse、debain、ubuntu、fedora版本区别" class="headerlink" title="1. Linux —— redhat、suse、debain、ubuntu、fedora版本区别"></a>1. Linux —— <strong>redhat</strong>、suse、debain、<strong>ubuntu</strong>、fedora版本区别</h2><p><a href="https://blog.csdn.net/lzx1104/article/details/41776977" target="_blank" rel="noopener">https://blog.csdn.net/lzx1104/article/details/41776977</a></p><h2 id="2-Linux和windows下的命令行解释器"><a href="#2-Linux和windows下的命令行解释器" class="headerlink" title="2. Linux和windows下的命令行解释器"></a>2. Linux和windows下的命令行解释器</h2><p>命令解释器，处于内核和用户之间，负责把用户的指令传递给内核并且把执行结果回显给用户</p><h3 id="Linux："><a href="#Linux：" class="headerlink" title="Linux："></a>Linux：</h3><p> shell 是一个<strong>命令行解释器</strong>，是终端和Linux内核之间的接口程序，在提示符下输入的每个命令都由 shell 先解释然后传给Linux内核。                                                                               在Linux 和 UNIX系统里可以使用不同的shell，最常用的 Bourne shell (sh), C shell (csh),  Korn shell (ksh)。</p><ul><li><p>bash (Bourne Again shell)： Bourne shell 的扩展</p><p>常见bash命令      <a href="http://blinkfox.com/chang-yong-bashming-ling-zheng-li-yi-cha-kan-wen-jian-he-mu-lu/" target="_blank" rel="noopener">http://blinkfox.com/chang-yong-bashming-ling-zheng-li-yi-cha-kan-wen-jian-he-mu-lu/</a></p></li><li><p>zsh：完美兼容bash，并且有比bash更强大的功能，用起来也比bash更优雅</p></li></ul><h3 id="windows："><a href="#windows：" class="headerlink" title="windows："></a>windows：</h3><p>windows系统中见到的桌面即explorer.exe (文件资源管理器)是<strong>图形</strong>shell，而cmd就是命令行解释器(相当于Linux的bash),，windows也有强大的shell叫windows power shell</p><p>git bash是Windows下的命令行工具，是基于cmd的，在windows下使用git命令的模拟终端，linux、unix可以直接使用git。</p><h2 id="3-版本问题"><a href="#3-版本问题" class="headerlink" title="3. 版本问题"></a>3. 版本问题</h2><ul><li>Alpha：内部测试版。α是希腊字母的第一个，表示最早的版本，这个版本包含很多BUG，功能也不全，主要是给开发人员和测试人员测试和找BUG用的。</li><li>Beta：公开测试版。β是希腊字母的第二个，这个阶段的版本会一直加入新的功能。</li><li>RC：(Release　Candidate) 候选版本，RC版不会再加入新的功能了，主要着重于除错。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：一篇我自己都搞不大明白的博文，咋办呢 … 得慢慢开始学 Linux 了&lt;/p&gt;
&lt;h2 id=&quot;Cygwin的安装&quot;&gt;&lt;a href=&quot;#Cygwin的安装&quot; class=&quot;headerlink&quot; title=&quot;Cygwin的安装&quot;&gt;&lt;/a&gt;Cygwin的安装&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Cygwin 用于各种版本的Microsoft Windows上，运行类UNIX系统。Cygwin的主要目的是通过重新编译，将POSIX系统（例如Linux、BSD，以及其他Unix系统）上的软件移植到Windows上。                     官网下载 &lt;a href=&quot;http://www.cygwin.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.cygwin.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://jingyan.baidu.com/article/6b97984d83dfe51ca2b0bf0e.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://jingyan.baidu.com/article/6b97984d83dfe51ca2b0bf0e.html&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安装vim 之前只安装了gcc， 后面又重新进去安装，在之前那步就把选项给勾了。                        &lt;a href=&quot;https://blog.csdn.net/u012247418/article/details/79719679&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/u012247418/article/details/79719679&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;helloworld 验证 (难道就我觉得用起来这么不方便的吗)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://jingyan.baidu.com/article/a948d6512fb5d70a2ccd2e6f.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://jingyan.baidu.com/article/a948d6512fb5d70a2ccd2e6f.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="装软件" scheme="http://yoursite.com/tags/%E8%A3%85%E8%BD%AF%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Git 学习</title>
    <link href="http://yoursite.com/2018/06/01/Git-%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/06/01/Git-学习/</id>
    <published>2018-06-01T09:05:53.000Z</published>
    <updated>2018-06-01T09:07:33.373Z</updated>
    
    <content type="html"><![CDATA[<p>PS：之前虽然用GitHub搭建博客，但是对于Git的理解非常不透彻，然后最近准备学Linux ，哎，之前挖的坑还是要填起来啊，Linux系统的源码由Git管理，我总要搞清楚Git 是个啥~ 感觉看完廖雪峰老师的教程心里大致有了个数</p><a id="more"></a><h2 id="1-Git-版本控制系统"><a href="#1-Git-版本控制系统" class="headerlink" title="1. Git 版本控制系统"></a>1. Git 版本控制系统</h2><ul><li>集中式的版本控制系统：CVS及SVN                                                                                                                                            版本库是集中存放在中央服务器的，，要先从中央服务器取得最新的版本，然后开始干活，干完活了，再把自己的活推送给中央服务器。最大缺点就是必须联网才能工作。</li></ul><ul><li>分布式版本控制系统：Git                                                                                                                        每个人的电脑上都是一个完整的版本库，就不需要联网，因为版本库就在自己的电脑上。对于多人协作的方式，只需把各自的修改推送给对方，就可以互相看到对方的修改。</li></ul><p>所有的版本控制系统<strong>只能跟踪文本文件的改动</strong>，txt，网页，程序代码等等。而图片、视频这些二进制文件，虽然也能由版本控制系统管理，但没法跟踪文件的变化，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道。而Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的，如果要真正使用版本控制系统，就要以纯文本方式编写文件。</p><p>文本文件与二进制文件的区在编码层次上</p><ul><li>文本文件是基于字符编码的文件，常见的编码有ASCII编码，UNICODE编码</li><li>二进制文件是基于值编码的文件</li></ul><p>文本文件编码基于字符定长，译码容易些；二进制文件编码变长，存储利用率要高，译码难一些</p><p>PS：Git 查看和修改用户名和邮箱 <a href="https://www.jianshu.com/p/d24e791a7679" target="_blank" rel="noopener">https://www.jianshu.com/p/d24e791a7679</a></p><h2 id="2-Git仓库修改版本"><a href="#2-Git仓库修改版本" class="headerlink" title="2. Git仓库修改版本"></a>2. Git仓库修改版本</h2><h3 id="1-工作区"><a href="#1-工作区" class="headerlink" title="1) 工作区"></a>1) 工作区</h3><p>看的见的文件目录，放自己的文件</p><h3 id="2-版本库或仓库-repository"><a href="#2-版本库或仓库-repository" class="headerlink" title="2) 版本库或仓库 (repository)"></a>2) 版本库或仓库 (repository)</h3><p>理解成一个目录，这个目录里每个文件的修改、删除，Git都能跟踪   <u>！划重点！</u>  <strong>Git管理的是修改不是文件</strong></p><h3 id="3-添加文件到仓库的具体步骤"><a href="#3-添加文件到仓库的具体步骤" class="headerlink" title="3) 添加文件到仓库的具体步骤"></a>3) 添加文件到仓库的具体步骤</h3><p>1.创建空的仓库                                                                                                                                                        2.在工作区目录下新建文件      <code>touch create_repository.txt</code>                                                                                                                        3.将这个文件添加到<strong>暂存区</strong>     <code>git add create_repository.txt</code>                                                            4.将<strong>暂存区所有内容提交到当前分支 master</strong>  <code>git commit -m &quot;本次提交说明&quot;</code>  commit就是保存一个文件快照</p><p>文件提交到版本库里分两步，暂存区和分支    <u>！划重点！</u> 所以每次修改，如果不用<code>git add</code>到暂存区，那就不会加到 <code>commit</code> 中</p><h3 id="4）修改仓库版本"><a href="#4）修改仓库版本" class="headerlink" title="4）修改仓库版本"></a>4）修改仓库版本</h3><p>查看版本历史  <code>git log</code>     查看命令修改历史  <code>git reflog</code>   </p><p>回到上一版本   <code>git reset --hard Head^</code>                                                                                         回到上上个版本   <code>git reset --hard Head^^</code>                                                                                            回到往上的100个版本   <code>git reset --hard Head~100</code>        指定到指定的版本   <code>git reset --hard 版本号</code>   </p><h2 id="3-远程仓库"><a href="#3-远程仓库" class="headerlink" title="3.远程仓库"></a>3.远程仓库</h2><p>Git是分布式版本控制系统，同一个Git仓库，可以分布到不同的机器上，且每台机器的版本库其实都是一样的。</p><p>实际情况是找一台电脑当服务器，每天24小时开机，每个人都从这个服务器仓库里克隆一份到自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交。</p><p>只要注册一个GitHub账号，就可以免费获得Git远程仓库，本地Git仓库和GitHub仓库之间的传输是通过SSH加密。</p><p>1) SSH (Secure Shell) 安全外壳协议                                                                                                                     </p><p><a href="https://www.jianshu.com/p/33461b619d53" target="_blank" rel="noopener">https://www.jianshu.com/p/33461b619d53</a> 嗯~看不懂~不想看</p><p>1) 本地仓库同步到远程仓库</p><ul><li><p>GitHub 创建一个新的仓库</p></li><li><p>将本地仓库与远程仓库关联 <code>git remote add origin https://github.com/Sophia0130/LearnGit.git</code></p><p>下面两条tips：</p><p>1&gt; 提示出错信息：fatal: remote origin already exists.      输入  <code>git remote rm origin</code> 再进行关联</p><p>2&gt; 第一次推送master分支时，加上了<code>-u</code>参数，Git不但会把本地的<code>master</code>分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令</p></li></ul><ul><li>把本地库的所有内容推送到远程库上 <code>git push -u origin master</code></li><li>要本地作了提交，通过 <code>git push origin master</code>  把本地master分支的最新修改推送至GitHub</li></ul><p>2) 从远程仓库克隆</p><h2 id="4-分支管理"><a href="#4-分支管理" class="headerlink" title="4. 分支管理"></a>4. 分支管理</h2><p>PS：这部分内容画 branch 分支图比较好理解</p><ul><li><p>创建一个分支，即增加一个分支的指针，并将Head指向该分支</p><p><code>git branch dev</code>  创建了一个分支dev，<code>git checkout dev</code>将Head指向这个分支</p></li></ul><ul><li><p>对工作区的修改和提交就是针对dev分支了，新提交一次后，dev指针往前移动一步，而master指针不变</p><p><code>git add</code> 和 <code>git commit -m &quot;xxxx&quot;</code>提交当前分支</p></li></ul><ul><li><p>在dev上的工作完成后，把master指向当前的dev,完成合并</p><p><code>git merge dev</code></p></li></ul><ul><li><p>删除dev分支，即把dev指针删除</p><p><code>git branch -d dev</code></p></li></ul><p>查看分支：<code>git branch</code>列出所有分支，当前分支前面会标一个<code>*</code>号</p><p>创建分支：<code>git branch</code></p><p>切换分支：<code>git checkout</code></p><p>创建+切换分支：<code>git checkout -b</code></p><p>合并某分支到当前分支：<code>git merge</code></p><p>删除分支：<code>git branch -d</code></p><p>PS：后面还有很多分支管理的内容，但是我觉得我够用了，就不看了，别骂我懒 ~</p><h2 id="5-标签管理"><a href="#5-标签管理" class="headerlink" title="5. 标签管理"></a>5. 标签管理</h2><p>由于版本号那串东西太复杂了，所以打标签可以更容易区分，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。</p><p>切换到需要打标签的分支上</p><p><code>git tag v0.0</code>用于新建一个标签</p><p><code>git tag -a  -m &quot;xxx&quot;</code>可以指定标签信息</p><p><code>git tag</code>查看所有标签</p><p>PS：后面还有操作标签的内容，我也懒得看了 ~</p><h2 id="6-搭建Git服务器"><a href="#6-搭建Git服务器" class="headerlink" title="6. 搭建Git服务器"></a>6. 搭建Git服务器</h2><p>搭建Git服务器需要准备一台运行Linux的机器，需要管理每个人的公钥和控制权限 </p><p> 现在也用不到啊</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：之前虽然用GitHub搭建博客，但是对于Git的理解非常不透彻，然后最近准备学Linux ，哎，之前挖的坑还是要填起来啊，Linux系统的源码由Git管理，我总要搞清楚Git 是个啥~ 感觉看完廖雪峰老师的教程心里大致有了个数&lt;/p&gt;
    
    </summary>
    
    
      <category term="Git" scheme="http://yoursite.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>CryptoKitties</title>
    <link href="http://yoursite.com/2018/05/28/CryptoKitties/"/>
    <id>http://yoursite.com/2018/05/28/CryptoKitties/</id>
    <published>2018-05-28T08:55:11.000Z</published>
    <updated>2018-05-28T08:55:41.159Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CryptoKitties"><a href="#CryptoKitties" class="headerlink" title="CryptoKitties"></a>CryptoKitties</h1><p>PS：区块链的技术发展，颠覆了我对金融的全新认识 ~ CryptoKitties 基于以太坊的虚拟养猫游戏</p><p>PS：花生的回答，让我对以太坊游戏化应用有了透彻心扉的领悟，谁都不会觉得自己成为庞氏骗局的而接盘侠 ~     <a href="https://www.zhihu.com/question/263599712" target="_blank" rel="noopener">https://www.zhihu.com/question/263599712</a></p><p>新手买猫教程，一只猫这么贵，心疼小老百姓的钱，玩不起 <a href="https://zhuanlan.zhihu.com/p/32213293" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32213293</a></p><p>超级6啊，技术流派啊，用爬虫来选猫，哈哈哈 <a href="https://www.zhihu.com/question/263605805" target="_blank" rel="noopener">https://www.zhihu.com/question/263605805</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CryptoKitties&quot;&gt;&lt;a href=&quot;#CryptoKitties&quot; class=&quot;headerlink&quot; title=&quot;CryptoKitties&quot;&gt;&lt;/a&gt;CryptoKitties&lt;/h1&gt;&lt;p&gt;PS：区块链的技术发展，颠覆了我对金融的全新认识 ~
      
    
    </summary>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
  </entry>
  
  <entry>
    <title>2050大会——关于图像处理的收获</title>
    <link href="http://yoursite.com/2018/05/28/2050%E5%A4%A7%E4%BC%9A%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E6%94%B6%E8%8E%B7/"/>
    <id>http://yoursite.com/2018/05/28/2050大会——关于图像处理的收获/</id>
    <published>2018-05-28T07:46:21.000Z</published>
    <updated>2018-05-28T07:49:31.145Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2050"><a href="#2050" class="headerlink" title="2050"></a>2050</h2><p>PS：我也不知道应该咋评价这个大会，可能我实在太菜了，在我眼里更多是企业孵化器的一部分，将创业公司推向大众，吸引融资方和合作者，不是我自以为的知识分享。</p><p>不过从我专业来看，发现两个比较好玩的东西，都是对皮下血液进行图像处理，一个进行健康检测，一个进行身份识别，不过两者的成像原理不同，一个利用可见光，一个利用红外成像。</p><a id="more"></a><h2 id="可见光透视皮肤，查看皮下血液活动"><a href="#可见光透视皮肤，查看皮下血液活动" class="headerlink" title="可见光透视皮肤，查看皮下血液活动"></a>可见光透视皮肤，查看皮下血液活动</h2><p>PS：李康发明一种透皮光学成像技术，可以通过检测面部血液流动的情况，来推测被测试者的情感和感受，检测心理压力和心率。感觉特别神奇之处在于用到的是可见光进行血流检测，他们的产品只需要用手机摄像头就可以检测血压等生理信号。</p><p><strong>透皮光学成像</strong>，就是透过人的皮肤去查看皮肤底下的血液活动。人们在经历不同情感的时候，比如羞愧啊，难过啊，害怕啊，即使表面不露声色，但是面部的血液流动一定会发生变化。比如撒谎的时候，脸颊部分的血液流动就会减少，鼻子附近血流会增加。通过这项技术，观察一个人的鼻子附近是不是血流变多了，就能判断他是不是撒谎了，准确率能上升到85%以上，匹诺曹的故事竟然也是有科学道理的。</p><p>具体成像原理是利用可见光穿过皮肤，光子会被弹来弹去，最后被搅得乱成一团，但这些光子并没有丢失，因此从原则上说，这个混杂光场是可以进行逆向分解的。</p><p>对于利用可见光进行皮下血液成像觉得很神奇吧，不知道这个光场逆向解析散射光具体在光路上怎么做到的，但是准确度啥的，本小白还没考证，希望这项技术能开发更精准的算法，和更多应用吧 ~</p><p>可见光透视人体 ~                                                                                                                                          <a href="http://www.xinhuanet.com/science/2016-11/25/c_135855643.htm" target="_blank" rel="noopener">http://www.xinhuanet.com/science/2016-11/25/c_135855643.htm</a></p><p>这篇是李康的TED演讲 ~                                                                         <a href="https://www.ted.com/talks/kang_lee_can_you_really_tell_if_a_kid_is_lying/transcript?language=zh-cn" target="_blank" rel="noopener">https://www.ted.com/talks/kang_lee_can_you_really_tell_if_a_kid_is_lying/transcript?language=zh-cn</a></p><h2 id="掌静脉身份识别"><a href="#掌静脉身份识别" class="headerlink" title="掌静脉身份识别"></a>掌静脉身份识别</h2><p>掌静脉识别，静脉是导血回心的血管，起于毛细血管，止于心房，表浅静脉在皮下可以看见。掌静脉，顾名思义，就是手掌内静脉。掌静脉识别是静脉识别的一种，属于生物识别，掌静脉识别系统就是首先通过静脉识别仪取得个人掌静脉分布图，从掌静脉分布图依据专用比对算法提取特征值，通过红外线CCD摄像头获取手指、手掌、手背静脉的图像，将静脉的数字图像存贮在计算机系统中，将特征值存储。静脉比对时，实时采取静脉图，提取特征值，运用先进的滤波、图像二值化、细化手段对数字图像提取特征，同存储在主机中静脉特征值比对，采用复杂的匹配算法对静脉特征进行匹配，从而对个人进行身份鉴定，确认身份。</p><p>这项技术已经不是太前沿的了，但是我想聚焦一下它的具体应用，最近比较火的各种网红无人便利店和自动贩卖机，利用掌静脉识别模块及掌静脉认证支付系统，无需投币，无需刷卡，只需刷手便可完成支付，可以实现拿了商品就走的自动结算。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;2050&quot;&gt;&lt;a href=&quot;#2050&quot; class=&quot;headerlink&quot; title=&quot;2050&quot;&gt;&lt;/a&gt;2050&lt;/h2&gt;&lt;p&gt;PS：我也不知道应该咋评价这个大会，可能我实在太菜了，在我眼里更多是企业孵化器的一部分，将创业公司推向大众，吸引融资方和合作者，不是我自以为的知识分享。&lt;/p&gt;
&lt;p&gt;不过从我专业来看，发现两个比较好玩的东西，都是对皮下血液进行图像处理，一个进行健康检测，一个进行身份识别，不过两者的成像原理不同，一个利用可见光，一个利用红外成像。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Advanced Technology" scheme="http://yoursite.com/tags/Advanced-Technology/"/>
    
  </entry>
  
  <entry>
    <title>虚拟机和云(特别简陋的认识)</title>
    <link href="http://yoursite.com/2018/05/18/%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%92%8C%E4%BA%91-%E7%89%B9%E5%88%AB%E7%AE%80%E9%99%8B%E7%9A%84%E8%AE%A4%E8%AF%86/"/>
    <id>http://yoursite.com/2018/05/18/虚拟机和云-特别简陋的认识/</id>
    <published>2018-05-18T09:00:10.000Z</published>
    <updated>2018-05-18T09:00:47.201Z</updated>
    
    <content type="html"><![CDATA[<h2 id="虚拟机"><a href="#虚拟机" class="headerlink" title="虚拟机"></a>虚拟机</h2><p>PS：我不知道我为啥要写这篇博，单纯就想了解吧，所以记录得乱七八糟，因为自己也没接触这块过。</p><ol><li>宿主机：指要安装虚拟机软件的计算机，你花钱买的物理机。</li><li>虚拟机：利用虚拟机工具构造出的一整套硬件设备，有自己操作系统，应用软件。                    </li><li>虚拟机的简单应用</li></ol><p><a href="http://www.pc841.com/article/20120413-5731.html" target="_blank" rel="noopener">http://www.pc841.com/article/20120413-5731.html</a></p><a id="more"></a><p>1) 最简单我们电脑中没有光驱，如果要安装系统我们就可以<strong>使用虚拟机来安装系统，虚拟机内部拥有虚拟光驱，支持直接打开系统镜像文件安装系统</strong>。</p><p>2) 另外虚拟机技术在游戏爱好者朋友眼中也相当实用，很多游戏不支持同时多开，但我们可以在电脑中多创建几个虚拟机，那么在虚拟机系统中即可单独再运行程序了，这样即可实现一台电脑同时多开同一游戏了。</p><h2 id="虚拟机和云"><a href="#虚拟机和云" class="headerlink" title="虚拟机和云"></a>虚拟机和云</h2><p>这篇知乎写的还是很详细的，但是后面术语真的看不懂。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;虚拟机&quot;&gt;&lt;a href=&quot;#虚拟机&quot; class=&quot;headerlink&quot; title=&quot;虚拟机&quot;&gt;&lt;/a&gt;虚拟机&lt;/h2&gt;&lt;p&gt;PS：我不知道我为啥要写这篇博，单纯就想了解吧，所以记录得乱七八糟，因为自己也没接触这块过。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;宿主机：指要安装虚拟机软件的计算机，你花钱买的物理机。&lt;/li&gt;
&lt;li&gt;虚拟机：利用虚拟机工具构造出的一整套硬件设备，有自己操作系统，应用软件。                    &lt;/li&gt;
&lt;li&gt;虚拟机的简单应用&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&quot;http://www.pc841.com/article/20120413-5731.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.pc841.com/article/20120413-5731.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="云" scheme="http://yoursite.com/tags/%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>啥叫运营商</title>
    <link href="http://yoursite.com/2018/05/18/%E5%95%A5%E5%8F%AB%E8%BF%90%E8%90%A5%E5%95%86/"/>
    <id>http://yoursite.com/2018/05/18/啥叫运营商/</id>
    <published>2018-05-18T07:21:14.000Z</published>
    <updated>2018-05-18T07:21:52.920Z</updated>
    
    <content type="html"><![CDATA[<p>运营商是指<strong>提供网络服务的供应商</strong>，如中国联通、中国电信、 中国移动、中国移动这些公司，而诺基亚、三星等这些<strong>通信设备的生产厂家叫生产商</strong>，因为国家在电信管理方面相当严格，只有拥有工信部颁发的运营执照的公司才能架设网络，从通信行业来说，设备生产商和运营商是相互依存的。</p><p>不过今天看到一篇文章讲5G时代的到来对设备商和运营商的影响，感觉还是蛮有趣的，而且对两者会有更深刻的认识吧~                                                                                                            <a href="http://www.sohu.com/a/197243837_100030976" target="_blank" rel="noopener">http://www.sohu.com/a/197243837_100030976</a></p><a id="more"></a><p>随着技术的更新，运营商对于基站的建设可以说是投资巨大，稍有不慎可能全部打水漂，特别是当下技术还不成熟时，这让我想到了当年的液晶和等离子之争，现在的OLED和QLED之争，谁知道几年以后会发生如何的变迁，最重要的是站对阵营啊 ~</p><p>关于为什么要推动5G技术发展，引用文章种中的比喻还是很形象的，设备商是军火商，没有战争，怎么发财。没有新技术，怎么忽悠运营商。而运营商也希望通过网络技术来建筑竞争壁垒，获取技术红利。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;运营商是指&lt;strong&gt;提供网络服务的供应商&lt;/strong&gt;，如中国联通、中国电信、 中国移动、中国移动这些公司，而诺基亚、三星等这些&lt;strong&gt;通信设备的生产厂家叫生产商&lt;/strong&gt;，因为国家在电信管理方面相当严格，只有拥有工信部颁发的运营执照的公司才能架设网络，从通信行业来说，设备生产商和运营商是相互依存的。&lt;/p&gt;
&lt;p&gt;不过今天看到一篇文章讲5G时代的到来对设备商和运营商的影响，感觉还是蛮有趣的，而且对两者会有更深刻的认识吧~                                                                                                            &lt;a href=&quot;http://www.sohu.com/a/197243837_100030976&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.sohu.com/a/197243837_100030976&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>图像处理之坐标系和宽高</title>
    <link href="http://yoursite.com/2018/05/16/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B9%8B%E5%9D%90%E6%A0%87%E7%B3%BB%E5%92%8C%E5%AE%BD%E9%AB%98/"/>
    <id>http://yoursite.com/2018/05/16/图像处理之坐标系和宽高/</id>
    <published>2018-05-16T12:31:13.000Z</published>
    <updated>2018-05-16T12:48:07.284Z</updated>
    
    <content type="html"><![CDATA[<p>PS：为啥写这篇博，因为我傻啊 ~ 每次写程序遇到图像  <strong>坐标系-宽高-行列</strong>，就开始抓狂                        哎呀到底是(i，j)还是(j，i)，这篇博写的很详细啦 ~</p><p><a href="https://blog.csdn.net/oqqenvy12/article/details/71933651" target="_blank" rel="noopener">https://blog.csdn.net/oqqenvy12/article/details/71933651</a></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E5%9D%90%E6%A0%87%E7%B3%BB.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;PS：为啥写这篇博，因为我傻啊 ~ 每次写程序遇到图像  &lt;strong&gt;坐标系-宽高-行列&lt;/strong&gt;，就开始抓狂                        哎呀到底是(i，j)还是(j，i)，这篇博写的很详细啦 ~&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>XML、YAML、JSON</title>
    <link href="http://yoursite.com/2018/05/14/XML%E3%80%81YAML%E3%80%81JSON/"/>
    <id>http://yoursite.com/2018/05/14/XML、YAML、JSON/</id>
    <published>2018-05-14T09:04:32.000Z</published>
    <updated>2018-05-14T09:44:25.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="XML、YML-、JSON-是序列化数据格式"><a href="#XML、YML-、JSON-是序列化数据格式" class="headerlink" title="XML、YML 、JSON 是序列化数据格式"></a>XML、YML 、JSON 是序列化<strong>数据格式</strong></h2><p>PS：为什么要写这个呢，以为今天看opencv的时候讲到了XML、YAML 文件的输入输出，觉得这个经常听到名字但是不知道干啥用的~然后前几天搭博客，又一直出现JSON ~~ 心累，我的理解就是某种数据格式，方便数据的传输吧~~下面这篇篇博客讲了几个的简单区别，了解就可以了                                            <a href="http://rensanning.iteye.com/blog/2379083" target="_blank" rel="noopener">http://rensanning.iteye.com/blog/2379083</a></p><a id="more"></a><h2 id="OpenCV-使用XML和YAML实现文件输入和输出"><a href="#OpenCV-使用XML和YAML实现文件输入和输出" class="headerlink" title="OpenCV 使用XML和YAML实现文件输入和输出"></a>OpenCV 使用XML和YAML实现文件输入和输出</h2><p>为什么需要呢？因为当处理完图像后需要将数据保存到文件上。                                                                举个栗子：我们对一幅图像进行特征提取之后，需要把特征点信息保存到文件上，以供后面的机器学习分类操作。所以我们需要搭建<strong>小型数据库文件</strong>，将数据写到文件上，下次需要时从文件里读出。实现上述方法，需要使用xml和yml，具有可读性。</p><p>PS：opencv 自带的教程实在是太复杂了可以直接看下面这篇的案例，简单好懂~~                                        <a href="http://www.cnblogs.com/skyfsm/p/7182313.html" target="_blank" rel="noopener">http://www.cnblogs.com/skyfsm/p/7182313.html</a></p><p>XML/YAML文件在OpenCV中的数据结构为FileStorage</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;XML、YML-、JSON-是序列化数据格式&quot;&gt;&lt;a href=&quot;#XML、YML-、JSON-是序列化数据格式&quot; class=&quot;headerlink&quot; title=&quot;XML、YML 、JSON 是序列化数据格式&quot;&gt;&lt;/a&gt;XML、YML 、JSON 是序列化&lt;strong&gt;数据格式&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;PS：为什么要写这个呢，以为今天看opencv的时候讲到了XML、YAML 文件的输入输出，觉得这个经常听到名字但是不知道干啥用的~然后前几天搭博客，又一直出现JSON ~~ 心累，我的理解就是某种数据格式，方便数据的传输吧~~下面这篇篇博客讲了几个的简单区别，了解就可以了                                            &lt;a href=&quot;http://rensanning.iteye.com/blog/2379083&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://rensanning.iteye.com/blog/2379083&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>傅里叶变换和图像处理</title>
    <link href="http://yoursite.com/2018/05/14/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%92%8C%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2018/05/14/傅里叶变换和图像处理/</id>
    <published>2018-05-14T07:28:48.000Z</published>
    <updated>2018-05-14T07:30:43.709Z</updated>
    
    <content type="html"><![CDATA[<h2 id="离散傅里叶变换"><a href="#离散傅里叶变换" class="headerlink" title="离散傅里叶变换"></a>离散傅里叶变换</h2><p>原理：傅立叶变换是一个将函数分解的工具，任一函数都可以表示成无数个正弦和余弦函数的和的形式。对一张图像使用傅立叶变换就是将它分解成正弦和余弦两部分，也就是将图像从空间域(spatial domain)转换到频域(frequency domain)</p><h2 id="离散傅里叶变化-DFT"><a href="#离散傅里叶变化-DFT" class="headerlink" title="离散傅里叶变化 DFT"></a>离散傅里叶变化 DFT</h2><p><strong>图像去噪</strong>： 当图像出现的噪声是有规律的，去某个频率的波，比如高斯噪声。但是当出现的噪声是没有规律的，随机出现的一些东西，DFT是没有作用的。</p><h2 id="离散余弦变换-DCT"><a href="#离散余弦变换-DCT" class="headerlink" title="离散余弦变换 DCT"></a>离散余弦变换 DCT</h2><p>图像的余弦波由实偶函数组成</p><p><strong>图像压缩</strong>：JPEG格式的图片就是用Huffman编码方式压缩图片的DCT的系数</p><a id="more"></a><p>PS：这个东西我真的理解了很多遍，还是等要用的时候再说吧，下面这个代码可以看懂，但是原理就…</p><p><a href="http://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/core/discrete_fourier_transform/discrete_fourier_transform.html" target="_blank" rel="noopener">http://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/core/discrete_fourier_transform/discrete_fourier_transform.html</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//离散傅里叶变换 DFT</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/core.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/imgproc.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/imgcodecs.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/highgui.hpp"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">help</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"This program demonstrated the use of the discrete Fourier transform (DFT). "</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"The dft of an image is taken and it's power spectrum is displayed."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> ** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">help();</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* filename = argv[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">Mat I = imread(filename, IMREAD_GRAYSCALE);</span><br><span class="line"><span class="keyword">if</span> (I.empty())&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Error opening image"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//! [expand]  当图像的尺寸是2， 3，5的整数倍时，傅里叶变换的计算速度最快</span></span><br><span class="line">Mat padded; </span><br><span class="line"><span class="keyword">int</span> m = getOptimalDFTSize(I.rows); <span class="comment">//获取最佳尺寸</span></span><br><span class="line"><span class="keyword">int</span> n = getOptimalDFTSize(I.cols);</span><br><span class="line">copyMakeBorder(I, padded, <span class="number">0</span>, m - I.rows, <span class="number">0</span>, n - I.cols, BORDER_CONSTANT, Scalar::all(<span class="number">0</span>)); <span class="comment">//将边缘像素填充为0</span></span><br><span class="line"><span class="comment">//! [expand]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//! [complex_and_real] 为傅里叶变换的实部虚部分配内存</span></span><br><span class="line">Mat planes[] = &#123; Mat_&lt;<span class="keyword">float</span>&gt;(padded), Mat::zeros(padded.size(), CV_32F) &#125;; </span><br><span class="line">    <span class="comment">//在padded基础上加一个初始化为0的通道</span></span><br><span class="line">Mat complexI;</span><br><span class="line">merge(planes, <span class="number">2</span>, complexI);  <span class="comment">//将单通道数组合并成一个多通道的数组，从而创建出一个多通道阵列 </span></span><br><span class="line"><span class="comment">//! [complex_and_real]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//! [dft]</span></span><br><span class="line">dft(complexI, complexI); </span><br><span class="line"><span class="comment">//! [dft]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// compute the magnitude </span></span><br><span class="line"><span class="comment">//! [magnitude]</span></span><br><span class="line">split(complexI, planes);    <span class="comment">// planes[0] = Re(DFT(I), planes[1] = Im(DFT(I))</span></span><br><span class="line">magnitude(planes[<span class="number">0</span>], planes[<span class="number">1</span>], planes[<span class="number">0</span>]);<span class="comment">// 计算实部和虚部的幅值，放在planes[0]</span></span><br><span class="line">Mat magI = planes[<span class="number">0</span>];</span><br><span class="line"><span class="comment">//! [magnitude]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//! [log] switch to logarithmic scale</span></span><br><span class="line"><span class="comment">// =&gt; log(1 + sqrt(Re(DFT(I))^2 + Im(DFT(I))^2))</span></span><br><span class="line">magI += Scalar::all(<span class="number">1</span>); <span class="comment">//傅立叶变换的幅度值范围大到不适合在屏幕上显示，需要做对数尺度缩放</span></span><br><span class="line"><span class="built_in">log</span>(magI, magI);</span><br><span class="line"><span class="comment">//! [log]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//! [crop_rearrange]</span></span><br><span class="line"><span class="comment">// crop the spectrum, if it has an odd number of rows or columns</span></span><br><span class="line">magI = magI(Rect(<span class="number">0</span>, <span class="number">0</span>, magI.cols &amp; <span class="number">-2</span>, magI.rows &amp; <span class="number">-2</span>)); <span class="comment">//剔除添加的像素(和-2按位与) ？？实在不懂是为什么啊</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// rearrange the quadrants of Fourier image  so that the origin is at the image center 重分布幅度图象限</span></span><br><span class="line"><span class="keyword">int</span> cx = magI.cols / <span class="number">2</span>;</span><br><span class="line"><span class="keyword">int</span> cy = magI.rows / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">Mat q0(magI, Rect(0, 0, cx, cy));   // Top-Left - Create a ROI per quadrant</span><br><span class="line">Mat q1(magI, Rect(cx, 0, cx, cy));  // Top-Right</span><br><span class="line">Mat q2(magI, Rect(0, cy, cx, cy));  // Bottom-Left</span><br><span class="line">Mat q3(magI, Rect(cx, cy, cx, cy)); // Bottom-Right</span><br><span class="line"></span><br><span class="line">Mat tmp;                           <span class="comment">// swap quadrants (Top-Left with Bottom-Right)</span></span><br><span class="line">q0.copyTo(tmp);</span><br><span class="line">q3.copyTo(q0);</span><br><span class="line">tmp.copyTo(q3);</span><br><span class="line"></span><br><span class="line">q1.copyTo(tmp);                    <span class="comment">// swap quadrant (Top-Right with Bottom-Left)</span></span><br><span class="line">q2.copyTo(q1);</span><br><span class="line">tmp.copyTo(q2);</span><br><span class="line"><span class="comment">//! [crop_rearrange]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//! [normalize]</span></span><br><span class="line">normalize(magI, magI, <span class="number">0</span>, <span class="number">1</span>, NORM_MINMAX);<span class="comment">//数将幅度归一化到可显示范围//（不能放到0-255）</span></span><br><span class="line"><span class="comment">//! [normalize]</span></span><br><span class="line"></span><br><span class="line">imshow(<span class="string">"Input Image"</span>, I);    </span><br><span class="line">imshow(<span class="string">"spectrum magnitude"</span>, magI);</span><br><span class="line">waitKey();</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;离散傅里叶变换&quot;&gt;&lt;a href=&quot;#离散傅里叶变换&quot; class=&quot;headerlink&quot; title=&quot;离散傅里叶变换&quot;&gt;&lt;/a&gt;离散傅里叶变换&lt;/h2&gt;&lt;p&gt;原理：傅立叶变换是一个将函数分解的工具，任一函数都可以表示成无数个正弦和余弦函数的和的形式。对一张图像使用傅立叶变换就是将它分解成正弦和余弦两部分，也就是将图像从空间域(spatial domain)转换到频域(frequency domain)&lt;/p&gt;
&lt;h2 id=&quot;离散傅里叶变化-DFT&quot;&gt;&lt;a href=&quot;#离散傅里叶变化-DFT&quot; class=&quot;headerlink&quot; title=&quot;离散傅里叶变化 DFT&quot;&gt;&lt;/a&gt;离散傅里叶变化 DFT&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;图像去噪&lt;/strong&gt;： 当图像出现的噪声是有规律的，去某个频率的波，比如高斯噪声。但是当出现的噪声是没有规律的，随机出现的一些东西，DFT是没有作用的。&lt;/p&gt;
&lt;h2 id=&quot;离散余弦变换-DCT&quot;&gt;&lt;a href=&quot;#离散余弦变换-DCT&quot; class=&quot;headerlink&quot; title=&quot;离散余弦变换 DCT&quot;&gt;&lt;/a&gt;离散余弦变换 DCT&lt;/h2&gt;&lt;p&gt;图像的余弦波由实偶函数组成&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图像压缩&lt;/strong&gt;：JPEG格式的图片就是用Huffman编码方式压缩图片的DCT的系数&lt;/p&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>opencv基本绘图</title>
    <link href="http://yoursite.com/2018/05/14/opencv%E5%9F%BA%E6%9C%AC%E7%BB%98%E5%9B%BE/"/>
    <id>http://yoursite.com/2018/05/14/opencv基本绘图/</id>
    <published>2018-05-14T03:37:13.000Z</published>
    <updated>2018-05-14T03:38:53.823Z</updated>
    
    <content type="html"><![CDATA[<p>PS：其实本来不想写这篇的，实在太简单了，但是当做个检索吧，以备不时之需~~</p><p><a href="https://blog.csdn.net/ubunfans/article/details/24421981" target="_blank" rel="noopener">https://blog.csdn.net/ubunfans/article/details/24421981</a></p><a id="more"></a><h3 id="Point"><a href="#Point" class="headerlink" title="Point"></a>Point</h3><h3 id="Scalar"><a href="#Scalar" class="headerlink" title="Scalar"></a>Scalar</h3><h3 id="Rectangle"><a href="#Rectangle" class="headerlink" title="Rectangle"></a>Rectangle</h3><h3 id="Line"><a href="#Line" class="headerlink" title="Line"></a>Line</h3><h3 id="Circle"><a href="#Circle" class="headerlink" title="Circle"></a>Circle</h3><h3 id="Ellipse"><a href="#Ellipse" class="headerlink" title="Ellipse"></a>Ellipse</h3><h3 id="PolyLine-多边形的绘制"><a href="#PolyLine-多边形的绘制" class="headerlink" title="PolyLine   多边形的绘制"></a>PolyLine   多边形的绘制</h3><h3 id="PutText-在窗口显示文本-但是只能显示英文，中文不支持"><a href="#PutText-在窗口显示文本-但是只能显示英文，中文不支持" class="headerlink" title="PutText     在窗口显示文本(但是只能显示英文，中文不支持)"></a>PutText     在窗口显示文本(但是<strong>只能显示英文</strong>，中文不支持)</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：其实本来不想写这篇的，实在太简单了，但是当做个检索吧，以备不时之需~~&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/ubunfans/article/details/24421981&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/ubunfans/article/details/24421981&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
</feed>
