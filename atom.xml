<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>绿小蕤</title>
  <icon>https://www.gravatar.com/avatar/e4d7a8bd1cb84fb3b4123916b4ea2f6b</icon>
  <subtitle>好逸恶劳,贪生怕死</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-08-15T02:57:41.810Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>绿小蕤</name>
    <email>528036346@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>置顶目录</title>
    <link href="http://yoursite.com/2018/08/15/%E7%BD%AE%E9%A1%B6%E7%9B%AE%E5%BD%95/"/>
    <id>http://yoursite.com/2018/08/15/置顶目录/</id>
    <published>2018-08-15T01:59:20.000Z</published>
    <updated>2018-08-15T02:57:41.810Z</updated>
    
    <content type="html"><![CDATA[<p>检索目录列表</p><a id="more"></a><p><br></p><h2 id="opencv"><a href="#opencv" class="headerlink" title="opencv"></a>opencv</h2><p><a href="https://sophia0130.github.io/2018/05/07/ImageWatch/" target="_blank" rel="noopener">ImageWatch — VS编译小工具</a></p><p>opencv（一）~（？）学习笔记系列 ~ 持更</p><p><br></p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>贝叶斯分类器和概率图模型</p><p><a href="https://sophia0130.github.io/2018/05/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%A4%A7%E6%80%BB%E7%BB%93/" target="_blank" rel="noopener">机器学习算法大总结</a></p><p><br></p><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p><a href="https://sophia0130.github.io/2018/06/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">深度学习与强化学习</a></p><p>Deep-Learning—吴恩达—作业（一）~（七）系列</p><p><br></p><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>1.cs231n笔记（一）~（十一）系列</p><ul><li><p>线性回归、逻辑回归、多分类</p></li><li><p>最优化（Optimization）：最小化损失函数</p></li><li>反向传播：链式法则求梯度</li><li>深度学习框架：PyTorch（Facebook）、Tensorflow（Google）</li><li>分类、检测、分割、跟踪</li><li>反卷积与特征可视化</li></ul><p>不同种类的网络：</p><ul><li>神经网络</li><li>卷积神经网络</li><li>经典CNN网络</li><li>循环神经网络</li><li>生成式对抗网络</li></ul><p>2.<a href="https://sophia0130.github.io/2018/08/01/NLP%E2%80%94%E2%80%94word2vec/" target="_blank" rel="noopener">NLP——word2vec</a></p><p><br></p><h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><p><a href="https://sophia0130.github.io/2018/06/13/anaconda%EF%BC%8Cconda%EF%BC%8Cpip%E7%9A%84%E5%85%B3%E7%B3%BB/" target="_blank" rel="noopener">anaconda，conda，pip的关系</a></p><p><a href="https://sophia0130.github.io/2018/07/29/Ipython-%E4%B8%8E-Jupyter-Notebook/" target="_blank" rel="noopener">Ipython 与 Jupyter Notebook</a></p><p><a href="https://sophia0130.github.io/2018/06/11/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" target="_blank" rel="noopener">正则表达式</a></p><p><a href="https://sophia0130.github.io/2018/06/28/%E8%A7%A3%E5%86%B3pip%E5%AE%89%E8%A3%85%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">解决pip安装慢的问题</a></p><p><br></p><h2 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h2><p><a href="https://sophia0130.github.io/2018/08/05/TensorFlow-%E6%80%BB%E7%BB%93/" target="_blank" rel="noopener">TensorFlow 使用总结</a></p><p><a href="https://sophia0130.github.io/2018/08/05/Tensorboard-%E6%80%BB%E7%BB%93/" target="_blank" rel="noopener">Tensorboard 使用总结</a></p><p>TensorFlow代码实现（一）~（四）系列</p><p><br></p><h2 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h2><p><a href="https://sophia0130.github.io/2018/07/14/%E7%A8%8B%E5%BA%8F%E6%98%AF%E6%80%8E%E6%A0%B7%E8%B7%91%E8%B5%B7%E6%9D%A5%E7%9A%84/" target="_blank" rel="noopener">《程序是怎样跑起来的》读书笔记</a></p><p><a href="https://sophia0130.github.io/2018/08/05/%E5%B5%8C%E5%85%A5%E5%BC%8F%E4%B8%8E%E5%90%84%E7%A7%8D%E6%9D%BF%E5%AD%90/" target="_blank" rel="noopener">嵌入式与板子：MCU、ARM、DSP、FPGA、SOC</a></p><p><a href="https://sophia0130.github.io/2018/07/15/%E6%98%BE%E5%8D%A1/" target="_blank" rel="noopener">显卡</a></p><p><a href="https://sophia0130.github.io/2018/07/26/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/" target="_blank" rel="noopener">进程和线程</a></p><p><br></p><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><p><a href="https://sophia0130.github.io/2018/05/04/%E7%BC%96%E8%AF%91%E5%99%A8%E5%92%8C%E8%A7%A3%E9%87%8A%E5%99%A8/" target="_blank" rel="noopener">编译器和解释器</a></p><p><a href="https://sophia0130.github.io/2018/05/04/make%E5%92%8CCMake/" target="_blank" rel="noopener">make和cmake</a></p><p><br></p><h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><p><a href="https://sophia0130.github.io/2018/05/14/XML%E3%80%81YAML%E3%80%81JSON/" target="_blank" rel="noopener">序列化数据格式 ：xml、yaml、json</a></p><p><a href="https://sophia0130.github.io/2018/05/18/%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%92%8C%E4%BA%91-%E7%89%B9%E5%88%AB%E7%AE%80%E9%99%8B%E7%9A%84%E8%AE%A4%E8%AF%86/" target="_blank" rel="noopener">虚拟机和云</a></p><p><a href="https://sophia0130.github.io/2018/05/04/%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%90%86%E8%A7%A3/" target="_blank" rel="noopener">网络基础知识</a></p><p><br></p><h2 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h2><p><a href="https://sophia0130.github.io/2018/06/01/Git-%E5%AD%A6%E4%B9%A0/" target="_blank" rel="noopener">Git学习笔记</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;检索目录列表&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>opencv（六）——图像坐标系和宽高</title>
    <link href="http://yoursite.com/2018/08/15/opencv%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%E5%9B%BE%E5%83%8F%E5%9D%90%E6%A0%87%E7%B3%BB%E5%92%8C%E5%AE%BD%E9%AB%98/"/>
    <id>http://yoursite.com/2018/08/15/opencv（六）——图像坐标系和宽高/</id>
    <published>2018-08-15T01:52:32.000Z</published>
    <updated>2018-08-15T01:52:54.391Z</updated>
    
    <content type="html"><![CDATA[<p>PS：为啥写这篇博，因为我傻啊 ~ 每次写程序遇到图像  <strong>坐标系-宽高-行列</strong>，就开始抓狂                        哎呀到底是(i，j)还是(j，i)，这篇博写的很详细啦 ~</p><p><a href="https://blog.csdn.net/oqqenvy12/article/details/71933651" target="_blank" rel="noopener">https://blog.csdn.net/oqqenvy12/article/details/71933651</a></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E5%9D%90%E6%A0%87%E7%B3%BB.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;PS：为啥写这篇博，因为我傻啊 ~ 每次写程序遇到图像  &lt;strong&gt;坐标系-宽高-行列&lt;/strong&gt;，就开始抓狂                        哎呀到底是(i，j)还是(j，i)，这篇博写的很详细啦 ~&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>opencv（五）——图像读取</title>
    <link href="http://yoursite.com/2018/08/15/opencv%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%E5%9B%BE%E5%83%8F%E8%AF%BB%E5%8F%96/"/>
    <id>http://yoursite.com/2018/08/15/opencv（五）——图像读取/</id>
    <published>2018-08-15T01:47:54.000Z</published>
    <updated>2018-08-15T01:51:30.614Z</updated>
    
    <content type="html"><![CDATA[<h2 id="扫描、读取图像的四种方法"><a href="#扫描、读取图像的四种方法" class="headerlink" title="扫描、读取图像的四种方法"></a>扫描、读取图像的四种方法</h2><p>1.C operator [] 指针</p><p>2.iterator 迭代法 用迭代器遍历 [推荐使用]</p><p>3.on-the-fly address generation  .at()函数</p><p>4.LUT function [推荐使用]</p><a id="more"></a><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//扫描图像的四种方法的比较</span></span><br><span class="line"><span class="comment">//输入的三个参数  imageName.jpg  divideWith  intValueToReduce [G](可省略)</span></span><br><span class="line"><span class="comment">//项目属性需要更改：项目属性 -&gt; 配置属性 -&gt; C/C++ -&gt; 代码生成 -&gt; 运行库</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/utility.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/imgcodecs.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">help</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">cout</span></span><br><span class="line">&lt;&lt; <span class="string">"\n--------------------------------------------------------------------------"</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"This program shows how to scan image objects in OpenCV (cv::Mat)."</span></span><br><span class="line">&lt;&lt; <span class="string">" we take an input image and divide the native color palette (255) with the input. "</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"Shows C operator[] method, iterators and at function for on-the-fly item address calculation."</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"./输入三个参数 &lt;imageNameToUse&gt; &lt;divideWith&gt; &lt;G&gt;(可省略)"</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"if you add a G parameter the image is processed in gray scale"</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"--------------------------------------------------------------------------"</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceC</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar* table)</span></span>;</span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceIterator</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar* table)</span></span>;</span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceRandomAccess</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar * table)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">help();</span><br><span class="line"><span class="keyword">if</span> (argc &lt; <span class="number">3</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Not enough parameters"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Mat I, J;</span><br><span class="line"><span class="keyword">if</span> (argc == <span class="number">4</span> &amp;&amp; !<span class="built_in">strcmp</span>(argv[<span class="number">3</span>], <span class="string">"G"</span>))</span><br><span class="line">I = imread(argv[<span class="number">1</span>], IMREAD_GRAYSCALE);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">I = imread(argv[<span class="number">1</span>], IMREAD_COLOR);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (I.empty())</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"The image"</span> &lt;&lt; argv[<span class="number">1</span>] &lt;&lt; <span class="string">" could not be loaded."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//用命令行参数argv[2]给出的整数进行颜色缩减</span></span><br><span class="line"><span class="keyword">int</span> divideWith = <span class="number">0</span>; <span class="comment">// convert our input string to number - C++ style</span></span><br><span class="line"><span class="built_in">stringstream</span> s;</span><br><span class="line">s &lt;&lt; argv[<span class="number">2</span>];</span><br><span class="line">s &gt;&gt; divideWith;</span><br><span class="line"><span class="keyword">if</span> (!s || !divideWith)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Invalid number entered for dividing. "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//查找表</span></span><br><span class="line">uchar table[<span class="number">256</span>]; <span class="comment">//查找表</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; ++i)</span><br><span class="line">table[i] = (uchar)(divideWith * (i / divideWith));</span><br><span class="line"><span class="comment">//divideWith=10时，0到9取为0，10到19取为10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 运行时间-单位毫秒</span></span><br><span class="line"><span class="comment">// getTickCount() 返回CPU自某个事件以来走过的时钟周期数</span></span><br><span class="line"><span class="comment">// getTickFrequency()  返回CPU一秒钟所走的时钟周期数</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> times = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">double</span> t;</span><br><span class="line"><span class="comment">//方法1</span></span><br><span class="line">t = (<span class="keyword">double</span>)getTickCount();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; times; ++i)</span><br><span class="line">&#123;</span><br><span class="line">cv::Mat clone_i = I.clone();</span><br><span class="line">J = ScanImageAndReduceC(clone_i, table);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">t = <span class="number">1000</span> * ((<span class="keyword">double</span>)getTickCount() - t) / getTickFrequency();</span><br><span class="line">t /= times;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Time of reducing with the C operator [] (averaged for "</span></span><br><span class="line">&lt;&lt; times &lt;&lt; <span class="string">" runs): "</span> &lt;&lt; t &lt;&lt; <span class="string">" milliseconds."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">//方法1</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">//方法2</span></span><br><span class="line">t = (<span class="keyword">double</span>)getTickCount();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; times; ++i)</span><br><span class="line">&#123;</span><br><span class="line">cv::Mat clone_i = I.clone();</span><br><span class="line">J = ScanImageAndReduceIterator(clone_i, table);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">t = <span class="number">1000</span> * ((<span class="keyword">double</span>)getTickCount() - t) / getTickFrequency();</span><br><span class="line">t /= times;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Time of reducing with the iterator (averaged for "</span></span><br><span class="line">&lt;&lt; times &lt;&lt; <span class="string">" runs): "</span> &lt;&lt; t &lt;&lt; <span class="string">" milliseconds."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">//方法2</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">//方法3</span></span><br><span class="line">t = (<span class="keyword">double</span>)getTickCount();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; times; ++i)</span><br><span class="line">&#123;</span><br><span class="line">cv::Mat clone_i = I.clone();</span><br><span class="line">ScanImageAndReduceRandomAccess(clone_i, table);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">t = <span class="number">1000</span> * ((<span class="keyword">double</span>)getTickCount() - t) / getTickFrequency();</span><br><span class="line">t /= times;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Time of reducing with the on-the-fly address generation - at function (averaged for "</span></span><br><span class="line">&lt;&lt; times &lt;&lt; <span class="string">" runs): "</span> &lt;&lt; t &lt;&lt; <span class="string">" milliseconds."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">//方法3</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">//方法4</span></span><br><span class="line"><span class="comment">//最被推荐的用于实现批量图像元素查找和更该操作图像方法，在图像处理中，对于一个给定的值，将其替换成其他的值是一个很常见的操作</span></span><br><span class="line"><span class="function">Mat <span class="title">lookUpTable</span><span class="params">(<span class="number">1</span>, <span class="number">256</span>, CV_8U)</span></span>;</span><br><span class="line">uchar* p = lookUpTable.ptr();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; ++i)</span><br><span class="line">p[i] = table[i]; <span class="comment">//table数组的值赋给lookUpTable Mat</span></span><br><span class="line"><span class="comment">//! [table-init]</span></span><br><span class="line"></span><br><span class="line">t = (<span class="keyword">double</span>)getTickCount();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; times; ++i)</span><br><span class="line"><span class="comment">//! [table-use]</span></span><br><span class="line">LUT(I, lookUpTable, J);</span><br><span class="line"><span class="comment">//! [table-use]</span></span><br><span class="line"></span><br><span class="line">t = <span class="number">1000</span> * ((<span class="keyword">double</span>)getTickCount() - t) / getTickFrequency();</span><br><span class="line">t /= times;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Time of reducing with the LUT function (averaged for "</span></span><br><span class="line">&lt;&lt; times &lt;&lt; <span class="string">" runs): "</span> &lt;&lt; t &lt;&lt; <span class="string">" milliseconds."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"> <span class="comment">//方法4</span></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//! [scan-c] c语言风格，用指针</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceC</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar* <span class="keyword">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// accept only char type matrices</span></span><br><span class="line">CV_Assert(I.depth() == CV_8U); <span class="comment">//CV_Assert()若括号中的表达式值为false，则返回一个错误信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> channels = I.channels();</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> nRows = I.rows;</span><br><span class="line"><span class="keyword">int</span> nCols = I.cols * channels;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (I.isContinuous()) <span class="comment">//判断图像在内存中是否连续存储</span></span><br><span class="line">&#123;</span><br><span class="line">nCols *= nRows;   <span class="comment">//若图像连续存储将图像作为一维数组</span></span><br><span class="line">nRows = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//若图像连续存储，则可以将图像的数据看成是一个一维数组</span></span><br><span class="line"><span class="comment">//p（uchar*）就是指向图像数据的第一个字节的，因此可以用data指针访问图像的数据,从而加速Mat图像的访问速度</span></span><br><span class="line"><span class="comment">//一般经过裁剪的Mat图像，都不再连续了</span></span><br><span class="line"><span class="keyword">int</span> i, j;</span><br><span class="line">uchar* p;</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; nRows; ++i)</span><br><span class="line">&#123;</span><br><span class="line">p = I.ptr&lt;uchar&gt;(i);</span><br><span class="line"><span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; nCols; ++j)</span><br><span class="line">&#123;</span><br><span class="line">p[j] = table[p[j]];</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//! [scan-c]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//迭代法</span></span><br><span class="line"><span class="comment">//! [scan-iterator] 获得图像矩阵的begin和end</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceIterator</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar* <span class="keyword">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// accept only char type matrices</span></span><br><span class="line">CV_Assert(I.depth() == CV_8U);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> channels = I.channels();</span><br><span class="line"><span class="keyword">switch</span> (channels)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">&#123;</span><br><span class="line">  MatIterator_&lt;uchar&gt; it, end;</span><br><span class="line">  <span class="keyword">for</span> (it = I.begin&lt;uchar&gt;(), end = I.end&lt;uchar&gt;(); it != end; ++it)</span><br><span class="line">  *it = table[*it];</span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">&#123;</span><br><span class="line">  MatIterator_&lt;Vec3b&gt; it, end;</span><br><span class="line">  <span class="keyword">for</span> (it = I.begin&lt;Vec3b&gt;(), end = I.end&lt;Vec3b&gt;(); it != end; ++it)</span><br><span class="line">  &#123;</span><br><span class="line">  (*it)[<span class="number">0</span>] = table[(*it)[<span class="number">0</span>]];</span><br><span class="line">  (*it)[<span class="number">1</span>] = table[(*it)[<span class="number">1</span>]];</span><br><span class="line">  (*it)[<span class="number">2</span>] = table[(*it)[<span class="number">2</span>]];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//! [scan-iterator]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// On-the-fly 地址计算</span></span><br><span class="line"><span class="comment">//! [scan-random] 这个方法并不推荐被用来进行图像扫描，需要知道元素的所在行数与列数还有数据类型</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceRandomAccess</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar* <span class="keyword">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// accept only char type matrices</span></span><br><span class="line">CV_Assert(I.depth() == CV_8U);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> channels = I.channels();</span><br><span class="line"><span class="keyword">switch</span> (channels)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; I.rows; ++i)</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; I.cols; ++j)</span><br><span class="line">  I.at&lt;uchar&gt;(i, j) = table[I.at&lt;uchar&gt;(i, j)];</span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">&#123;</span><br><span class="line">  Mat_&lt;Vec3b&gt; _I = I;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; I.rows; ++i)</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; I.cols; ++j)</span><br><span class="line">  &#123;</span><br><span class="line">  _I(i, j)[<span class="number">0</span>] = table[_I(i, j)[<span class="number">0</span>]];</span><br><span class="line">  _I(i, j)[<span class="number">1</span>] = table[_I(i, j)[<span class="number">1</span>]];</span><br><span class="line">  _I(i, j)[<span class="number">2</span>] = table[_I(i, j)[<span class="number">2</span>]];</span><br><span class="line">  &#125;</span><br><span class="line">  I = _I;</span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="结果比较"><a href="#结果比较" class="headerlink" title="结果比较"></a>结果比较</h2><p>不同的图片几种方法得到的执行时间差很大：                                        </p><ol><li><strong>尽量使用 OpenCV 内置函数</strong>，调用LUT 函数可以获得最快的速度，这是因为OpenCV库可以通过英特尔线程架构启用多线程。                                                    </li><li>指针法也很快，但是前提是<strong>指针连续</strong>。                                        </li><li>.at()函数方法并不推荐被用来进行图像扫描。</li></ol><p><br></p><h2 id="程序解读"><a href="#程序解读" class="headerlink" title="程序解读"></a>程序解读</h2><h3 id="1-计算运算时间的方法"><a href="#1-计算运算时间的方法" class="headerlink" title="1.计算运算时间的方法"></a>1.计算运算时间的方法</h3><p>getTickCount() 返回CPU自某个事件以来走过的时钟周期数<br>getTickFrequency()  返回CPU一秒钟所走的时钟周期数</p><p><br></p><h3 id="2-查找表缩短颜色空间"><a href="#2-查找表缩短颜色空间" class="headerlink" title="2.查找表缩短颜色空间"></a>2.查找表缩短颜色空间</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">uchar table[<span class="number">256</span>]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; ++i)</span><br><span class="line">       table[i] = divideWith* (i/divideWith); <span class="comment">//当divideWith=10时，0到9取为0，10到19取为10</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;扫描、读取图像的四种方法&quot;&gt;&lt;a href=&quot;#扫描、读取图像的四种方法&quot; class=&quot;headerlink&quot; title=&quot;扫描、读取图像的四种方法&quot;&gt;&lt;/a&gt;扫描、读取图像的四种方法&lt;/h2&gt;&lt;p&gt;1.C operator [] 指针&lt;/p&gt;
&lt;p&gt;2.iterator 迭代法 用迭代器遍历 [推荐使用]&lt;/p&gt;
&lt;p&gt;3.on-the-fly address generation  .at()函数&lt;/p&gt;
&lt;p&gt;4.LUT function [推荐使用]&lt;/p&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>opencv（四）——AdaBoost</title>
    <link href="http://yoursite.com/2018/08/14/opencv%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94AdaBoost/"/>
    <id>http://yoursite.com/2018/08/14/opencv（四）——AdaBoost/</id>
    <published>2018-08-14T13:18:27.000Z</published>
    <updated>2018-08-15T01:45:06.865Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="一、Haar、LBP"><a href="#一、Haar、LBP" class="headerlink" title="一、Haar、LBP"></a>一、Haar、LBP</h2><ol><li>HAAR特征是浮点数计算，LBP特征是整数计算；</li><li>LBP训练需要的样本数量比HAAR大 </li><li>LBP的速度一般比HAAR快 </li><li>同样的样本HAAR训练出来的检测结果要比LBP准确</li><li>扩大LBP的样本数据可达到HAAR的训练效果</li></ol><p><br></p><h2 id="二、级联分类器"><a href="#二、级联分类器" class="headerlink" title="二、级联分类器"></a>二、级联分类器</h2><p><a href="https://zhuanlan.zhihu.com/p/35058334" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35058334</a></p><ol><li>级联分类器结构：弱分类器线性组合成为强分类器，强分类器用决策树的形式级联</li><li>存储分类器的XML中的参数</li></ol><p><img src="http://p8ge6t5tt.bkt.clouddn.com/xml1.JPG" alt=""></p><p><br></p><h2 id="三、利用并查集合并窗口和丢弃零散分布的误检窗口"><a href="#三、利用并查集合并窗口和丢弃零散分布的误检窗口" class="headerlink" title="三、利用并查集合并窗口和丢弃零散分布的误检窗口"></a>三、利用并查集合并窗口和丢弃零散分布的误检窗口</h2><p><br></p><h2 id="四、AdaBoost-算法"><a href="#四、AdaBoost-算法" class="headerlink" title="四、AdaBoost 算法"></a>四、AdaBoost 算法</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1. 概念"></a>1. 概念</h3><p>（1）准确率</p><ul><li>查准率 Precision</li><li>查全率 Recall，precision和recall都越高越好</li><li>命中率 hitRate，度量检测器对正样本的通过能力，越接近1越好  </li><li>虚警率 falseAlarm，度量检测器对负样本的通过能力，越接近0越好</li></ul><p>（2）positive sample会产生：</p><ul><li>TP(true positive)，即positive sample被检测器判定为目标</li><li>FN(false negative)，即positive samples被检测器判断为非目标</li></ul><p>（3）negative sample会产生：</p><ul><li>TN(true negative)，即negative sample被检测器判定为非目标</li><li>FP(false positive)，即negative sample被检测器判定为目标</li></ul><p><br></p><h3 id="2-AdaBoost-步骤"><a href="#2-AdaBoost-步骤" class="headerlink" title="2. AdaBoost 步骤"></a>2. AdaBoost 步骤</h3><ol><li>找TP和FP作为训练样本</li><li>计算每个Haar特征在当前权重下的Best split threshold+leftvalue+rightvalue，组成了一个个弱分类器</li><li>通过WSE寻找最优的弱分类器</li><li>更新权重</li><li>按照minHitRate估计stageThreshold</li><li>重复上述1-5步骤，直到falseAlarmRate到达要求，或弱分类器数量足够，停止循环，输出stage</li><li>进入下一个stage训练</li></ol><p><br></p><h2 id="五、opencv-提供的模型和工具"><a href="#五、opencv-提供的模型和工具" class="headerlink" title="五、opencv 提供的模型和工具"></a>五、opencv 提供的模型和工具</h2><p>应用放的位置  <strong>D:\program\opencv3.0.0\opencv\build\x64\vc12\bin</strong></p><p>1.opencv 自带的检测模型</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/opencv%E8%87%AA%E5%B8%A6%E7%9A%84xml.JPG" alt="img"></p><p>2.opencv 自带的AdaBoost分类器</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/opencvtrain.JPG" alt="img"></p><p>opencv_annotation         用来在一张大图中标定一个或多个需要检测的目标<br>opencv_createsamples   用来制作positive sample的vec<br>opencv_traincascade      用来训练得到需要的cascade.xml</p><p><br></p><h2 id="六、AdaBoost训练过程"><a href="#六、AdaBoost训练过程" class="headerlink" title="六、AdaBoost训练过程"></a>六、AdaBoost训练过程</h2><h3 id="阶段一：训练数据的准备"><a href="#阶段一：训练数据的准备" class="headerlink" title="阶段一：训练数据的准备"></a>阶段一：训练数据的准备</h3><h4 id="1-导出正负样本的文件列表"><a href="#1-导出正负样本的文件列表" class="headerlink" title="1. 导出正负样本的文件列表"></a>1. 导出正负样本的文件列表</h4><p>（1）注意：</p><ol><li><p>最好使用图片的<strong>绝对路径</strong>，不然最后训练可能找不到图片 Image reader can not be created from</p></li><li><p><strong>删除</strong>存放路径文件中的其它文件路径，只保留图片的路径</p></li><li><p>存放图片的路径的文件<strong>可以是 .txt 也可以是 .dat 文件</strong></p><p>txt扩展名的文件：存放ASCII码形式的文件</p><p>dat扩展名的文件：存放ASCII码，也可以是二进制（内存格式）形式的文件</p></li></ol><p>（2）用windows中的 <strong>dir</strong> 命令导出图片文件的列表</p><p><code>dir /b /s /o:n /a:a &gt; pos.dat</code>  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/b   表示去除摘要信息，且顶格显示完整路径</span><br><span class="line"></span><br><span class="line">/s   表示枚举嵌套文件夹中的内容</span><br><span class="line"></span><br><span class="line">/o:n 表示根据文件名排序</span><br><span class="line"></span><br><span class="line">/a:a 表示只枚举文件而不枚举其他</span><br></pre></td></tr></table></figure><p>单独使用dir /b与dir /s 都不会显示完整路径，只有这两个组合才会显示完整路径</p><p><br></p><h4 id="2-制作正样本的vec文件"><a href="#2-制作正样本的vec文件" class="headerlink" title="2. 制作正样本的vec文件"></a>2. 制作正样本的vec文件</h4><h5 id="（1）生成正样本描述文件-（-dat-或-txt-）"><a href="#（1）生成正样本描述文件-（-dat-或-txt-）" class="headerlink" title="（1）生成正样本描述文件 （.dat 或 .txt ）"></a>（1）生成正样本描述文件 （.dat 或 .txt ）</h5><p><strong>情况一：文件不需要裁剪</strong></p><p>将第一步中生成文件用替换的方法</p><p>将<code>D:\热像仪手势识别\jzy\adaboost\ConsoleApplication2\pos_resize\pos_1.jpg</code></p><p>修改为 <code>D:\热像仪手势识别\jzy\adaboost\ConsoleApplication2\pos_resize\pos_1.jpg 1 0 0 20 20</code></p><p><strong>情况二：文件需要裁剪</strong></p><p>用 opencv_annotation 框选目标的位置，得到的文件形式和上面方法一样</p><h5 id="（2）生成-vec-文件"><a href="#（2）生成-vec-文件" class="headerlink" title="（2）生成 vec 文件"></a>（2）生成 vec 文件</h5><p>用 opencv_createsamples 生成 vec 文件</p><p><br></p><h3 id="阶段二：训练级联分类器"><a href="#阶段二：训练级联分类器" class="headerlink" title="阶段二：训练级联分类器"></a>阶段二：训练级联分类器</h3><p>用 opencv_traincascade 训练级联分类器</p><p><code>opencv_traincascade.exe -data cascade -vec pos_resize\pos.vec -bg neg\neg.dat -numPos 100 -numNeg 300 -numStages 10  -featureType LBP  -w 20 -h 20</code></p><p>注意 </p><ol><li><code>-data cascade</code> 是指需要提前创建一个文件cascade放测试结果</li><li>正样本的.vec文件，负样本的.txt 或 .dat文件 要用<strong>相对路径</strong></li></ol><p>cascade文件：</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/traincascade3.JPG" alt=""></p><p>params：训练的参数，比如这里训练的分类器的级数是10</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/traincascade1.JPG" alt=""></p><p>第十级的训练情况：</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/traincascade2.JPG" alt=""></p><p><br></p><h3 id="阶段三：利用训练好的分类器进行检测"><a href="#阶段三：利用训练好的分类器进行检测" class="headerlink" title="阶段三：利用训练好的分类器进行检测"></a>阶段三：利用训练好的分类器进行检测</h3><p>PS：在使用opencv中ClassifierCascade类时，无法加载级联分类器，可能是因为 opencv3.0 版本不太稳定，将xml文件复制，使用相当路径 ，可能会好一点</p><p><br></p><p><strong>多尺度目标检测</strong></p><p><code>detectMultiScale (const Mat&amp; image, CV_OUT vector&amp; objects, double scaleFactor = 1.1, int minNeighbors = 3, int flags = 0, Size minSize = Size(), Size maxSize = Size())</code></p><ul><li><p>image—待检测图片，一般为<strong>灰度图像</strong>，加快检测速度</p></li><li><p>objects—检测结果的矩形框向量组，用来<strong>画出 bounding box</strong></p></li><li><p>scaleFactor—前后两次扫描中，搜索窗口的比例系数，默认为1.1，即每次搜索窗口依次扩大10%</p></li><li><p>minNeighbors—构成检测目标的相邻矩形的最小个数</p><p>​                             如果组成检测目标的小矩形的个数和小于 min_neighbors -1，就会被排除</p><p>​                             如果min_neighbors 为 0, 则函数不做任何操作就返回所有的被检候选矩形框</p></li><li><p>flags—使用默认值</p><p>​            使用CV_HAAR_DO_CANNY_PRUNING，使用Canny边缘检测来排除边缘过多或过少的区域</p></li><li><p>minSize和maxSize—限制得到的目标区域的范围。</p></li></ul><p><br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;一、Haar、LBP&quot;&gt;&lt;a href=&quot;#一、Haar、LBP&quot; class=&quot;headerlink&quot; title=&quot;一、Haar、LBP&quot;&gt;&lt;/a&gt;一、Haar、LBP&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;HAAR特征是浮点数计算，L
      
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>opencv（三）——绘图</title>
    <link href="http://yoursite.com/2018/08/14/opencv%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E7%BB%98%E5%9B%BE/"/>
    <id>http://yoursite.com/2018/08/14/opencv（三）——绘图/</id>
    <published>2018-08-14T13:15:51.000Z</published>
    <updated>2018-08-14T13:16:17.600Z</updated>
    
    <content type="html"><![CDATA[<p>PS：其实本来不想写这篇的，实在太简单了，但是当做个检索吧，以备不时之需~~</p><p><a href="https://blog.csdn.net/ubunfans/article/details/24421981" target="_blank" rel="noopener">https://blog.csdn.net/ubunfans/article/details/24421981</a></p><a id="more"></a><h3 id="Point"><a href="#Point" class="headerlink" title="Point"></a>Point</h3><h3 id="Scalar"><a href="#Scalar" class="headerlink" title="Scalar"></a>Scalar</h3><h3 id="Rectangle"><a href="#Rectangle" class="headerlink" title="Rectangle"></a>Rectangle</h3><h3 id="Line"><a href="#Line" class="headerlink" title="Line"></a>Line</h3><h3 id="Circle"><a href="#Circle" class="headerlink" title="Circle"></a>Circle</h3><h3 id="Ellipse"><a href="#Ellipse" class="headerlink" title="Ellipse"></a>Ellipse</h3><h3 id="PolyLine-多边形的绘制"><a href="#PolyLine-多边形的绘制" class="headerlink" title="PolyLine   多边形的绘制"></a>PolyLine   多边形的绘制</h3><h3 id="PutText-在窗口显示文本-但是只能显示英文，中文不支持"><a href="#PutText-在窗口显示文本-但是只能显示英文，中文不支持" class="headerlink" title="PutText     在窗口显示文本(但是只能显示英文，中文不支持)"></a>PutText     在窗口显示文本(但是<strong>只能显示英文</strong>，中文不支持)</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：其实本来不想写这篇的，实在太简单了，但是当做个检索吧，以备不时之需~~&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/ubunfans/article/details/24421981&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/ubunfans/article/details/24421981&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>opencv（二）——imgproc</title>
    <link href="http://yoursite.com/2018/08/14/opencv%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94imgproc/"/>
    <id>http://yoursite.com/2018/08/14/opencv（二）——imgproc/</id>
    <published>2018-08-14T12:49:42.000Z</published>
    <updated>2018-08-14T13:14:53.285Z</updated>
    
    <content type="html"><![CDATA[<p>PS：具体代码参考opencv /samples里的源码</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>平滑图像/模糊</li><li>腐蚀和膨胀</li><li>形态变化 ：开运算 、闭运算 、形态梯度、 顶帽 、黑帽</li><li>Hit-or-Miss 击中击不中</li><li>图像金字塔</li><li>二值化</li><li>线性滤波器</li><li>霍夫变换：着直线、找圆</li><li>重映射</li><li>仿射变换</li><li>直方图</li><li>模板匹配</li><li>轮廓查找</li><li>凸包</li><li>多边形拟合</li><li>矩</li></ol><a id="more"></a><p><br></p><h2 id="一、-平滑图像-模糊"><a href="#一、-平滑图像-模糊" class="headerlink" title="一、 平滑图像/模糊"></a>一、 平滑图像/模糊</h2><ol><li>模糊 cv :: blur                                                                            </li><li>高斯平滑 cv :: GaussianBlur                                                        </li><li>中值滤波 cv :: medianBlur                                                    </li><li>双边滤波 cv :: bilateralFilter</li></ol><p><br></p><h2 id="二、腐蚀和膨胀"><a href="#二、腐蚀和膨胀" class="headerlink" title="二、腐蚀和膨胀"></a>二、腐蚀和膨胀</h2><ol><li>腐蚀 cv::erode  输出像素的值是原图被掩膜所覆盖的所有像素中取<strong>最小像素</strong>                                                                                                                                            <img src="https://img.w3cschool.cn/attachments/image/20170901/1504236679969494.png" alt="腐蚀">        </li><li>膨胀 cv::dilate   输出像素的值是原图被掩膜所覆盖的所有像素中取<strong>最大像素</strong>                                                                                                    <img src="https://img.w3cschool.cn/attachments/image/20170901/1504236632842236.gif" alt="膨胀"></li></ol><p><br></p><h2 id="三、-形态变化"><a href="#三、-形态变化" class="headerlink" title="三、 形态变化"></a>三、 形态变化</h2><p>cv :: morphologyEx<br>开运算 、闭运算 、形态梯度、 顶帽 、黑帽</p><p><br></p><h2 id="四、-Hit-or-Miss-击中击不中"><a href="#四、-Hit-or-Miss-击中击不中" class="headerlink" title="四、 Hit-or-Miss 击中击不中"></a>四、 Hit-or-Miss 击中击不中</h2><p>cv :: morphologyEx 函数有一个参数 MORPH_HITMISS 实现击中击不中</p><p>PS：知乎上的大神说就是一个完全的<strong>模板匹配过程</strong>                                                                                                                                                        原理看这篇比较简单  <a href="https://blog.csdn.net/horseinch/article/details/50127955" target="_blank" rel="noopener">https://blog.csdn.net/horseinch/article/details/50127955</a>                                                        但是结合具体找钥匙的应用，一下子就明白啥叫击中击不中，猛推这篇博文，简直一语戳醒我~~                                            <strong>击中击不中变换是形态学中用来检测特定形状所处位置的一个基本工具</strong>                                                                                                                                                                        <a href="https://blog.csdn.net/jinshengtao/article/details/20707711" target="_blank" rel="noopener">https://blog.csdn.net/jinshengtao/article/details/20707711</a></p><p><br></p><h2 id="五、-图像金字塔"><a href="#五、-图像金字塔" class="headerlink" title="五、 图像金字塔"></a>五、 图像金字塔</h2><p>可以看浅墨大神写的 ~ ~ <a href="https://blog.csdn.net/poem_qianmo/article/details/26157633" target="_blank" rel="noopener">https://blog.csdn.net/poem_qianmo/article/details/26157633</a></p><p>图像金字塔是图像中多尺度表达的一种，是来源于同一张原始图的图像集合。                                                        越向上层级越大，图像越小，分辨率越低，金字塔<strong>底部是高分辨率图像</strong>，而<strong>顶部是低分辨率</strong>的近似。</p><h3 id="1-图像金字塔和采样"><a href="#1-图像金字塔和采样" class="headerlink" title="1. 图像金字塔和采样"></a>1. 图像金字塔和采样</h3><ul><li>高斯金字塔：用来向下采样，从低层向上(这句话一定要看下面这张图理解)</li><li>拉普拉斯金字塔： <strong>将拉普拉斯金字塔理解为高斯金字塔的逆形式</strong></li></ul><p>向上采样：pyrUp                                                                                                                                                                                        </p><p>向下采样：pyrDown                                                                    </p><p>PS：<strong>采样方向和金字塔方向相反</strong>(重要的事情不说三遍) 在这里卡了好久，<strong>金字塔的上下按照层级来，采样的上下按照尺寸来</strong>，直接上图吧，好理解~~</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/pyramid.png" alt=""></p><p>  <strong>pryUp不是PryDown的逆操作</strong>。图像首先在每个维度上扩大为原来的两倍，新增的行（偶数行）以0填充，用指定的滤波器卷积，估计“丢失”像素的近似值。</p><p>  pryDown( )是一个会丢失信息的函数。为了恢复原来更高的分辨率的图像，要获得由降采样操作丢失的信息，这些数据就和拉普拉斯金字塔有关系。</p><h3 id="2-图像金字塔的作用"><a href="#2-图像金字塔的作用" class="headerlink" title="2. 图像金字塔的作用"></a>2. 图像金字塔的作用</h3><p>图像压缩</p><p>图像金字塔<strong>构建尺度空间</strong>，用于<strong>目标检测</strong></p><p>下面这篇博文提到了SIFT特征，没想到图像金字塔引出的尺寸空间这么有用~~</p><p><a href="https://blog.csdn.net/xiaowei_cqu/article/details/8069548" target="_blank" rel="noopener">https://blog.csdn.net/xiaowei_cqu/article/details/8069548</a></p><p><br></p><h2 id="六、阈值操作"><a href="#六、阈值操作" class="headerlink" title="六、阈值操作"></a>六、阈值操作</h2><ul><li><p>cv :: threshold</p></li><li><p>cv :: inRange</p><p>区别：两个函数都能实现二值化                                                                                                              </p><p>但是 <strong>inRange()可以同时针对多通道操作</strong>，使用起来非常方便</p><p><br></p></li></ul><h2 id="七、-线性滤波器"><a href="#七、-线性滤波器" class="headerlink" title="七、 线性滤波器"></a>七、 线性滤波器</h2><p>cv::filter2D</p><p><br></p><h2 id="八、设置边框"><a href="#八、设置边框" class="headerlink" title="八、设置边框"></a>八、设置边框</h2><p>cv::copyMakeBorder</p><p>除了字面上的添加边框外，还有一个实际用处，如果<strong>评估点位于图像的边缘，如何卷积图像</strong>？                                                                                                            大多数OpenCV功能是将给定的图像复制到另一个稍大的图像上，然后用上式自动填充边界</p><p><strong>随机数生成器RNG</strong>                                                                             uniform函数可以返回指定范围的随机数，gaussian函数返回一个高斯随机数，fill则用随机数填充矩阵                <a href="https://blog.csdn.net/zyttae/article/details/41719349" target="_blank" rel="noopener">https://blog.csdn.net/zyttae/article/details/41719349</a></p><p><br></p><h2 id="九、-霍夫变换"><a href="#九、-霍夫变换" class="headerlink" title="九、 霍夫变换"></a>九、 霍夫变换</h2><h3 id="1-Hough-Line变换-找直线"><a href="#1-Hough-Line变换-找直线" class="headerlink" title="1.Hough Line变换 找直线"></a>1.Hough Line变换 找直线</h3><p>cv::HoughLines</p><p>cv::HoughLinesP</p><h3 id="2-Hough-Circle变换-找圆-圆心和半径"><a href="#2-Hough-Circle变换-找圆-圆心和半径" class="headerlink" title="2.Hough Circle变换 找圆(圆心和半径)"></a>2.Hough Circle变换 找圆(圆心和半径)</h3><p>cv::HoughCircles</p><p><br></p><h2 id="十、重映射（Remapping）"><a href="#十、重映射（Remapping）" class="headerlink" title="十、重映射（Remapping）"></a>十、重映射（Remapping）</h2><p>cv::remap</p><p>重映射，重新映射，就是从图像中的一个位置获取像素并将它们定位在新图像中的另一位置的过程。为了完成映射过程，可能需要对非整数像素位置进行一些<strong>插值</strong>，因为在源图像和目的图像之间不一定存在一对一像素的对应关系。</p><p><a href="https://blog.csdn.net/poem_qianmo/article/details/30974513" target="_blank" rel="noopener">https://blog.csdn.net/poem_qianmo/article/details/30974513</a></p><p><br></p><h2 id="十一、仿射变换（Affine-Transformations）"><a href="#十一、仿射变换（Affine-Transformations）" class="headerlink" title="十一、仿射变换（Affine Transformations）"></a>十一、仿射变换（Affine Transformations）</h2><ol><li><p>实现仿射：                                                                                                                cv::warpAffine   可以实现平移(Translation)、缩放(Scale)、旋转(Rotation)</p></li><li><p>计算仿射矩阵的两种方法：                                                          cv::getRotationMatrix2D  根据三个系数<strong>center旋转中心 、angle旋转角度、 scale、缩放系数</strong>计算仿射矩阵                                                        </p><p>cv::getAffineTransform  根据三个点的映射计算仿射矩阵</p></li></ol><p>2 x 3的矩阵M来表示仿射变换        <img src="http://p8ge6t5tt.bkt.clouddn.com/affine.png" alt=""></p><p>再上浅墨大神的教程~~ <a href="https://blog.csdn.net/poem_qianmo/article/details/33320997" target="_blank" rel="noopener">https://blog.csdn.net/poem_qianmo/article/details/33320997</a></p><p><br></p><h2 id="十二、直方图"><a href="#十二、直方图" class="headerlink" title="十二、直方图"></a>十二、直方图</h2><h3 id="1-计算直方图"><a href="#1-计算直方图" class="headerlink" title="1.计算直方图"></a>1.计算直方图</h3><p>cv::split 将图像分割成对应的平面，将图像分解为R，G和B平面</p><p>cv::calcHist 来计算图像数组的直方图</p><p>cv::normalize 对数组进行归一化</p><h3 id="2-直方图均衡化"><a href="#2-直方图均衡化" class="headerlink" title="2.直方图均衡化"></a>2.直方图均衡化</h3><p>对图像中像素个数多的灰度级进行展宽，像素个数少的灰度进行压缩，从而提高了对比度和灰度色调的变化，使图像更加清晰)                                                                                                                                                cv::equalizeHis</p><h3 id="3-直方图比较"><a href="#3-直方图比较" class="headerlink" title="3.直方图比较"></a>3.直方图比较</h3><p>使用不同的指标来比较直方图，两个直方图相互匹配的程度，提供了四种比较方法                            cv::compareHist</p><h3 id="4-反向投影"><a href="#4-反向投影" class="headerlink" title="4.反向投影"></a>4.反向投影</h3><p><strong>反向投影图是指图像的某一位置上像素值用对应在直方图的所属于的bin上的值来代替该像素值</strong>，不想看文字，可以看下面这篇博文的数学描述                                                                                            <a href="https://blog.csdn.net/chenjiazhou12/article/details/22150421" target="_blank" rel="noopener">https://blog.csdn.net/chenjiazhou12/article/details/22150421</a>    </p><p>PS：开始觉得这个反向投影能有啥用啊，如果你和我一样，看看下面，反向投影在<strong>定位</strong>上可是很有用~~            反向投影用于在输入图像 (通常较大) 中<strong>查找特定的模板图像</strong> (通常较小) 最匹配的点或者区域最亮。                                    cv::calcBackProject </p><p><br></p><h2 id="十三、-模板匹配"><a href="#十三、-模板匹配" class="headerlink" title="十三、 模板匹配"></a>十三、 模板匹配</h2><p>通过滑动，从左到右，从上到下，在各个位置，比较模板和源图的匹配程度<br><strong>矩阵R用于存放metric</strong>，即匹配好坏程度的值 ，R中(x,y)存放对应位置的匹配度量</p><p>cv::matchTemplate() 用来搜索模板和输入图像之间的匹配</p><p>cv::minMaxLoc() 用来查找R矩阵，即匹配好坏程度中的最大值和最小值(以及位置)</p><p><br></p><h2 id="十四、轮廓查找"><a href="#十四、轮廓查找" class="headerlink" title="十四、轮廓查找"></a>十四、轮廓查找</h2><p>cv::findContours      查找轮廓 </p><p>cv::drawContours     画轮廓</p><p><br></p><h2 id="十五、凸包"><a href="#十五、凸包" class="headerlink" title="十五、凸包"></a>十五、凸包</h2><p>凸包又叫凸壳，凸包能包含点集中所有的点，如果在集合A内连接任意两个点的直线段都在A的内部，则称集合A是凸形的。字面意思，就是一个多边型，没有凹的地方。</p><p><a href="https://blog.csdn.net/keith_bb/article/details/70194073" target="_blank" rel="noopener">https://blog.csdn.net/keith_bb/article/details/70194073</a></p><p><br></p><h2 id="十六、-多边形拟合"><a href="#十六、-多边形拟合" class="headerlink" title="十六、 多边形拟合"></a>十六、 多边形拟合</h2><p>cv::approxPolyDP  对边缘轮廓进行多边形拟合                                    </p><p>对于拟合后的轮廓求包围盒</p><p><a href="http://www.cnblogs.com/mikewolf2002/p/3427079.html" target="_blank" rel="noopener">http://www.cnblogs.com/mikewolf2002/p/3427079.html</a></p><p><br></p><h2 id="十七、空间矩，中心矩，归一化中心矩，Hu矩"><a href="#十七、空间矩，中心矩，归一化中心矩，Hu矩" class="headerlink" title="十七、空间矩，中心矩，归一化中心矩，Hu矩"></a>十七、空间矩，中心矩，归一化中心矩，Hu矩</h2><p>PS：关于矩的概念，若不想深究，只看公式，可以看下面这篇~~</p><ul><li>中心矩：<strong>平移不变性</strong></li><li>归一化中心矩：<strong>平移不变性，比例不变性</strong></li><li>Hu矩：利用二阶和三阶中心矩构造<strong>七个不变矩</strong>，具有<strong>平移、缩放、旋转不变性</strong></li></ul><p><a href="https://blog.csdn.net/kuweicai/article/details/79027388" target="_blank" rel="noopener">https://blog.csdn.net/kuweicai/article/details/79027388</a></p><p>cv::moments 计算图像的中心矩                                                </p><p>cv::HuMoments  由中心矩计算Hu矩                                        </p><p>cv::contourArea 计算轮廓面积                                                </p><p>cv::arcLength  计算封闭轮廓或曲线长度</p><p><br></p><h2 id="十八、判断点在多边形内部还是外部"><a href="#十八、判断点在多边形内部还是外部" class="headerlink" title="十八、判断点在多边形内部还是外部"></a>十八、判断点在多边形内部还是外部</h2><p>cv::pointPolygonTest  返回值是图像中的该点到某轮廓的最短距离，通过返回值的正负，判断该点在这个轮廓里面还是外面</p><p><br></p><h2 id="使用形态学操作来提取水平和垂直线，加水印的方法"><a href="#使用形态学操作来提取水平和垂直线，加水印的方法" class="headerlink" title="使用形态学操作来提取水平和垂直线，加水印的方法"></a>使用形态学操作来提取水平和垂直线，加水印的方法</h2><p>(1) 形态学是一组图像处理操作，其基于预定义的也称为内核的结构元素来处理图像，两个最基本的形态操作是腐蚀和膨胀。                                                                                                获得水平线，根据结构元素<img src="https://img.w3cschool.cn/attachments/image/20170901/1504236903564726.png" alt="水平">先腐蚀得到水平线，再膨胀加粗</p><p>同理，获得竖直线，只不过结构元素需要改变<img src="https://img.w3cschool.cn/attachments/image/20170901/1504236942451433.png" alt="竖直">                                                        这种方法结构元素的大小和形状很关键，不然找直线还是用霍夫变换吧</p><p>​</p><p>(2) PS：这个程序里用到了 copyTo 加了个mask参数，查了一下，竟然是一种<strong>做水印的方法</strong>！！神奇~~                                                                                                                        src.copyTo(dst)     src.copyTo(dst, mask)                                                                                                            原理：<strong>src为水印图片，mask作为一个掩模板，mask在(i, j)其值为1，则把src.at(i, j)处的值直接赋给dst.at(i, j)                                        mask在(i, j)其值为0，dst.at(i, j)处保留其原始像素值</strong>                                                            (mask 可以由src 经过阈值化得到，mask为1的地方就是水印src需要加上去的地方，mask为0的地方，就是水印src，即原图dst不会被掩盖)                                                                                                                水印案例：可以参考下面这个博文~~                                                                                                                                                                                                                                <a href="http://www.cnblogs.com/xianglan/archive/2011/07/30/2122186.html" target="_blank" rel="noopener">http://www.cnblogs.com/xianglan/archive/2011/07/30/2122186.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：具体代码参考opencv /samples里的源码&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;平滑图像/模糊&lt;/li&gt;
&lt;li&gt;腐蚀和膨胀&lt;/li&gt;
&lt;li&gt;形态变化 ：开运算 、闭运算 、形态梯度、 顶帽 、黑帽&lt;/li&gt;
&lt;li&gt;Hit-or-Miss 击中击不中&lt;/li&gt;
&lt;li&gt;图像金字塔&lt;/li&gt;
&lt;li&gt;二值化&lt;/li&gt;
&lt;li&gt;线性滤波器&lt;/li&gt;
&lt;li&gt;霍夫变换：着直线、找圆&lt;/li&gt;
&lt;li&gt;重映射&lt;/li&gt;
&lt;li&gt;仿射变换&lt;/li&gt;
&lt;li&gt;直方图&lt;/li&gt;
&lt;li&gt;模板匹配&lt;/li&gt;
&lt;li&gt;轮廓查找&lt;/li&gt;
&lt;li&gt;凸包&lt;/li&gt;
&lt;li&gt;多边形拟合&lt;/li&gt;
&lt;li&gt;矩&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>opencv（一）——HighGUI</title>
    <link href="http://yoursite.com/2018/08/14/opencv%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94HighGUI/"/>
    <id>http://yoursite.com/2018/08/14/opencv（一）——HighGUI/</id>
    <published>2018-08-14T12:46:25.000Z</published>
    <updated>2018-08-14T12:48:26.506Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HighGUI-模块"><a href="#HighGUI-模块" class="headerlink" title="HighGUI 模块"></a>HighGUI 模块</h2><p>PS：HighGUI 图形用户界面模块，包括图像和视频的读入显示、编码解码、图形交互界面接口</p><p>(啥意思？后来想想可能是指滑动条这种可以产生交互的函数)</p><p><strong>滑动条的使用</strong>                                                                                 1.创建窗口        namedWindow()                                                                                                                     2.创建滑动条    createTrackbar()                                                                        3.回掉函数        on_trackbar()  函数原型必须是 void XX(int,void*)  第一个为轨迹位置，第二个为用户数据，滑动条位置改变，调用回掉函数</p><p><br></p><p>PS：<strong>void* 表明该指针与一地址值有关，但不知道存储在此地址上的对象的类型</strong>                                              void*只支持有限的操作，<strong>向函数传递void<em>指针或者返回void\</em>指针，给另一个void*指针赋值</strong>                                                                 int i = 1   void *pi = &amp;i</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;HighGUI-模块&quot;&gt;&lt;a href=&quot;#HighGUI-模块&quot; class=&quot;headerlink&quot; title=&quot;HighGUI 模块&quot;&gt;&lt;/a&gt;HighGUI 模块&lt;/h2&gt;&lt;p&gt;PS：HighGUI 图形用户界面模块，包括图像和视频的读入显示、编码解码、
      
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（十一）——生成式对抗网络</title>
    <link href="http://yoursite.com/2018/08/14/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2018/08/14/cs231n笔记（十一）——生成式对抗网络/</id>
    <published>2018-08-14T05:57:10.000Z</published>
    <updated>2018-08-14T06:44:02.155Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>生成式对抗网络（GAN）</li><li>条件GAN（CGAN）</li><li>变分自编码器（Variational Auto-Encoder，VAE）</li></ol><a id="more"></a><p><br></p><h1 id="生成式对抗网络（GAN）"><a href="#生成式对抗网络（GAN）" class="headerlink" title="生成式对抗网络（GAN）"></a>生成式对抗网络（GAN）</h1><h2 id="一、GAN的思想"><a href="#一、GAN的思想" class="headerlink" title="一、GAN的思想"></a>一、GAN的思想</h2><p>GAN的思想是一种二人零和博弈思想（two-player game），博弈双方的利益之和是一个常数。GAN中有两个博弈者，一个人名字是生成模型（G），另一个人名字是判别模型（D）。</p><p>二、两个网络</p><p><strong>生成网络</strong>：输入为随机数，输出为生成数据。即造样本，使得自己造样本尽可能强，判别网络没法判断是真样本还是假样本。</p><p><strong>判别网络</strong>：二分类器，来判断输入的样本是真是假。（就是输出值大于0.5还是小于0.5）</p><p><br></p><h2 id="二、图像生成模型举例说明"><a href="#二、图像生成模型举例说明" class="headerlink" title="二、图像生成模型举例说明"></a>二、图像生成模型举例说明</h2><p>图片生成模型和判别模型之间的博弈：生成模型生成一些图片—&gt; 判别模型学习区分生成的图片和真实图片—&gt;生成模型根据判别模型改进自己，生成新的图片…… </p><p>直至判别模型无法判断一张图片是生成的还是真实的而结束，此时生成模型就会成为一个完美的模型。</p><p><img src="https://static.leiphone.com/uploads/new/article/740_740/201706/59350f1a80282.jpg?imageMogr2/format/jpg/quality/90" alt="最简单易懂的GAN（生成对抗网络）教程：从理论到实践（附代码）"></p><p><br></p><h2 id="三、GAN与传统机器学习的区别"><a href="#三、GAN与传统机器学习的区别" class="headerlink" title="三、GAN与传统机器学习的区别"></a>三、GAN与传统机器学习的区别</h2><p><strong>GAN强大之处在于可以自动学习原始真实样本集的数据分布</strong></p><p>传统机器学习方法，一般都会定义一个模型让数据去学习。这些方法都在直接或者间接的告诉数据该怎么映射，只是不同的映射方法能力不一样。</p><p>假设我们知道原始数据属于高斯分布呀，只是不知道高斯分布的参数，这个时候我们定义高斯分布，然后利用数据去学习高斯分布的参数得到我们最终的模型。</p><p>再比如说定义一个分类器 SVM，然后强行让数据进行各种高维映射，最后可以变成一个简单的分布，SVM 也是给了一个模型，这个模型就是核映射（径向基函数等等），其实是核映射的参数的学习。</p><p>GAN，生成模型则是通过噪声生成一个完整的真实数据（比如人脸），然而最开始时，生成模型并不知道从随机噪声到人脸数据的分布规律</p><p><br></p><p><br></p><h1 id="条件GAN（CGAN）"><a href="#条件GAN（CGAN）" class="headerlink" title="条件GAN（CGAN）"></a>条件GAN（CGAN）</h1><p>针对 GAN <strong>不能生成具有特定属性的图片</strong>的问题，提出了CGAN</p><p>其核心在于将属性信息融入生成器和判别器中，属性可以是任何标签信息，例如图像的类别、人脸图像的面部表情等。</p><p><br></p><p><br></p><h1 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h1><h1 id="（Variational-Auto-Encoder，VAE）"><a href="#（Variational-Auto-Encoder，VAE）" class="headerlink" title="（Variational Auto-Encoder，VAE）"></a>（Variational Auto-Encoder，VAE）</h1><h2 id="一、自动编码器（Auto-Encoder）"><a href="#一、自动编码器（Auto-Encoder）" class="headerlink" title="一、自动编码器（Auto Encoder）"></a>一、自动编码器（Auto Encoder）</h2><p>最开始作为一种数据的压缩方法，现在主要应用有数据去噪，可视化降维，生成数据</p><p>编码器将数据分布的高级特征映射到数据的低级表征，低级表征叫作隐含向量（latent vector），解码器吸收数据的低级表征，然后输出同样数据的高级表征。X 作为编码器的输入，z 作为本隐含向量，X′作为解码器的输出。</p><p><br></p><p><strong>自动编码器与GAN相比：</strong></p><ul><li>GAN：生成图片使用的是随机高斯噪声，这意味着没法决定使用哪种随机噪声产生需要的特定图片</li><li>自动编码器：能够<strong>通过选择特定的噪声来生成想要生成的图片</strong>，相当于知道每种图片对应的噪声分布</li></ul><p><br></p><h2 id="二、变分自动编码器-Variational-Autoencoder"><a href="#二、变分自动编码器-Variational-Autoencoder" class="headerlink" title="二、变分自动编码器(Variational Autoencoder)"></a>二、变分自动编码器(Variational Autoencoder)</h2><p>变分编码器是自动编码器的升级版本，也由编码器和解码器构成</p><p><br></p><p><strong>变分自编码器与自动编码器的比较相比：</strong></p><ul><li>自动编码器：输入一张图片，然后将一张图片编码之后得到一个<strong>隐含向量</strong>，这比随机取一个随机噪声更好，因为这包含着原图片的信息，然后<strong>隐含向量解码得到与原图片对应的照片</strong>。缺点是不能任意生成图片，因为没有办法自己去构造隐藏向量，<strong>需要通过一张图片输入编码才知道隐含向量是什么</strong>。</li><li>变分自动编码器：只需要在编码过程给它增加一些限制，<strong>使生成的隐含向量能够粗略的遵循某分布</strong>，这样通过解码器就能够生成我们想要的图片，而不需要给它一张原始图片的编码。</li></ul><p><br></p><p><br></p><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>将文本翻译成图像，使用自然语言的描述属性生成相应的图像。</p><p><img src="https://pic.36krcnd.com/201802/10074355/g1oyov9niu9gebz3.jpeg!1200" alt="生成对抗网络 GAN：让 AI 有创造力，机器学习十年来最激动人心的点子"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;生成式对抗网络（GAN）&lt;/li&gt;
&lt;li&gt;条件GAN（CGAN）&lt;/li&gt;
&lt;li&gt;变分自编码器（Variational Auto-Encoder，VAE）&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（十）——反卷积与特征可视化</title>
    <link href="http://yoursite.com/2018/08/14/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%E5%8F%8D%E5%8D%B7%E7%A7%AF%E4%B8%8E%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://yoursite.com/2018/08/14/cs231n笔记（十）——反卷积与特征可视化/</id>
    <published>2018-08-14T05:50:36.000Z</published>
    <updated>2018-08-14T05:56:26.231Z</updated>
    
    <content type="html"><![CDATA[<p>PS：纠结了一个晚上，但到底可视化的是什么 ，一开始觉得可视化的是学习到的参数，好吧，我傻了……</p><a id="more"></a><p><br></p><h2 id="一、可视化的特征"><a href="#一、可视化的特征" class="headerlink" title="一、可视化的特征"></a>一、可视化的特征</h2><p>正常卷积的过程  conv—ReLu—Pooling，而为了理解特定的 Pooling 值代表什么，先把其他Pooling 值设为0，利用 Deconvnet 把这个给定的激活值映射到初始像素层，得到的特征，即可视化想要的结果</p><p><br></p><h2 id="二、反卷积过程"><a href="#二、反卷积过程" class="headerlink" title="二、反卷积过程"></a>二、反卷积过程</h2><p>反卷积又被称为Transposed（转置）Convolution，其实卷积层的前向传播过程就是反卷积层的反向传播过程，卷积层的反向传播过程就是反卷积层的前向传播过程。</p><h3 id="1-Unpooling"><a href="#1-Unpooling" class="headerlink" title="1.Unpooling"></a>1.Unpooling</h3><p>由于pooling是不可逆的，所以unpooling只是正常pooling的一种近似</p><h3 id="2-Recitfication"><a href="#2-Recitfication" class="headerlink" title="2.Recitfication"></a>2.Recitfication</h3><p>通过ReLU函数变换unpooling特征</p><h3 id="3-Filtering"><a href="#3-Filtering" class="headerlink" title="3.Filtering"></a>3.Filtering</h3><p>利用卷积过程filter的转置（实际上就是水平和数值翻转filter）版本来计算卷积前的特征图；从而形成重构的特征。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：纠结了一个晚上，但到底可视化的是什么 ，一开始觉得可视化的是学习到的参数，好吧，我傻了……&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（九）——分类、检测、分割、跟踪</title>
    <link href="http://yoursite.com/2018/08/13/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB%E3%80%81%E6%A3%80%E6%B5%8B%E3%80%81%E5%88%86%E5%89%B2%E3%80%81%E8%B7%9F%E8%B8%AA/"/>
    <id>http://yoursite.com/2018/08/13/cs231n笔记（九）——分类、检测、分割、跟踪/</id>
    <published>2018-08-13T09:57:35.000Z</published>
    <updated>2018-08-13T10:00:42.905Z</updated>
    
    <content type="html"><![CDATA[<p>PS：其实一直没有认真理解计算机视觉可以解决的问题，得把这些任务区分一些</p><a id="more"></a><p><br></p><ol><li><p>图像分类（Image Classification）</p><p>给定一张输入图像，判断该图像所属类别</p><p><br></p></li><li><p>目标定位（Object Localization）</p><p>在图像分类的基础上，确定目标具体在图像的什么位置，通常是以<strong>包围盒的(bounding box)</strong>形式</p><p><br></p></li><li><p>目标识别（Object recognition）</p><p><strong>只有目标和非目标两个类别</strong>，找到目标所在的矩形框</p><p>例如，人脸检测（人脸为目标、背景为非目标）、汽车检测（汽车为目标、背景为非目标） </p><p><br></p></li><li><p>目标检测（Object Detection）</p><p>检测图像中<strong>所有的目标</strong>，得到检测到的目标的矩形框，并对所有检测到的矩形框进行分类 </p><p><br></p><p><strong>多任务学习中，网络有两个输出分支：</strong></p><p>一个分支用于做图像分类，即全连接+softmax判断目标类别，和单纯图像分类区别在于这里还另外需要一个<strong>『“背景”』</strong>类</p><p>另一个分支用于判断目标位置，即完成回归任务输出四个数字标记包围盒位置，该分支输出结果<strong>只有在分类分支判断不为“背景”时才使用</strong></p><p><br></p></li><li><p>语义分割（Semantic Segmentation）</p><p>分割是目标检测更进阶的任务，目标检测只需要框出每个目标的包围盒，分割需要进一步判断图像中哪些像素属于哪个目标</p><p>对图片中的每一个像素点进行分类，相同的类别进行标注，但是<strong>同一物体的不同实例不需要单独分割</strong></p><p><br></p></li><li><p>实例分割(Instance Segmentation)</p><p>对图像中的每一个像素点进行分类，<strong>同种物体的不同实例也用不同的类标进行标注</strong></p><p><br></p></li><li><p>目标跟踪（Object Tracking）</p><p>跟踪是基于检测的,必须先定位目标,才能后续跟踪</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：其实一直没有认真理解计算机视觉可以解决的问题，得把这些任务区分一些&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>嵌入式与各种板子</title>
    <link href="http://yoursite.com/2018/08/05/%E5%B5%8C%E5%85%A5%E5%BC%8F%E4%B8%8E%E5%90%84%E7%A7%8D%E6%9D%BF%E5%AD%90/"/>
    <id>http://yoursite.com/2018/08/05/嵌入式与各种板子/</id>
    <published>2018-08-05T09:19:51.000Z</published>
    <updated>2018-08-15T02:41:42.990Z</updated>
    
    <content type="html"><![CDATA[<p>PS：写完这个，我也搞不懂MCU、ARM、DSP、FPGA、SOC到底有什么区别，我真的放弃硬件了</p><a id="more"></a><h2 id="一、嵌入式"><a href="#一、嵌入式" class="headerlink" title="一、嵌入式"></a>一、嵌入式</h2><p>嵌入式：在硬件和软件上都只保留需要的部分，而将不需要的部分裁去。所以嵌入式系统一般都具有便携、低功耗、性能单一等特性。</p><p><br></p><h2 id="二、MCU、ARM、DSP、FPGA、SOC"><a href="#二、MCU、ARM、DSP、FPGA、SOC" class="headerlink" title="二、MCU、ARM、DSP、FPGA、SOC"></a>二、MCU、ARM、DSP、FPGA、SOC</h2><ul><li>MCU（单片机）</li><li><strong>ARM属于MCU</strong>，ARM其实应该叫<strong>ARM架构</strong>，ARM之所以在<strong>移动市场</strong>上得到极大的成功，其中最主要的原因便是它的低功耗</li><li>DSP（数字信号处理器）：单芯片成本较高，主要还是应用于对计算能力要求高的应用</li><li>FPGA（现场可编程逻辑阵列）：想要它有什么功能完全靠编程人员设计，所有过程都是硬件，包括VHDL和Verilog HDL程序设计也是硬件范畴</li><li>SOC（片上系统）</li></ul><p><br></p><h2 id="三、ZYNQ"><a href="#三、ZYNQ" class="headerlink" title="三、ZYNQ"></a>三、ZYNQ</h2><p>ZYNQ：Xilinx推出的行业第一个可扩展处理平台ZYNQ系列</p><p>旨在为视频监视、汽车驾驶员辅助以及工厂自动化等高端嵌入式应用提供所需的处理与计算性能水平。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：写完这个，我也搞不懂MCU、ARM、DSP、FPGA、SOC到底有什么区别，我真的放弃硬件了&lt;/p&gt;
    
    </summary>
    
    
      <category term="硬件" scheme="http://yoursite.com/tags/%E7%A1%AC%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Tensorboard 总结</title>
    <link href="http://yoursite.com/2018/08/05/Tensorboard-%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/08/05/Tensorboard-总结/</id>
    <published>2018-08-05T07:26:21.000Z</published>
    <updated>2018-08-05T07:27:26.099Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h2><p><a href="https://blog.csdn.net/u010099080/article/details/77426577" target="_blank" rel="noopener">https://blog.csdn.net/u010099080/article/details/77426577</a></p><p><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html" target="_blank" rel="noopener">http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html</a></p><p><br></p><a id="more"></a><h3 id="1-Web端的7个栏目"><a href="#1-Web端的7个栏目" class="headerlink" title="1.Web端的7个栏目"></a>1.Web端的7个栏目</h3><p>SCALARS ： <code>tf.summary.scalar()</code></p><p>记录诸如准确率accuracy、损失loss和学习率learning  rate等单个值的变化趋势</p><p>IMAGES ：图像信息</p><p>AUDIO ：声音的信息</p><p>GRAPHS ：展示构建的网络整体结构</p><p>DISTRIBUTIONS ：展示网络中各参数随训练步数的增加的变化情况</p><p>HISTOGRAMS ：也是展示网络中各参数的变化，和 DISTRIBUTIONS 是对同一数据不同方式的展现</p><p>EMBEDDINGS ：展示的是嵌入向量</p><p><br></p><h3 id="2-代码流程"><a href="#2-代码流程" class="headerlink" title="2.代码流程"></a>2.代码流程</h3><p>TensorBoard 通过读取 TensorFlow 的事件文件来运行</p><ol><li>使用summary定义各种图SCALAR、GRAPHS等</li><li>将所有summary合并    <code>merge_summary = tf.summary.merge_all()</code></li><li>将合并后的summary放入磁盘文件   <code>writer = tf.summary.FileWriter(dir,sess.graph)</code></li><li>每隔一定的训练间隔需要run使得各种summary真正的生成</li><li><code>train_summary = sess.run(merge_summary,feed_dict =  {...})</code></li><li>并且将每隔一定的训练间隔将各种summary放入磁盘文件</li><li><code>writer.add_summary(train_summary,step)</code></li></ol><p>PS ：通过<strong>名称域</strong>把节点分组来得到可读性高的图表很关键 <code>with tf.name_scope(&#39; &#39;) as scope:</code></p><p><br></p><h3 id="3-web端查看生成的文件"><a href="#3-web端查看生成的文件" class="headerlink" title="3.web端查看生成的文件"></a>3.web端查看生成的文件</h3><ol><li>在<code>writer = tf.summary.FileWriter(&quot;logs/&quot;, sess.graph)</code>目录下面找到生成的文件</li><li>打开终端，进入文件目录的<strong>上层文件夹</strong> <code>tensorboard --logdir=logs</code></li><li>执行命令之后会出现网址，将网址在浏览器中打开就可以看到定义的可视化信息</li></ol><p><br></p><h3 id="4-刷新-tensorboard-注意"><a href="#4-刷新-tensorboard-注意" class="headerlink" title="4.刷新 tensorboard 注意"></a>4.刷新 tensorboard 注意</h3><ol><li>同时运行 Jupyter 服务器和 TensorBoard 服务器，需对每个服务器使用不同的端口</li><li>更新tensorboard中的计算图，将内存中以前的数据，清理掉</li></ol><ol><li><p>去logs目录下把文件删除   jupyter–&gt;kernel–&gt;<strong>restart</strong>&amp;run all  </p><p>（<strong>restart</strong> 很重要，我估计是起到了清理数据的作用，我之前一直在，重新run，结果图越来越多）</p></li><li><p>把原<strong>来的cmd命令行窗口关闭</strong>，重新执行启动tensorboard命令</p></li></ol><table><thead><tr><th>符号</th><th>意义</th></tr></thead><tbody><tr><td><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/namespace_node.png" alt="名称域"></td><td><em>High-level</em>节点代表一个名称域，双击则展开一个高层节点。</td></tr><tr><td><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/horizontal_stack.png" alt="断线节点序列"></td><td>彼此之间不连接的有限个节点序列。</td></tr><tr><td><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/vertical_stack.png" alt="相连节点序列"></td><td>彼此之间相连的有限个节点序列。</td></tr><tr><td><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/op_node.png" alt="操作节点"></td><td>一个单独的操作节点。</td></tr><tr><td><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/constant.png" alt="常量节点"></td><td>一个常量结点。</td></tr><tr><td><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/summary.png" alt="摘要节点"></td><td>一个摘要节点。</td></tr><tr><td><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/dataflow_edge.png" alt="数据流边"></td><td>显示各操作间的数据流边。</td></tr><tr><td><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/control_edge.png" alt="控制依赖边"></td><td>显示各操作间的控制依赖边。</td></tr><tr><td><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/reference_edge.png" alt="引用边"></td><td>引用边，表示出度操作节点可以使入度tensor发生变化。</td></tr></tbody></table><p><br></p><h2 id="5-name-scope-variable-scope"><a href="#5-name-scope-variable-scope" class="headerlink" title="5.name_scope  variable_scope"></a>5.name_scope  variable_scope</h2><p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-12-scope/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-12-scope/</a></p><h3 id="1-tf-name-scope"><a href="#1-tf-name-scope" class="headerlink" title="(1) tf.name_scope()"></a>(1) tf.name_scope()</h3><p>生成变量 variable的两种方法：</p><ul><li><code>tf.get_variable()</code>：不会被<code>tf.name_scope()</code>当中的名字所影响</li></ul><ul><li><code>tf.Variable()</code>：若将变量的<code>name</code> 定义成一样，但<strong>实际上生成了新的变量</strong>，变量的名字是不一样的</li></ul><h3 id="2-tf-variable-scope"><a href="#2-tf-variable-scope" class="headerlink" title="(2) tf.variable_scope()"></a>(2) tf.variable_scope()</h3><ul><li><code>tf.get_variable()</code>：若将变量的<code>name</code> 定义成一样，<strong>会报错</strong></li></ul><ul><li><code>tf.Variable()</code>：若将变量的<code>name</code> 定义成一样，但是Tensorflow 输出的变量名其实并不一样</li></ul><h3 id="3-重复利用变量"><a href="#3-重复利用变量" class="headerlink" title="(3) 重复利用变量"></a>(3) 重复利用变量</h3><p>重复使用同样名字的变量：</p><p><code>tf.variable_scope()</code>+ <code>tf.get_variable()</code>+ <code>scope.reuse_variables()</code></p><p>（而且不会像 <code>tf.Variable()</code> 每次都会产生新的变量）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"a_variable_scope"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    var3 =tf.get_variable(name=<span class="string">'var3'</span>,shape[<span class="number">1</span>],dtype=tf.float32,initializer=initializer)</span><br><span class="line">    scope.reuse_variables()</span><br><span class="line">    var3_reuse = tf.get_variable(name=<span class="string">'var3'</span>,)</span><br></pre></td></tr></table></figure><p>重复使用同样名字的变量在<strong>RNN</strong>中比较常用</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Tensorboard&quot;&gt;&lt;a href=&quot;#Tensorboard&quot; class=&quot;headerlink&quot; title=&quot;Tensorboard&quot;&gt;&lt;/a&gt;Tensorboard&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/u010099080/article/details/77426577&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/u010099080/article/details/77426577&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 总结</title>
    <link href="http://yoursite.com/2018/08/05/TensorFlow-%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/08/05/TensorFlow-总结/</id>
    <published>2018-08-05T07:24:09.000Z</published>
    <updated>2018-08-05T07:27:36.898Z</updated>
    
    <content type="html"><![CDATA[<p>TensorFlow 学习教程分享</p><ol><li>莫烦大神的视频，他讲的比较浅，适合入门学习者</li><li><a href="https://blog.csdn.net/xierhacker/article/details/53103979" target="_blank" rel="noopener">https://blog.csdn.net/xierhacker/article/details/53103979</a></li><li>cs20si</li></ol><a id="more"></a><p><br></p><h2 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h2><h3 id="1-关于-TensorFlow-可以加速"><a href="#1-关于-TensorFlow-可以加速" class="headerlink" title="1. 关于 TensorFlow 可以加速"></a>1. 关于 TensorFlow 可以加速</h3><p>TensorFlow提出的计算图概念，是为了更高效运行。计算图只是定义了变量以及相应的运算，但是不包含具体数据，只是在运行阶段再给计算图中的变量赋值并运算。好处是可以<strong>将复杂的运算放在Python外执行</strong>（可以调用高效的C代码实现），而 <strong>Python代码只是负责定义运算</strong>。</p><p>python外执行的理解：</p><p>很多函数 <strong>底层用别的语言实现，但是提供了Python函数接口</strong>。比如计算幂函数，Python的math库本身有pow函数，用Python实现，numpy的pow函数，用C实现，两个的计算速度不是一个量级。</p><p><br></p><h3 id="2-TensorFlow-架构"><a href="#2-TensorFlow-架构" class="headerlink" title="2. TensorFlow 架构"></a>2. TensorFlow 架构</h3><p>参考博客：<a href="https://www.jianshu.com/p/667cbb20d802" target="_blank" rel="noopener">https://www.jianshu.com/p/667cbb20d802</a></p><h4 id="（1）TensorFlow-的系统结构"><a href="#（1）TensorFlow-的系统结构" class="headerlink" title="（1）TensorFlow 的系统结构"></a>（1）TensorFlow 的系统结构</h4><p>ensorFlow 的系统结构以C API为界，将整个系统分为「前端」和「后端」两个子系统：</p><ul><li>前端系统：提供编程模型，负责构造计算图</li><li>后端系统：提供运行时环境，负责执行计算图</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/2254249-bf86142555d23538.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><br></p><h4 id="（2）四个组件"><a href="#（2）四个组件" class="headerlink" title="（2）四个组件"></a>（2）四个组件</h4><ul><li><p><strong>Client</strong></p><p>前端系统的主要组成部分，<strong>提供计算图</strong>的编程模型，支持<strong>多语言编程</strong>环境程模型</p><p>Client通以Session为桥梁，连接后端，启动计算图</p><p><br></p></li></ul><ul><li><p><strong>Master</strong></p><ul><li>在分布式的运行时环境中，Distributed Master根据Session.run的Fetching参数，从计算图中反向遍历，找到所依赖的「最小子图」</li><li>然后，Distributed Master将该「子图」分裂为多个「子图片段」，以便在不同的进程和设备上运行「子图片段」</li><li>最后，Distributed Master将这些「子图片段」派发给Work Service，Worker Service成为「本地子图」</li></ul><p><br></p></li><li><p><strong>Worker</strong></p><ul><li><p>处理来自Master的请求</p></li><li><p>执行本地子图</p></li><li><p>协同任务之间的数据通信</p><p>Worker Service派发OP到本地设备，执行特定的Kernel。它将尽最大可能地利用多CPU/GPU的处理能力，并发地执行Kernel</p><p><br></p></li></ul></li></ul><ul><li><p><strong>Kernel</strong></p><p>每一个OP根据设备类型都会存在一个优化了的Kernel实现，运行时根据本地设备的类型，<strong>为OP选择特定的Kernel实现</strong>，完成OP的计算</p></li></ul><p><br></p><h3 id="3-多语言编程"><a href="#3-多语言编程" class="headerlink" title="3.多语言编程"></a>3.多语言编程</h3><p>tensorflow 使用 <strong>bazel</strong> 进行自动化编译管理</p><p>在编译之前需要启动 <strong>Swig</strong> ：Swig是一种让脚本语言调用C/C++接口的工具</p><p>Swig通过<code>tf_session.i</code>自动生成了两个文件：</p><ul><li><p><code>pywrap_tensorflow.py</code> ：对接上层Python调用</p><p><code>pywrap_tensorflow.py</code>模块首次被加载时，自动地加载<code>_pywrap_tensorflow.so</code>的动态链接库。从而实现<code>pywrap_tensorflow.py</code>到<code>pywrap_tensorflow.cpp</code>的对应函数调用</p><p><br></p></li><li><p><code>pywrap_tensorflow.cpp</code> ：对接下层C实现</p><p>在<code>pywrap_tensorflow.cpp</code>的实现中，静态注册了一个函数符号表，按照Python的函数名称，匹配找到对应的C函数实现，最终转调到<code>c_api.c</code>的具体实现</p></li></ul><p><img src="https://upload-images.jianshu.io/upload_images/2254249-6b09734b9eb93e83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/656" alt="img"></p><p><br></p><h3 id="4-TensorFlow-的缺点"><a href="#4-TensorFlow-的缺点" class="headerlink" title="4. TensorFlow 的缺点"></a>4. TensorFlow 的缺点</h3><p>TensorFlow<strong>想要动态修改计算图比较困难</strong>。比如训练机器翻译或者聊天机器人的模型，句子长度不一样，计算图其实是不一样的。TensorFlow发布了一个工具，TensorFlow Fold，可以相对方便的动态修改计算图。不过总的来说，TensorFlow在计算图的设计灵活性上还是有些欠缺。</p><p><br></p><p><br></p><h2 id="一、Graph和Session"><a href="#一、Graph和Session" class="headerlink" title="一、Graph和Session"></a>一、Graph和Session</h2><p>TensorFlow的程序一般分为两个部分：</p><ol><li>构建计算图</li><li>运行计算图</li></ol><h3 id="（一）Graph"><a href="#（一）Graph" class="headerlink" title="（一）Graph"></a>（一）Graph</h3><p><a href="https://morvanzhou.github.io/static/results/tensorflow/1_4_1.png" target="_blank" rel="noopener"><img src="https://morvanzhou.github.io/static/results/tensorflow/1_4_1.png" alt="处理结构"></a></p><p>TensorFlow 使用Graph（数据流图）来描述计算的过程。而<strong>图必须在会话里被启动</strong>，会话将图的<strong>节点op</strong>使用特定的Kernel实现，分发到诸如 CPU 或 GPU 之类的设备上, 同时提供执行 op 的方法。</p><p>执行结果： </p><p>在 Python 中, 返回的 tensor 是 numpy ndarray </p><p>在 C 和 C++ 语言中, 返回的 tensor 是 tensorflow::Tensor </p><p><br></p><p>一旦开始任务，就<strong>已经有一个默认的图已经创建好了</strong>。而且可以通过调用<code>tf.get_default_graph()</code>来访问，只需要添加操作到默认的图里面，也可以新建一个图，向里面添加操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()            <span class="comment">#创建新的graph</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> g.as_default():       <span class="comment">#上下文管理器,使得g成为当前默认的graph</span></span><br><span class="line">  <span class="keyword">with</span> g.device(<span class="string">'/gpu:0'</span>): <span class="comment">#指定添加的op使用什么设备</span></span><br><span class="line">    b = tf.constant([<span class="number">1</span>])   <span class="comment">#向g图里面添加操作</span></span><br></pre></td></tr></table></figure><p><br></p><h3 id="（二）会话"><a href="#（二）会话" class="headerlink" title="（二）会话"></a>（二）会话</h3><p>PS：会话启动图的两种方法 Session 和 InteractiveSession</p><h4 id="1-Session"><a href="#1-Session" class="headerlink" title="1. Session"></a>1. Session</h4><p>1.将指定的图投放到session里</p><p><code>tf.Session.__init__(target=”, graph=None, config=None)</code></p><p>2.运行节点操作（op）得到tensor</p><p><code>tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)</code></p><ul><li><strong>fetch：需要取出来的tensor</strong></li><li><strong>feed：需要传入的tensor</strong></li></ul><p>3.session不再用的话，就要调用 <code>sess.close()</code> 函数来释放资源，或者更方便的方法，也可以用上下文管理器</p><p><br></p><h4 id="2-InteractiveSession"><a href="#2-InteractiveSession" class="headerlink" title="2.  InteractiveSession"></a>2.  InteractiveSession</h4><p>1.InteractiveSession 代替 Session 类，避免使用一个变量 sess 来持有会话</p><p>2.代替的原理：<code>tf.InteractiveSession()</code> <strong>加载它自身作为默认构建的session</strong>，而 <code>tensor.eval()</code>和 <code>operation.run()</code> 取决于默认的session</p><p>3.代替 Session.run() 的操作</p><p>tensor的操作使用：<code>tensor.eval()</code></p><p>op的操作使用：<code>operation.run()</code></p><p><br></p><p><br></p><h2 id="二、变量、常量、placeholder"><a href="#二、变量、常量、placeholder" class="headerlink" title="二、变量、常量、placeholder"></a>二、变量、常量、placeholder</h2><h3 id="（一）类"><a href="#（一）类" class="headerlink" title="（一）类"></a>（一）类</h3><h4 id="1-Tensor"><a href="#1-Tensor" class="headerlink" title="1.Tensor"></a>1.Tensor</h4><ul><li>零阶张量为 标量 (scalar) 也就是一个数值. 比如 <code>[1]</code></li><li>一阶张量为 向量 (vector), 比如 一维的 <code>[1, 2, 3]</code></li><li>二阶张量为 矩阵 (matrix), 比如 二维的 <code>[[1, 2, 3],[4, 5, 6],[7, 8, 9]]</code></li><li>还有 三阶 三维的 …</li></ul><p><strong>tensor里面并不负责储存值</strong>，想要得到值，得去Session中run，所以需要构造了一个Session的对象用来执行图<code>sess=tf.Session()</code> 。</p><p>得到Tensor的值：</p><p>1 <code>Seesion.run（）</code></p><p>2 <code>t.eval（session=sess）</code>  t是Tensor的名字，session=sess属于的会话</p><p><br></p><h4 id="2-Variable"><a href="#2-Variable" class="headerlink" title="2.Variable"></a>2.Variable</h4><p>变量<strong>需要初始值</strong> （特别重要：变量的初始化）可以是一个任何类型任何形状的Tensor，值可以通过assign一个tensor来改变变量。</p><p><br></p><h3 id="（二）函数"><a href="#（二）函数" class="headerlink" title="（二）函数"></a>（二）函数</h3><h4 id="1-constant"><a href="#1-constant" class="headerlink" title="1.constant()"></a>1.constant()</h4><p>创建一个常量tensor </p><p><br></p><h4 id="2-placeholder"><a href="#2-placeholder" class="headerlink" title="2.placeholder()"></a>2.placeholder()</h4><p>placeholder的作用为占个位置，不知道具体值，但是知道类型和形状等等一些信息，然后用feed的方式来把这些数据“填”进去。</p><p>PS：我的理解，需要定义变量是变量，常量是常量。变量、常量、placeholder都是tensor，只是用于网络中的不同地方。</p><p><br></p><p><br></p><h2 id="三、shape-形状"><a href="#三、shape-形状" class="headerlink" title="三、shape 形状"></a>三、shape 形状</h2><p><strong>tf.shape()</strong> —— tensor的形状</p><p><strong>tf.size()</strong> —— 元素的总数</p><p><strong>tf.reshape()</strong> —— 改变tensor的形状</p><p><br></p><p><strong>tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)</strong></p><p>计算tensor的某个维度上面元素的和，缩减维度</p><p><strong>tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)</strong></p><p>计算tensor的某个维度上面元素的平均值，缩减维度</p><p><br></p><p>维度：几层括号</p><p>轴axis：多维数组的索引</p><p>举个例子 ：三维数组  a=[ [[1,2], [3,4]], [[5,6], [7,8]] ]</p><p>a[0]= [[1,2], [3,4]]</p><p>a[0,1,0]=3</p><p><br></p><p><br></p><h2 id="四、随机数"><a href="#四、随机数" class="headerlink" title="四、随机数"></a>四、随机数</h2><p>主要用于网络权重初始化</p><p>1.<strong>正态分布</strong>填充得到tensor</p><p><strong>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</strong></p><p><code>var = tf.Variable(tf.random_normal([2, 3], mean=-1, stddev=4))</code></p><p>2.<strong>均匀分布</strong>的随机数</p><p><strong>tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)</strong></p><p>3.将第一维度重新<strong>洗牌</strong></p><p><strong>tf.random_shuffle(value, seed=None, name=None)</strong></p><p><br></p><p><br></p><h2 id="五、读取数据"><a href="#五、读取数据" class="headerlink" title="五、读取数据"></a>五、读取数据</h2><p>程序读取数据一共有3种方法:</p><p>1.预加载数据</p><p>将<strong>数据直接内嵌到Graph中</strong>（在Graph创建constant），再把Graph传入Session中运行</p><p>2.供给数据 feed </p><p>用占位符替代数据，待运行的时候<code>feed_dict</code>填充数据</p><p>PS：上面两种方法的效率很低</p><p><br></p><p><strong>3.从文件读取数据</strong></p><p><strong>不同的文件格式：</strong></p><p><strong>（1）CSV文件</strong></p><p><strong>（2）二进制文件</strong></p><p><strong>（3）图像文件</strong></p><p><strong>（4）TFRecords文件： TFRecords 数据文件是一种将图像数据和标签统一存储的二进制文件</strong></p><p><a href="https://blog.csdn.net/m0_37407756/article/details/80672192" target="_blank" rel="noopener">https://blog.csdn.net/m0_37407756/article/details/80672192</a></p><p><br></p><p><br></p><h2 id="六、队列和线程"><a href="#六、队列和线程" class="headerlink" title="六、队列和线程"></a>六、队列和线程</h2><p>对应上一节的 <strong>从文件读取数据</strong> ，不过是多线程</p><p>PS：我就把它简单看作多线程的并行，每个线程占用一个CPU，不考虑并发</p><p><br></p><p><strong>为什么需要多线程？</strong></p><p>深度学习的模型训练过程往往需要大量的数据，而数据读取涉及磁盘操作，速度慢。Tensorflow 的计算主要使用CPU/GPU和内存，速度快。因此通常会使用多个线程读取数据，然后使用一个线程计算数据。</p><p><a href="https://blog.csdn.net/chaipp0607/article/details/72924572" target="_blank" rel="noopener">https://blog.csdn.net/chaipp0607/article/details/72924572</a></p><p><br></p><h3 id="（一）队列"><a href="#（一）队列" class="headerlink" title="（一）队列"></a>（一）队列</h3><p>队列是为了数据读取的有序性</p><p>操作队列的函数主要有：</p><p><strong>FIFOQueue()——创建一个先入先出（FIFO）的队列</strong><br><strong>RandomShuffleQueue()——创建一个随机出队的队列</strong><br><strong>enqueue_many()——初始化队列中的元素</strong><br><strong>dequeue()——出队</strong><br><strong>enqueue()——入队</strong></p><p><br></p><h4 id="1-FIFOQueue-先入先出的队列"><a href="#1-FIFOQueue-先入先出的队列" class="headerlink" title="1.FIFOQueue : 先入先出的队列"></a>1.FIFOQueue : 先入先出的队列</h4><p>循环神经网络能够记住样本的先后次序，所以希望读入的训练样本是有序的</p><p>在使用循环神经网络时,希望读入的训练样本是有序的,就要用到FIFOQueue</p><h4 id="2-RandomShuffleQueue：随机队列"><a href="#2-RandomShuffleQueue：随机队列" class="headerlink" title="2.RandomShuffleQueue：随机队列"></a>2.RandomShuffleQueue：随机队列</h4><p>使用CNN的网络结构，训练图像样本时，希望可以无序的读入训练样本</p><p><br></p><h3 id="（二）线程"><a href="#（二）线程" class="headerlink" title="（二）线程"></a>（二）线程</h3><p>tensorFlow提供了两个类来帮助多线程的实现，从设计上这两个类必须被一起使用。</p><p><code>Coordinator</code>  协调器，协调线程间的关系</p><p><code>QueueRunner</code>    队列管理器， 用来协调多个线程同时将多个张量推入同一个队列中</p><p><br></p><p><br></p><h2 id="七、模型保存"><a href="#七、模型保存" class="headerlink" title="七、模型保存"></a>七、模型保存</h2><h3 id="1-Saver"><a href="#1-Saver" class="headerlink" title="1.Saver"></a>1.Saver</h3><p>saver 的使用：<a href="https://blog.csdn.net/u011500062/article/details/51728830" target="_blank" rel="noopener">https://blog.csdn.net/u011500062/article/details/51728830</a></p><h3 id="2-checkpoints文件"><a href="#2-checkpoints文件" class="headerlink" title="2.checkpoints文件"></a>2.checkpoints文件</h3><ol><li><p>Checkpoints文件是一个二进制文件，它把变量名映射到对应的tensor值 </p></li><li><p>Saver类提供了向checkpoints文件保存和从checkpoints文件中恢复变量的相关方法</p></li><li><p>为了避免填满整个磁盘，Saver可以自动的管理Checkpoints文件，可以指定保存最近的N个Checkpoints文件</p><p><br></p></li></ol><h3 id="3-生成的文件"><a href="#3-生成的文件" class="headerlink" title="3.生成的文件"></a>3.生成的文件</h3><p>.meta文件保存了当前图结构</p><p>.index文件保存了当前参数名</p><p>.data文件保存了当前参数值</p><p><br></p><p><strong>注意：</strong></p><p>如果不给<code>tf.train.Saver()</code>传入任何参数，那么saver将<strong>处理graph中的所有变量</strong>。其中每一个变量都以变量创建时传入的名称被保存。</p><p>可以创建多个saver，来保存和恢复模型变量的<strong>不同子集</strong>，同一个变量可被列入多个saver对象中</p><p>如果仅在session开始时restore模型变量的一个子集初始化变量，你需要对剩下的变量执行初始化</p><p>有时候在检查点文件中明确定义变量的名称很有用，特别是，<strong>仅保存和恢复模型的一部分变量</strong>时。比如，训练得到一个模型，其中有个变量命名为<code>&quot;weights&quot;</code>，想把它的值恢复到一个新的变量<code>&quot;params&quot;</code>中，这时变量的名称很有用。</p><p><br></p><p><br></p><h2 id="八、神经网络”组件”"><a href="#八、神经网络”组件”" class="headerlink" title="八、神经网络”组件”"></a>八、神经网络”组件”</h2><h3 id="（一）激活函数"><a href="#（一）激活函数" class="headerlink" title="（一）激活函数"></a>（一）激活函数</h3><p>常用的激活函数  <a href="https://blog.csdn.net/xierhacker/article/details/71524713" target="_blank" rel="noopener">https://blog.csdn.net/xierhacker/article/details/71524713</a></p><p><strong>tf.nn.relu</strong><br><strong>tf.nn.dropout</strong><br><strong>tf.sigmoid</strong><br><strong>tf.tanh</strong></p><p><br></p><h3 id="（二）损失函数"><a href="#（二）损失函数" class="headerlink" title="（二）损失函数"></a>（二）损失函数</h3><ul><li><strong>tf.nn.softmax</strong> —— 用于多分类</li></ul><ul><li><p><strong>tf.nn.softmax_cross_entropy_with_logits</strong> ——logits=W*X+b和labels=y 之间计算softmax交叉熵</p><p><br></p></li></ul><p><strong>交叉熵（Cross Entropy）</strong> ：是Loss函数的一种，用于描述模型<strong>预测值与真实值的差距大小</strong>。交叉熵是正的，并且当输入$x$的输出越接近期望输出$y$的话，交叉熵的值将会接近0</p><p>逻辑回归的loss function：<br>$$<br>C=−1n∑x[ylna+(1−y)ln(1−a)]<br>$$<br>四种交叉熵函数  <a href="http://dataunion.org/26447.html" target="_blank" rel="noopener">http://dataunion.org/26447.html</a></p><p><br></p><h3 id="（三）优化器Optimizer"><a href="#（三）优化器Optimizer" class="headerlink" title="（三）优化器Optimizer"></a>（三）优化器Optimizer</h3><p>优化器是为了<strong>加快训练的速度</strong>，<strong>基于学习效率learning rate的改变</strong> （注意：学习效率的改变，使得每种方法走过的loss的路径都是不一样的）</p><p><strong>Optimizer</strong><br><strong>GradientDescentOptimizer</strong><br><strong>AdagradOptimizer</strong><br><strong>AdagradDAOptimizer</strong><br><strong>MomentumOptimizer</strong><br><strong>AdamOptimizer</strong><br><strong>FtrlOptimizer</strong><br><strong>RMSPropOptimizer</strong></p><p>class tf.train.Optimizer 是基类，基本上不会直接使用这个类，一般使用子类 <code>GradientDescentOptimizer</code> 、<code>MomentumOptimizer</code>、<code>AdamOptimizer</code></p><p><br></p><p><br></p><h2 id="九、Timeline"><a href="#九、Timeline" class="headerlink" title="九、Timeline"></a>九、Timeline</h2><p>Tensorflow的Timeline模块，可以记录会话中每个操作执行时间和资源分配及消耗的情况</p><p><strong>生成方法</strong> ：</p><ol><li>在需要查看的操作 sess.run() 加入 option和run_metadata参数，然后创建timeline对象，并写入到timeline.json</li><li>打开Google Chrome，转到该页面 chrome://tracing并加载该timeline.json文件</li></ol><p>具体操作过程  <a href="https://walsvid.github.io/2017/03/25/profiletensorflow/" target="_blank" rel="noopener">https://walsvid.github.io/2017/03/25/profiletensorflow/</a></p><p> Timeline 的使用 <a href="https://www.jianshu.com/p/937a0ce99f56" target="_blank" rel="noopener">https://www.jianshu.com/p/937a0ce99f56</a></p><p><br></p><p><br></p><h2 id="十、TFDBG"><a href="#十、TFDBG" class="headerlink" title="十、TFDBG"></a>十、TFDBG</h2><p>TFDBG是 TensorFlow 的专用调试程序，借助该调试程序，可以在训练和推理期间查看运行中 TensorFlow 图的内部结构和状态</p><p><br></p><ol><li>TFDBG 的命令行接口（CLI）</li><li>TFDBG的图形化用户界面（GUI）：<strong>TensorBoard 调试插件</strong></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TensorFlow 学习教程分享&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;莫烦大神的视频，他讲的比较浅，适合入门学习者&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/xierhacker/article/details/53103979&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/xierhacker/article/details/53103979&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;cs20si&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（八）——CNN经典网络</title>
    <link href="http://yoursite.com/2018/08/05/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89%E2%80%94%E2%80%94CNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2018/08/05/cs231n笔记（八）——CNN经典网络/</id>
    <published>2018-08-05T06:54:20.000Z</published>
    <updated>2018-08-05T06:55:00.729Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ul><li><p>LeNet又称LeNet-5：没有参与ILSVRC比赛，但它是卷积网络的鼻祖</p><p>它是一个7层的网络，它的深度为5，包含2个卷积层、2个池化层、2个全连接层和1个Guassian connection，是用于手写字体的识别的一个经典CNN</p></li></ul><ul><li>AlexNe ：2012年ILSVRC的冠军</li></ul><ul><li>ZFNet ：2013年ILSVRC的冠军，ZFNet的网络结构，是在AlexNet上进行了微调</li></ul><ul><li>GoogLeNet ：2014年ILSVRC冠军，有22层</li></ul><ul><li><p>VGGNet ：2014年ILSVRC比赛中，有16和19层</p><p>（VGG代表的是Oxford 大学 Visual Geometry Group）</p></li></ul><ul><li>ResNet ：2015年ILSVRC的冠军，层数有 18，34，50，101和152</li></ul><p><br></p><p>PS：ILSVRC （ImageNet Large Scale Visual Recognition Challenge）是图像分类领域的比赛</p><a id="more"></a><p><br></p><h2 id="LeNet-5（1998年）"><a href="#LeNet-5（1998年）" class="headerlink" title="LeNet-5（1998年）"></a>LeNet-5（1998年）</h2><p><img src="https://pic3.zhimg.com/80/v2-af836017b97c0ca7231f527c10892f49_hd.jpg" alt="img"></p><p><strong>1. 输入层</strong>：32*32的图片，相当于1024个神经元，比MNIST数据集的图片要大一些，这么做的原因是希望潜在的明显特征如笔画断点能够出现在最高层<strong>特征检测子感受野（receptive field）</strong>的中心。因此在训练整个网络之前，需要对28*28的图像加上paddings。</p><p><br></p><p><strong>2. C1层：</strong>卷积层，6 个特征卷积核，卷积核大小 5*5，得到 6 个feature map，大小为28*28（ 32-5+1=28）</p><p><br></p><p><strong>3. S2层：</strong>下采样层，使用最大池化进行下采样，池化的size (2,2)，得到6 个feature map，大小为14*14（28/2=14）</p><p><br></p><p><strong>4. C3层</strong>：卷积层，16 个特征卷积核，卷积核大小 5*5，得到 16个feature map，大小为 10*10（ 14-5+1=10）  </p><p><br></p><p><strong>5. S4层：</strong>下采样层，使用最大池化进行下采样，池化的size (2,2) ，得到 16个feature map，大小为5*5（10/2=5）</p><p><br></p><p><strong>6. C5层：</strong>卷积层，120 个特征卷积核，卷积核大小 5*5，得到 120 个神经元</p><p><br></p><p><strong>7. F6层</strong>：与C5层全连接，有84个神经元</p><p>（为什么是84？</p><p>LeNet模型希望能够识别ASCII打印字符，而标准ASCII字符打印大小是用7*12大小的位图表示）</p><p><br></p><p><strong>8. 输出层</strong>：该层与F6层全连接，输出长度为 10 的张量，表示分类结果</p><p>输出层由『 <strong>欧式径向基函数单元</strong> 』组成，每类一个单元。该单元是用于计算输入向量（84个）和参数向量之间的欧式距离。输入向量离参数向量越远，RBF 输出值越大，表示与这种类别越不相似。</p><p><strong>RBF输出</strong>可以理解为 输入和与该类别的模型的匹配程度的惩罚项，越不相似，值越大，惩罚越大。</p><p><br></p><p><br></p><h2 id="AlexNet（2012年）"><a href="#AlexNet（2012年）" class="headerlink" title="AlexNet（2012年）"></a>AlexNet（2012年）</h2><p><a href="https://blog.csdn.net/maweifei/article/details/53117830" target="_blank" rel="noopener">https://blog.csdn.net/maweifei/article/details/53117830</a></p><p> AlexNet 在12年 ILSVRC 使用的模型结构，在错误率上有很大的降低，这也是<strong>第一次基于CNN的冠军</strong></p><p><br></p><h3 id="一、精简网络结构"><a href="#一、精简网络结构" class="headerlink" title="一、精简网络结构"></a>一、精简网络结构</h3><p>5个卷积层+3个全连接层+1个softmax层</p><p>输出 1000，因为 ImageNet数据集有1000个类别</p><p><img src="data:image/jpeg;base64,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" alt="“AlexNet精简结构”的图片搜索结果"></p><p><br></p><h3 id="二、各层运算"><a href="#二、各层运算" class="headerlink" title="二、各层运算"></a>二、各层运算</h3><p><img src="http://www.2cto.com/uploadfile/Collfiles/20160802/201608020923331170.jpg" alt="img"></p><p><br></p><h3 id="三、AlexNet主要使用到的新技术点"><a href="#三、AlexNet主要使用到的新技术点" class="headerlink" title="三、AlexNet主要使用到的新技术点"></a>三、AlexNet主要使用到的新技术点</h3><h4 id="1-ReLU"><a href="#1-ReLU" class="headerlink" title="1. ReLU"></a>1. ReLU</h4><p>tanh(x)或者sigmoid(x)函数：由于此类函数在x非常大或者非常小时，函数输出基本不变，所以此类函数成为<strong>饱和函数</strong></p><p>f(x)=max(0,x)：是一种非线性的<strong>非饱和函数</strong></p><p>优势：</p><p>非饱和函数比饱和函数训练更快</p><p>这种扭曲线性函数，不但保留了非线性的表达能力，而且由于其具有线性性质（正值部分），相比tanh和sigmoid函数在误差反向传递时，<strong>不会有由于非线性引起的梯度消失</strong></p><p> <br></p><h4 id="2-在多个GPU上训练"><a href="#2-在多个GPU上训练" class="headerlink" title="2. 在多个GPU上训练"></a>2. 在多个GPU上训练</h4><p>目前的GPU特别适合<strong>跨GPU并行化</strong>，因为它们能够直接从另一个GPU的内存中读出和写入，不需要通过主机内存 『采用的并行方案基本上是：<strong>在每个GPU中放置一半神经元</strong>』</p><p>还有一个额外的技巧：<strong>GPU间的通讯只在某些层进行</strong>。例如，第3层的核需要从第2层中所有核映射输入，而第4层的核只需要从第3层中位于同一GPU的那些核映射输入</p><p><br></p><h4 id="3-局部响应归一化-LRN-Local-Response-Normalization"><a href="#3-局部响应归一化-LRN-Local-Response-Normalization" class="headerlink" title="3. 局部响应归一化 LRN(Local Response Normalization)"></a>3. 局部响应归一化 LRN(Local Response Normalization)</h4><p>局部响应归一化原理是仿造生物学上<strong>活跃的神经元对相邻神经元的抑制现象</strong>（侧抑制）</p><ul><li>对局部神经元的活动创建竞争机制，<strong>使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元</strong></li><li>归一化有助于快速收敛</li></ul><p><img src="http://www.2cto.com/uploadfile/Collfiles/20160802/201608020923311161.png" alt="img"><br><br></p><h4 id="4-重叠-Pooling"><a href="#4-重叠-Pooling" class="headerlink" title="4. 重叠 Pooling"></a>4. 重叠 Pooling</h4><p><strong>（1）重叠的理解</strong></p><p>根据不同滑动的步长，<strong>滑动的窗口间有大量重叠区域</strong>，卷积结果有冗余，进行最大pooling或者平均pooling就是减少冗余。减少冗余的同时，pooling也丢掉了局部<strong>位置信息</strong>，所以局部有微小形变。</p><p>（PS：之前对池化一直一知半解，上面这句话真的点醒了我）</p><p><strong>（2）最大池化</strong></p><p>AlexNet<strong>全部使用最大池化</strong>，<strong>避免平均池化的模糊化效果</strong>。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。</p><p><br></p><h4 id="5-减少过拟合"><a href="#5-减少过拟合" class="headerlink" title="5.减少过拟合"></a>5.减少过拟合</h4><p><1>数据扩充</1></p><p>（1）图像平移和翻转 </p><p>（2）调整RGB像素值</p><p><2>Dropout</2></p><p><br></p><h2 id="VGG（2014年）"><a href="#VGG（2014年）" class="headerlink" title="VGG（2014年）"></a>VGG（2014年）</h2><p>深度<strong>16</strong>和<strong>19</strong>层的 VGGNet 网络模型在分类和定位任务上的效果最好</p><ul><li>小卷积核：卷积核全部替换为3x3（极少用了1x1）</li><li>小池化核：相比AlexNet的3x3的池化核，VGG全部为2x2的池化核</li><li>层数更深特征图更宽：基于前两点外，卷积核专注于扩大通道数、池化专注于缩小宽和高</li></ul><p><br></p><h3 id="一、小卷积核的优势"><a href="#一、小卷积核的优势" class="headerlink" title="一、小卷积核的优势"></a>一、小卷积核的优势</h3><ul><li>小卷积核，虽然需要卷积多层，但是需要的参数其实更小</li><li>越大的卷积核计算量越大</li><li>多个小卷积核的堆叠比单一大卷积核带来了精度提升</li></ul><p><br></p><h3 id="二、感受野（Receptive-Field）"><a href="#二、感受野（Receptive-Field）" class="headerlink" title="二、感受野（Receptive Field）"></a>二、感受野（Receptive Field）</h3><p>从CNN角度上看，输出featuremap某个节点的响应对应的输入图像的区域就是感受野</p><p><br></p><p>感受野的计算：（个人感觉画图吧，直观，从深层向浅层看）</p><p>stride=1，pad=0，2个3x3的卷积堆叠，获得的感受野，相当于一个5x5的卷积</p><p>stride=1，pad=0，3个3x3卷积的堆叠，获得的感受野，相当于一个7x7的卷积</p><p><br></p><h2 id="GoogLeNet（Google-Inception-Net）"><a href="#GoogLeNet（Google-Inception-Net）" class="headerlink" title="GoogLeNet（Google Inception Net）"></a>GoogLeNet（Google Inception Net）</h2><p>有<strong>22</strong>层，<strong>没有全连接层</strong>，但大小却比AlexNet和VGG小很多。GoogleNet参数为500万个，AlexNet参数个数是GoogleNet的12倍，VGGNet参数是AlexNet的3倍。</p><p><a href="http://www.chinacion.cn/article/2774.html" target="_blank" rel="noopener">http://www.chinacion.cn/article/2774.html</a></p><p><br></p><h3 id="一、提升网络性能的方法"><a href="#一、提升网络性能的方法" class="headerlink" title="一、提升网络性能的方法"></a>一、提升网络性能的方法</h3><p>通过增加网络深度和宽度提升网络性能（深度指网络层次数、宽度指神经元数量），但存在以下问题：</p><p>（1）参数太多，很容易过拟合<br>（2）计算复杂<br>（3）网络越深，容易出现梯度消失</p><p><br></p><p>解决上面问题的方法：</p><ul><li>第一步：全连接变成稀疏连接</li></ul><p>稀疏连接，可以减少参数，但全连接变成稀疏连接后，计算所消耗的时间却没有大幅减少</p><p>（稀疏矩阵，元素大部分为零的矩阵）</p><ul><li>第二步：Inception网络结构</li></ul><p>GoogLeNet 对网络中的传统卷积层进行了修改，提出了被称为 <strong>Inception</strong> 的结构，<strong>将稀疏矩阵聚类为较为密集的子矩阵</strong> ，来提高计算性能，既能减少参数，但不减少特征数，又能利用密集矩阵的高计算性能</p><p><br></p><h3 id="二、Inception"><a href="#二、Inception" class="headerlink" title="二、Inception"></a>二、Inception</h3><p><a href="https://zhuanlan.zhihu.com/p/32702031" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32702031</a></p><p><a href="https://blog.csdn.net/dcrmg/article/details/79246654" target="_blank" rel="noopener">https://blog.csdn.net/dcrmg/article/details/79246654</a></p><p><br></p><h3 id="（一）1×1卷积核和Network-in-Network（NIN）"><a href="#（一）1×1卷积核和Network-in-Network（NIN）" class="headerlink" title="（一）1×1卷积核和Network in Network（NIN）"></a>（一）1×1卷积核和Network in Network（NIN）</h3><p>MLP代替GLM：GLM是广义线性模型，MLP是指在做卷积操作的时候，把线性操作变为多层感知机</p><p>（看了很多博客都是这句话，什么玩意儿，能不能讲人话，还是知乎大神实在，不玩虚的）</p><p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blogs2016/imgs_inception/1.png" alt="img"></p><p>左图—传统的卷积层结构（线性卷积）：在一个尺度上只有一次卷积</p><p>右图——Network in Network结构（NIN）：先进行一次普通的卷积（比如3x3），再进行一次1x1的卷积，1x1卷积等效于全连接，所以右侧图的1x1卷积画成了全连接层的形式，将两个卷积串联，就能组合出更多的非线性特征。</p><p>（注意：NIN结构中无论是第一个3x3卷积还是新增的1x1卷积，后面都紧跟着激活函数）</p><p>『1×1卷积：起到降维的作用（减少参数，减少计算量），但是输出特征量并没有减少』</p><p>『NIN：一个神经网络之中，其每一层中的特征提取里又加上了一个小的神经网络』</p><p><br></p><h3 id="（二）多个尺寸上进行卷积再聚合"><a href="#（二）多个尺寸上进行卷积再聚合" class="headerlink" title="（二）多个尺寸上进行卷积再聚合"></a>（二）多个尺寸上进行卷积再聚合</h3><p><strong>用稀疏矩阵分解成密集矩阵计算的原理来加快收敛速度</strong></p><p>举个例子：</p><p>左图—稀疏矩阵，和一个2x2的矩阵进行卷积，需要对稀疏矩阵中的『每一个元素进行计算』</p><p>右图—把稀疏矩阵分解成2个子密集矩阵，再和2x2矩阵进行卷积，稀疏矩阵中0较多的区域不用计算</p><p><strong>这个原理应用到Inception上就是多尺度提取特征</strong></p><ul><li>传统的卷积层的输入数据只和一种尺度的卷积核进行卷积，输出固定维度的数据</li><li>inception模块在多个尺度上提取特征，这样特征集中相关性较强的特征聚集在了一起，不相关的非关键特征就被弱化，<strong>Inception方法输出的冗余信息较少</strong></li></ul><p><img src="https://pic3.zhimg.com/80/v2-eacad5957624f2b0dec823af256817cf_hd.jpg" alt="img"></p><p><br></p><h3 id="（三）全卷积代替全连接层"><a href="#（三）全卷积代替全连接层" class="headerlink" title="（三）全卷积代替全连接层"></a>（三）全卷积代替全连接层</h3><p>平均池化层代替了最后的全连接层</p><p>使用卷积层代替全连接层，<strong>突破了输入尺寸的限制</strong></p><p>PS：突然明白了卷积网络和一般神经网络相比，除了可以共享参数等优点，还有很重要的一点，那就是突破了输入尺寸的限制</p><p>一神经网络的输入图像大小是固定的，但是卷积神经网络的输入图像大小其实可以不固定</p><p><br></p><p>下图是Inception结构</p><p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blogs2016/imgs_inception/4.png" alt=""></p><p><br></p><h3 id="三、GoogLeNet-的-Inception-的四个版本"><a href="#三、GoogLeNet-的-Inception-的四个版本" class="headerlink" title="三、GoogLeNet 的 Inception 的四个版本"></a>三、GoogLeNet 的 Inception 的四个版本</h3><p><a href="https://blog.csdn.net/hejin_some/article/details/78636586" target="_blank" rel="noopener">https://blog.csdn.net/hejin_some/article/details/78636586</a></p><p><br></p><p><br></p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p><a href="https://blog.csdn.net/u012816943/article/details/51702520" target="_blank" rel="noopener">https://blog.csdn.net/u012816943/article/details/51702520</a></p><p><a href="https://blog.csdn.net/lanran2/article/details/79057994" target="_blank" rel="noopener">https://blog.csdn.net/lanran2/article/details/79057994</a></p><h3 id="一、ResNet-解决的问题"><a href="#一、ResNet-解决的问题" class="headerlink" title="一、ResNet 解决的问题"></a>一、ResNet 解决的问题</h3><p>1.深层网络存在的问题</p><p>仅仅只是向卷积网络上堆叠卷积层，得到更深的网络，会发现不论是训练集还是测试集，深层网络的效果都不如稍深的网络好，那就不是过拟合造成的，那是因为<strong>深层网络更难优化参数</strong></p><p>2.ResNet 残差神经网络在解决的问题：<strong>对于深层神经网络的优化，浅层信息的远距离传输问题</strong></p><p>保证了深层网络至少和浅层网络效果一样好，即网络的性能不会随着深度增加而降低。</p><p><br></p><h3 id="二、ResNet结构"><a href="#二、ResNet结构" class="headerlink" title="二、ResNet结构"></a>二、ResNet结构</h3><p>使用 residual mapping 而不是 direct mapping<img src="https://i.stack.imgur.com/msvse.png" alt=""></p><p>ResNet中有两种 mapping</p><ul><li><strong>identity mapping</strong>：指输入本身，就是x，这个支路叫做<strong>shortcut connection</strong></li><li><strong>residual mapping</strong>：指残差，就是F(x) = y -x</li></ul><p>residual mapping 可以看成是对 identity mapping 的修正。若网络已到达最优，继续加深网络，residual mapping 将被push为0，只剩下identity mapping，这样保证了深层网络至少和浅层网络效果一样好，即网络的性能不会随着深度增加而降低。</p><p><br></p><h3 id="三、-Bottleneck-Layer"><a href="#三、-Bottleneck-Layer" class="headerlink" title="三、 Bottleneck Layer"></a>三、 Bottleneck Layer</h3><p><img src="http://upload-images.jianshu.io/upload_images/2228224-f9b16ae3483b14da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>两种ResNet设计</p><ul><li>左图—一般的残差网络</li></ul><ul><li><p>右图—<strong>bottleneck design</strong>，用于<strong>降低参数的数目，减少计算</strong></p><p>右图中，第一个1x1的卷积用于降维，第二个1x1的卷积用于将维度恢复到和输入时一样</p></li></ul><p><br></p><h3 id="四、ResNet不同结构"><a href="#四、ResNet不同结构" class="headerlink" title="四、ResNet不同结构"></a>四、ResNet不同结构</h3><p>ResNet不同的结构，层数为 18，34，50，101和152</p><p><img src="http://d.ifengimg.com/w600/p0.ifengimg.com/pmop/2017/0228/A8B9353D30DB1937672E3BC00C7771472D6E2F53_size99_w740_h326.jpeg" alt=""></p><p>以 101-layer 为例</p><p>输入时卷积一次，然后经过3 + 4 + 23 + 3 = 33个building block，每个block为3层，所以有33 x 3 = 99层，最后 fc 层用于分类，总计 1 + 99 + 1 = 101层</p><p>（这里网络层数<strong>仅仅指卷积或者全连接层</strong>，而激活层、池化层没有计算在内）</p><p><br></p><h3 id="五、ResNet-的特点"><a href="#五、ResNet-的特点" class="headerlink" title="五、ResNet 的特点"></a>五、ResNet 的特点</h3><ol><li><p>Batch Normalization 批标准化：因为数据分布会对训练产生影响</p><p>进入激励函数的值的分布是很重要，需要在有效范围内，『<strong>BN被添加在全连接层和激励函数之间</strong>』</p></li><li><p>Xavier初始化</p></li><li><p>SGD + Momentum</p></li><li><p>学习率使用类似时间表的可变学习率</p></li><li><p>Weight decay 权值衰减，防止过拟合</p></li><li><p>没有dropout</p></li></ol><p><br></p><h3 id="六、对于ResNet-的改进"><a href="#六、对于ResNet-的改进" class="headerlink" title="六、对于ResNet 的改进"></a>六、对于ResNet 的改进</h3><ul><li>宽网络：通过增加宽度来代替增加网络的深度，优点是可以进行并行计算，计算效率高<ul><li>Wide ResNet：通过增加卷积核，来增加宽度，以这种方式代替增加网络的深度</li><li>ResNeXt：通过在残差模块里建立分支，增加残差模块的宽度</li></ul></li></ul><ul><li><p>随机深度：训练时丢弃一些层，使用更浅的网络，防止梯度消失（像是dropout为了防止过拟合）</p><p><br></p></li></ul><h3 id="七、新的网络"><a href="#七、新的网络" class="headerlink" title="七、新的网络"></a>七、新的网络</h3><p>可以媲美ResNet的网络 </p><ul><li>FractalNet ：分形网络，无残差的极深神经网络</li><li>DenseNet：密集连接的卷积神经网络</li><li>SqueezeNet</li></ul><p><br></p><p><br></p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p><strong>1. feature map</strong></p><p>在卷积层中，上层的 『<strong>每个 feature map</strong>』 跟对应的卷积核做卷积，产生下层的『<strong>一个feature map</strong>』</p><p>feature map 可以看成许多个二维图片叠在一起，如果是灰度图片，那就只有一个feature map，如果是彩色图片，一般就是3个feature map</p><p><br></p><p><strong>2. 卷积后图片的尺寸计算</strong></p><p>设步长为 s，填充长度为 p，输入图片大小为 n * n，滤波器大小为 f * f</p><p>则卷积后图片的尺寸 ⌊（n+2p-f）/s+1⌋ × ⌊（n+2p-f）/s+1⌋（向下取整）</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;LeNet又称LeNet-5：没有参与ILSVRC比赛，但它是卷积网络的鼻祖&lt;/p&gt;
&lt;p&gt;它是一个7层的网络，它的深度为5，包含2个卷积层、2个池化层、2个全连接层和1个Guassian connection，是用于手写字体的识别的一个经典CNN&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;AlexNe ：2012年ILSVRC的冠军&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;ZFNet ：2013年ILSVRC的冠军，ZFNet的网络结构，是在AlexNet上进行了微调&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;GoogLeNet ：2014年ILSVRC冠军，有22层&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;VGGNet ：2014年ILSVRC比赛中，有16和19层&lt;/p&gt;
&lt;p&gt;（VGG代表的是Oxford 大学 Visual Geometry Group）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;ResNet ：2015年ILSVRC的冠军，层数有 18，34，50，101和152&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;PS：ILSVRC （ImageNet Large Scale Visual Recognition Challenge）是图像分类领域的比赛&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（七）——深度学习框架</title>
    <link href="http://yoursite.com/2018/08/03/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    <id>http://yoursite.com/2018/08/03/cs231n笔记（七）——深度学习框架/</id>
    <published>2018-08-03T03:40:58.000Z</published>
    <updated>2018-08-03T03:45:04.455Z</updated>
    
    <content type="html"><![CDATA[<p>PS ：我现在学的是 TensorFlow</p><a id="more"></a><p><br></p><h2 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h2><h3 id="一、CPU-vs-GPU"><a href="#一、CPU-vs-GPU" class="headerlink" title="一、CPU vs GPU"></a>一、CPU vs GPU</h3><h4 id="1-CPU"><a href="#1-CPU" class="headerlink" title="1.CPU"></a>1.CPU</h4><p> AMD vs Intel</p><p>CPU擅长处理具有复杂计算步骤和复杂数据依赖的计算任务</p><h4 id="2-GPU-（显卡）"><a href="#2-GPU-（显卡）" class="headerlink" title="2.GPU （显卡）"></a>2.GPU （显卡）</h4><p>NVIDIA（英伟达） vs AMD  深度学习用的大多是NVIDIA的GPU</p><p>GPU的核数远超CPU，称为<strong>众核</strong>，但每个核拥有的缓存大小相对小，数字逻辑运算单元也少。GPU拥有超高的运算速度，擅长做并行算法（矩阵乘法）的处理。</p><p><br></p><h3 id="二、CUDA-和OpenCL"><a href="#二、CUDA-和OpenCL" class="headerlink" title="二、CUDA 和OpenCL"></a>二、CUDA 和OpenCL</h3><h4 id="1-CUDA"><a href="#1-CUDA" class="headerlink" title="1.CUDA"></a>1.CUDA</h4><p>CUDA是NVIDIA 发明的一种并行计算平台和编程模型</p><p>CUDA 代码只能在NVIDIA的GPU上运行，NVIDIA 提供了很多库，不需要直接写CUDA的代码。</p><h4 id="2-OpenCL"><a href="#2-OpenCL" class="headerlink" title="2.OpenCL"></a>2.OpenCL</h4><p>OpenCL是一个为异构平台编写程序的框架，此异构平台可由CPU，GPU或其他类型的处理器组成。提供了基于任务分割和数据分割的并行计算机制。</p><p><br></p><h2 id="三、从磁盘中读取数据降低训练速度"><a href="#三、从磁盘中读取数据降低训练速度" class="headerlink" title="三、从磁盘中读取数据降低训练速度"></a>三、从磁盘中读取数据降低训练速度</h2><p>解决方法：</p><p>1.数据小：将数据存储在RAM</p><p>2.数据大：使用SSD固态硬盘而非HHD混合硬盘存储</p><p>3.使用多线程CPU提前取数据，GPU在处理数据的同时，CPU从磁盘中读取数据</p><p><br></p><h2 id="四、深度学习框架"><a href="#四、深度学习框架" class="headerlink" title="四、深度学习框架"></a>四、深度学习框架</h2><h3 id="1-为什么需要深度学习框架？"><a href="#1-为什么需要深度学习框架？" class="headerlink" title="1.为什么需要深度学习框架？"></a>1.为什么需要深度学习框架？</h3><p>（1）使用GPU可以高效执行深度学习神经网络的计算</p><p>深度学习是包含许多节点的神经网络，并且每个节点都需要在学习过程中不断更新，计算量很大。使用GPU，可以高效执行深度学习神经网络的计算（GPU是专门为并行计算相同指令而设计的）。</p><p>（2）深度学习框架是在GPU上高效运行深度学习</p><p>这些深度学习框架都依赖于<strong>计算图</strong>的概念，计算图定义了需要执行的计算顺序。然后，计算图可以并行地在目标GPU中优化和运行。</p><p><br></p><h3 id="2-开源深度学习框架"><a href="#2-开源深度学习框架" class="headerlink" title="2. 开源深度学习框架"></a>2. 开源深度学习框架</h3><p>PyTorch（Facebook）</p><p>Tensorflow（Google）</p><p><br></p><p>使用框架的优点</p><p>1.很容易创建computation graph（计算图）</p><p>2.很容易在计算图中计算梯度</p><p>3.不需要考虑像CUDA这样的底层代码，就能在GPU上高效运行</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS ：我现在学的是 TensorFlow&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（六）——循环神经网络</title>
    <link href="http://yoursite.com/2018/08/02/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2018/08/02/cs231n笔记（六）——循环神经网络/</id>
    <published>2018-08-02T12:18:15.000Z</published>
    <updated>2018-08-02T12:25:20.521Z</updated>
    
    <content type="html"><![CDATA[<p>循环神经网络属于时间递归神经网络，主要解决<strong>序列</strong>数据的处理，比如文本、语音、视频等等</p><p>PS：因为我现在在做图像，CNN用的比较多，所以RNN和LSTM的学习一笔带过</p><a id="more"></a><p> <br></p><h2 id="一、序列数据与词嵌入"><a href="#一、序列数据与词嵌入" class="headerlink" title="一、序列数据与词嵌入"></a>一、序列数据与词嵌入</h2><p>序列数据：比如文本、语音、视频等等。这类数据的<strong>样本间存在顺序关系</strong>，每个样本和它之前的样本存在关联。比如说，在文本中，一个词和它前面的词是有关联的；在气象数据中，一天的气温和前几天的气温是有关联的。一组观察数据定义为一个序列，从分布中可以观察出多个序列。</p><p>词嵌入（ word embedding） ：不能直接将文字符号输入网络，所以需要将词语转换为数值形式</p><p>之前写过一篇关于 word2vec 的博客：</p><p><a href="https://sophia0130.github.io/2018/08/01/NLP%E2%80%94%E2%80%94word2vec/" target="_blank" rel="noopener">https://sophia0130.github.io/2018/08/01/NLP%E2%80%94%E2%80%94word2vec/</a></p><p><br></p><h2 id="二、RNN（recurrent-neural-networks）"><a href="#二、RNN（recurrent-neural-networks）" class="headerlink" title="二、RNN（recurrent neural networks）"></a>二、RNN（recurrent neural networks）</h2><p><img src="http://7xo0y8.com1.z0.glb.clouddn.com/RNN.png" alt="RNN_3"></p><p><br></p><h3 id="1-什么是RNN？"><a href="#1-什么是RNN？" class="headerlink" title="1.什么是RNN？"></a>1.什么是RNN？</h3><p>将循环神经网络进行展开成神经网络，隐含层h的边实际上是和上一时刻的h相连，这是一个<strong>时间序列</strong></p><p>输入单元(Input units)：{x0,x1,…,xt,xt+1,…}</p><p>输出单元(Output units)：{y0,y1,…,yt,yt+1.,..}</p><p>隐藏单元(Hidden units)：{h0,h1,…,ht,ht+1,…}</p><p><br></p><p>1）<strong>循环函数</strong>（recurrence formula ）：ht = Fθ (ht−1,xt)</p><p>2）ht：隐藏层的第t步的状态，它是网络的记忆单元，取决于当前输入层的输入与上一步隐藏层的状态</p><p>3）yt：第t步的输出，yt = softmax(Vst)</p><p>不同于 ht包含了前面所有步的隐藏层状态，输出层的输出yt只与当前步的ht有关</p><p><br></p><h3 id="2-RNN-的参数共享"><a href="#2-RNN-的参数共享" class="headerlink" title="2.RNN 的参数共享"></a>2.RNN 的参数共享</h3><p>RNN看上去和HMM很像。两者最大的区别在于，<strong>RNN的参数是跨时刻共享</strong>。也就是说，任意时刻的网络参数都是相同的。</p><p>共享参数使得模型的复杂度减少，这和CNN是相通的，CNN在二维数据的空间位置之间共享卷积核参数，而RNN则是在序列数据的时刻之间共享参数。</p><p><br></p><h3 id="3-输入输出的模式"><a href="#3-输入输出的模式" class="headerlink" title="3.输入输出的模式"></a>3.输入输出的模式</h3><p>one-to-many</p><p>many-to-one</p><p>sequence to sequence：many-to-one + one-to-many</p><p><br></p><h2 id="三、梯度消失和梯度爆炸"><a href="#三、梯度消失和梯度爆炸" class="headerlink" title="三、梯度消失和梯度爆炸"></a>三、梯度消失和梯度爆炸</h2><h2 id="Gradient-Vanishing-Exploding"><a href="#Gradient-Vanishing-Exploding" class="headerlink" title="(Gradient Vanishing/Exploding )"></a>(Gradient Vanishing/Exploding )</h2><p>对于深层网络（其层数过深）或循环神经网络（其递归结构），反向传播的连乘使得误差梯度<strong>累积</strong>，然后导致网络权重的大幅更新，并因此使网络变得不稳定。</p><p><br></p><h3 id="1-梯度消失"><a href="#1-梯度消失" class="headerlink" title="1. 梯度消失"></a>1. 梯度消失</h3><p>通常神经网络所用的激活函数是sigmoid函数，这个函数有个特点，就是能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))，因此两个0到1之间的数相乘，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新。</p><p><br></p><h3 id="2-梯度爆炸"><a href="#2-梯度爆炸" class="headerlink" title="2. 梯度爆炸"></a>2. 梯度爆炸</h3><p>由于初始化权值过大，反向传播时会导致误差梯度在更新中累积，变成非常大的梯度，对于比较浅的层数，其网络权重的大幅更新，使网络变得不稳定。</p><p><br></p><h3 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3.解决方法"></a>3.解决方法</h3><p>使用 ReLU 激活函数</p><p>使用长短期记忆网络（LSTM）</p><p>使用梯度截断（Gradient Clipping）</p><p>使用权重正则化（Weight Regularization）</p><p><br></p><h2 id="四、LSTM-长短时记忆（Long-Short-Term-Memory）"><a href="#四、LSTM-长短时记忆（Long-Short-Term-Memory）" class="headerlink" title="四、LSTM 长短时记忆（Long-Short Term Memory）"></a>四、LSTM 长短时记忆（Long-Short Term Memory）</h2><h3 id="1-LSTM的作用"><a href="#1-LSTM的作用" class="headerlink" title="1. LSTM的作用"></a>1. LSTM的作用</h3><p>权重指数级爆炸或消失，使得普通 RNN 没有办法回忆起久远记忆，而结合不同的LSTM可以很好解决这个问题</p><p><br></p><h3 id="2-LSTM-多了三个控制器"><a href="#2-LSTM-多了三个控制器" class="headerlink" title="2. LSTM 多了三个控制器"></a>2. LSTM 多了三个控制器</h3><p>概念理解 ：<a href="https://blog.csdn.net/hust_tsb/article/details/79485268" target="_blank" rel="noopener">https://blog.csdn.net/hust_tsb/article/details/79485268</a></p><p><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181722_631.png" alt="img"></p><ul><li>输入门（input gate）：输入门打开，数据就写入记忆细胞（门的打开和关闭通过学习得到）</li></ul><ul><li><p>遗忘门（forget gate)：是否将记忆中的值保留，刷新记忆</p><p>LSTM 模型的关键之一就在于这个“遗忘门”， 它能够控制训练时候梯度在这里的收敛性（从而避免了 RNN 中的梯度 vanishing/exploding 问题），同时也能够保持长期的记忆性。</p></li></ul><ul><li>输出门（output gate）：是否将记忆中的值读出（通过学习）</li></ul><p><br></p><h3 id="3-LSTM核心计算"><a href="#3-LSTM核心计算" class="headerlink" title="3. LSTM核心计算"></a>3. LSTM核心计算</h3><p>（1）输入时选择忘记过去某些信息</p><p><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_259.png" alt="img"></p><p><br></p><p>（2）记忆现在的信息</p><p><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_897.png" alt="img"></p><p>包含两个部分：</p><ul><li>sigmoid 层：决定什么值我们将要更新</li></ul><ul><li>tanh 层：<strong>创建一个新的候选向量</strong> Ct~，被加入到状态中。 语言模型的例子中，我们希望增加新的代词的类别到细胞状态中，来替代旧的需要忘记的代词。</li></ul><p><br></p><p>（3）将输入和更新后的细胞状态合并</p><p><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_883.png" alt="这里写图片描述"></p><p>更新旧细胞状态，Ct-1到Ct</p><p>旧状态与ft相乘，丢弃掉我们确定需要丢弃的信息，接着加上新的候选值it∗Ct~</p><p><br></p><p>（4）output gate：输出结果</p><p><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_463.png" alt="img"></p><p><br></p><h3 id="4-LSTM模型流程"><a href="#4-LSTM模型流程" class="headerlink" title="4.LSTM模型流程"></a>4.LSTM模型流程</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/rnn.JPG" alt="img"></p><p>（1）输入节点（gc）：接受上一个时刻隐藏节点的输出以及当前的输入作为输入，然后通过一个tanh激活函数</p><p>（2）输入门（ic）：将输入节点（gc）的输出相乘与将输入门（ic）的输出（输入门为 sigmoid，其输出为0-1之间，以起控制信息量的作用）</p><p>（3）忘记门（fc）：将上一个时刻的内部状态节点的输出（sc）与忘记门（fc）的输出相乘（忘记门为 sigmoid，其输出为0-1之间，起控制信息量的作用）</p><p>（4）内部状态节点（sc）：被输入门过滤的当前输入和经过忘记门的上一个时刻内部状态节点输出之</p><p>（5）输出门（oc）：将内部状态节点的输出（sc）与输出门（oc）相乘（输出门为 sigmoid，其输出为0-1之间，起控制信息量的作用）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;循环神经网络属于时间递归神经网络，主要解决&lt;strong&gt;序列&lt;/strong&gt;数据的处理，比如文本、语音、视频等等&lt;/p&gt;
&lt;p&gt;PS：因为我现在在做图像，CNN用的比较多，所以RNN和LSTM的学习一笔带过&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow（四）——RNN实现MNIST的分类</title>
    <link href="http://yoursite.com/2018/08/02/TensorFlow%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94RNN%E5%AE%9E%E7%8E%B0MNIST%E7%9A%84%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/08/02/TensorFlow（四）——RNN实现MNIST的分类/</id>
    <published>2018-08-02T12:17:20.000Z</published>
    <updated>2018-08-02T12:45:02.845Z</updated>
    
    <content type="html"><![CDATA[<p>PS：最近得开始做项目了，所以RNN只能草率收尾了，很多东西都不是很懂 ~ 只能先这样子了，后面如果有机会做语音和视频，还是得好好看一下</p><a id="more"></a><p><br></p><p>关于cell要注意：<br>理论上进入cell的数据就是单纯的数据，写代码的时候发现并非如此<br>这里，进入cell的数据其实是W*x+b，而非x</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/RNN.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：最近得开始做项目了，所以RNN只能草率收尾了，很多东西都不是很懂 ~ 只能先这样子了，后面如果有机会做语音和视频，还是得好好看一下&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>NLP——word2vec</title>
    <link href="http://yoursite.com/2018/08/01/NLP%E2%80%94%E2%80%94word2vec/"/>
    <id>http://yoursite.com/2018/08/01/NLP——word2vec/</id>
    <published>2018-08-01T02:44:45.000Z</published>
    <updated>2018-08-15T02:10:30.826Z</updated>
    
    <content type="html"><![CDATA[<p>PS：因为TensorFlow的官网教程里有，所以就顺带了解一下NLP，但不做编程训练，我还是要主攻图像</p><p>这篇博文比较清楚得解释了word2vec模型是个什么 ：<a href="https://zhuanlan.zhihu.com/p/26306795" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26306795</a></p><p><br></p><a id="more"></a><h1 id="一、word2vec模型是什么"><a href="#一、word2vec模型是什么" class="headerlink" title="一、word2vec模型是什么"></a>一、word2vec模型是什么</h1><p>word2vec模型：该模型是用于通过学习得到<strong>文字符号的向量表示</strong>，即是模型训练完后神经网络的权重。</p><p><br></p><h2 id="1-NLP-自然语言处理"><a href="#1-NLP-自然语言处理" class="headerlink" title="1. NLP (自然语言处理)"></a>1. NLP (自然语言处理)</h2><p>PS：NLP具体是做什么工作的  <a href="https://www.zhihu.com/question/19895141/answer/149475410" target="_blank" rel="noopener">https://www.zhihu.com/question/19895141/answer/149475410</a></p><p>1.<strong>句法语义分析</strong>：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。</p><p>2.<strong>信息抽取</strong>：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、专有名词等等</p><p>3.<strong>文本挖掘</strong>（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。</p><p>4.<strong>机器翻译</strong>：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。</p><p>5.<strong>信息检索</strong>：对大规模的文档进行索引。可</p><p>6.<strong>问答系统</strong>： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。</p><p><br></p><h2 id="2-词嵌入（-word-embedding"><a href="#2-词嵌入（-word-embedding" class="headerlink" title="2. 词嵌入（ word embedding)"></a>2. 词嵌入（ word embedding)</h2><p>词嵌入（ word embedding) ：将词语转换为数值形式</p><p>举个简单例子：如果需要判断一个词的词性，是动词还是名词。</p><p>用机器学习的思路解决，通过一系列样本(x,y)， x 是词语，y 是它们的词性，构建 f(x)：x-&gt;y 的映射。</p><p>但数学模型 f 只接受数值型输入，而 NLP 里的词语是符号形式（比如中文、英文、拉丁文），所以需要把符号转换成数值，或者说是嵌入到一个数学空间里，这种嵌入方式就叫词嵌入（word embedding）。</p><p><br></p><h2 id="3-one-hot"><a href="#3-one-hot" class="headerlink" title="3. one hot"></a>3. one hot</h2><p>one hot：属于词嵌入（ word embedding）的一种，当某一位为一的时候其他位都为零，这个向量就代表一个单词</p><p>缺点： </p><ul><li>由于向量长度是根据单词个数来的，如果有新词出现，这个向量还得增加</li><li>主观性太强</li><li>很难计算单词之间的相似性 </li></ul><p><br></p><h2 id="4-Word2vec"><a href="#4-Word2vec" class="headerlink" title="4. Word2vec"></a>4. Word2vec</h2><p>Word2vec：属于词嵌入（ word embedding）的一种，通过神经网或者深度学习对词进行训练，得到一个指定维度的向量，将练完后模型参数作为输入词语的向量化表示</p><p>在 NLP 中，把 x 看做一个句子里的一个词语，y 是这个词语的上下文词语，那么这里的 f，便是 NLP 中经常出现的『语言模型』（language model），这个模型的目的，就是判断 (x,y) 这个样本，是否符合自然语言的法则，即词语x和词语y放在一起，是不是人话。</p><p>Word2vec 正是来源于这个思想，它只关心模型训练完后模型参数（这里特指神经网络的权重），并将这些参数，作为输入 x 的某种向量化的表示，这个向量便叫做词向量，而这里的向量其实是经过降维。</p><p><br></p><p><br></p><h1 id="二、语言模型"><a href="#二、语言模型" class="headerlink" title="二、语言模型"></a>二、语言模型</h1><p>Word2vec是种高效率词嵌套学习的预测模型，其两种变体为：</p><ul><li>Skip-gram 模型：将一个词语作为输入，来预测它周围的上下文</li><li>CBOW 模型：将一个词语的上下文作为输入，来预测这个词语本身</li></ul><p><br></p><h2 id="1-Skip-gram-和-CBOW-的简单情形"><a href="#1-Skip-gram-和-CBOW-的简单情形" class="headerlink" title="1. Skip-gram 和 CBOW 的简单情形"></a>1. Skip-gram 和 CBOW 的简单情形</h2><p>用当前词 x 预测它的下一个词 y（Skip-gram）</p><p>用上下文词 x 预测当前词 y（CBOW）  </p><p><strong>（1）one-hot encoder： x 的输入形式</strong></p><p>假设全世界所有的词语总共有 V 个，这 V 个词语有自己的<strong>先后顺序</strong>，假设『吴彦祖』这个词是第1个词，就可以表示为一个 V 维全零向量、把第1个位置的0变成1，『我』这个单词是第2个词，表示为 V 维全零向量、把第2个位置的0变成1。每个词语都可以找到属于自己的唯一表示。</p><p><strong>（2）Skip-gram 的网络结构：x 就是 one-hot encoder 形式的输入，y 是在这 V 个词上输出的概率</strong></p><p><img src="https://pic4.zhimg.com/80/v2-a1a73c063b32036429fbd8f1ef59034b_hd.jpg" alt=""></p><p><strong>（3）Word2vec ：one-hot encoder形式的输入x，存在与这个位置相对应的被激活的权重vx</strong></p><p>当模型训练完后，最后得到的其实是<strong>神经网络的权重</strong>，比如现在输入一个 x 的 one-hot encoder [1,0,0,…,0]，对应词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，从而这些权重组成一个向量 vx 来表示x，而每个词语的 one-hot encoder 里面 1 的位置是不同的，所以被激活的向量 vx 就可以用来唯一表示 x。</p><p>词向量的维度与隐含层节点数一致，且一般情况下要远远小于词语总数 V 的大小，所以 Word2vec 本质上是一种<strong>降维</strong>操作</p><p>（<strong>！重点理解上面这段话！</strong>）</p><p><br></p><h2 id="2-Skip-gram-更一般的情形"><a href="#2-Skip-gram-更一般的情形" class="headerlink" title="2. Skip-gram 更一般的情形"></a>2. Skip-gram 更一般的情形</h2><p>需要预测的上下文 y 有多个词时</p><p><img src="https://pic2.zhimg.com/80/v2-ca81e19caa378cee6d4ba6d867f4fc7c_hd.jpg" alt=""></p><p><br></p><h2 id="3-CBOW-更一般的情形"><a href="#3-CBOW-更一般的情形" class="headerlink" title="3. CBOW 更一般的情形"></a>3. CBOW 更一般的情形</h2><p>输入的上下文 x有多个词时</p><p><img src="https://pic3.zhimg.com/80/v2-d1ca2547dfb91bf6a26c60782a26aa02_hd.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：因为TensorFlow的官网教程里有，所以就顺带了解一下NLP，但不做编程训练，我还是要主攻图像&lt;/p&gt;
&lt;p&gt;这篇博文比较清楚得解释了word2vec模型是个什么 ：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/26306795&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/26306795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow（三）——两层CNN神经网络实现MNIST的分类</title>
    <link href="http://yoursite.com/2018/07/31/TensorFlow%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E4%B8%A4%E5%B1%82CNN%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0MNIST%E7%9A%84%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/07/31/TensorFlow（三）——两层CNN神经网络实现MNIST的分类/</id>
    <published>2018-07-31T02:02:49.000Z</published>
    <updated>2018-07-31T05:25:56.920Z</updated>
    
    <content type="html"><![CDATA[<p><strong>两层CNN神经网络实现MNIST的分类</strong></p><p>version 1：和之前的神经网络训练MNIST数据集实现分类，整体过程差不多，只不过将计算换成卷积</p><p>version 2 ：相比 version 1多了 <strong>权重和卷积结果的图像可视化</strong></p><p>整个网络框架：image —&gt;conv1(32 kernels) —&gt;max pool1 —&gt;conv2 (64 kernels) —&gt;max pool2 </p><p>—&gt;fc1 —&gt;fc2 —&gt;classifier</p><p><br></p><a id="more"></a><h1 id="Version-1"><a href="#Version-1" class="headerlink" title="Version 1"></a>Version 1</h1><p>version 1和之前的神经网络训练MNIST数据集实现分类，整体过程差不多，只不过将计算换成卷积</p><p><br></p><h3 id="1-训练"><a href="#1-训练" class="headerlink" title="1. 训练"></a>1. 训练</h3><p>可以看到loss的值是有震荡的，所以超参数的选取需要改，我估计还是学习率</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_cnn1_train.jpg" alt=""></p><p><br></p><h3 id="2-测试"><a href="#2-测试" class="headerlink" title="2. 测试"></a>2. 测试</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_cnn1_test.jpg" alt=""></p><p><br></p><h3 id="3-代码解读"><a href="#3-代码解读" class="headerlink" title="3. 代码解读"></a>3. 代码解读</h3><h4 id="1-conv2d"><a href="#1-conv2d" class="headerlink" title="1) conv2d"></a>1) conv2d</h4><p><code>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)</code></p><p>input的shape：[batch, in_height, in_width, in_channels] </p><p>filter的shape：[filter_height, filter_width, in_channels, out_channels]</p><p>strides=[1,x,y,1] strides[0]和strides[3]默认值是1，中间两个表示在x方向移动的步数，y方向移动的步数</p><p><br></p><h4 id="2-max-pool"><a href="#2-max-pool" class="headerlink" title="2) max_pool"></a>2) max_pool</h4><p><code>tf.nn.max_pool(value, ksize, strides, padding, name=None)</code></p><p>value的shape：[batch, in_height, in_width, in_channels] </p><p>ksize的shape：池化窗口的大小，[1, height, width, 1]，因为不想在batch和channels上做池化，所以这两个维度设为了1</p><p>strides=[1,x,y,1] strides[0]和strides[3]默认值是1，中间两个表示在x方向移动的步数，y方向移动的步数</p><p>（一般strides=[1,2,2,1]）</p><p><br></p><h4 id="3-dropout"><a href="#3-dropout" class="headerlink" title="3)dropout"></a>3)dropout</h4><p><code>dropout(x, keep_prob, noise_shape=None, seed=None, name=None)</code></p><p>x：输入用于训练的数据</p><p>keep_prob：神经元保留概率</p><p>（需要对随机失活做scaling 缩放： l /= keep_prob  <strong>这一步很重要</strong> ）</p><p><br></p><p><strong>关于 keep_prob 需要注意</strong> </p><p>只有训练的时候 keep_prob &lt;1 才进行 dropout</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(train,feed_dict =&#123;x_input:x_train_batch, y_input:y_train_batch, keep_prob:<span class="number">0.8</span>&#125;,  options = run_options,run_metadata = run_metadata)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(loss_cross_entropy,feed_dict = &#123;x_input:x_train_batch, y_input:y_train_batch, keep_prob:<span class="number">1.0</span>&#125;,options=run_options,run_metadata=run_metadata)</span><br></pre></td></tr></table></figure><p><br></p><h3 id="4-tensorboard"><a href="#4-tensorboard" class="headerlink" title="4. tensorboard"></a>4. tensorboard</h3><h4 id="1-计算图"><a href="#1-计算图" class="headerlink" title="1) 计算图"></a>1) 计算图</h4><p><img src="http://p8ge6t5tt.bkt.clouddn.com/cnn_graph.png" alt=""></p><h4 id="2-scalar"><a href="#2-scalar" class="headerlink" title="2)scalar"></a>2)scalar</h4><p>可以看出震荡还是蛮大的，是超参数没设置好</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/cnn_scalar.png" alt=""> </p><p><br></p><p><br></p><p>PS：相比version 1多了 <strong>权重和卷积结果的图像可视化</strong></p><p>想做卷积层输出结果的图像输出，查了很多资料，总算找到了一个合适的，在此基础上做了些修改</p><p>感觉是个很低调的大神分享的代码，GitHub ：<a href="https://github.com/grishasergei/conviz" target="_blank" rel="noopener">https://github.com/grishasergei/conviz</a></p><p>权值可视化那部分改了很久，一直都报错，差点放弃了，还好坚持print了几个变量的shape，果然我还是对各数据的结构掌握不全，最后做出来的那一刻，真的开心到尖叫，旋转，跳跃，也很感谢大神的分享</p><p><br></p><h3 id="1-训练-1"><a href="#1-训练-1" class="headerlink" title="1. 训练"></a>1. 训练</h3><p>和version 1 一样，losss值震荡，等把几个经典网络学完，就得开始试着自己调参，分析结果，调参……</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_cnn2_train.jpg" alt=""></p><h3 id="2-测试-1"><a href="#2-测试-1" class="headerlink" title="2. 测试"></a>2. 测试</h3><p>与大神分享的代码相比，在权值可视化那部分改动很大，主要是conv_weight[] 这个量在原代码里那样用会在shape上出现错误，所以就改成下面这种</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_cnn2_test.jpg" alt=""></p><p><br></p><h3 id="3-修改"><a href="#3-修改" class="headerlink" title="3. 修改"></a>3. 修改</h3><ol><li><code>x_ = tf.reshape(x_, tf.pack([-1, n]))</code> 改为 <code>x_ = tf.reshape(x_, tf.stack([-1, n]))</code></li><li><code>while step * batch_size &lt; training_iters</code> 改为 <code>for step in range(1,1001)</code></li><li>增加了保存模型的那部分Saver</li><li>session部分的权值可视化</li></ol><p><br></p><h3 id="4-权值可视化"><a href="#4-权值可视化" class="headerlink" title="4. 权值可视化"></a>4. 权值可视化</h3><h4 id="1-conv1"><a href="#1-conv1" class="headerlink" title="1) conv1"></a>1) conv1</h4><p>输入数据的厚度为1，32个滤波器</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/conv1_weight.png" alt=""></p><h4 id="2-conv2"><a href="#2-conv2" class="headerlink" title="2) conv2"></a>2) conv2</h4><p>输入数据的厚度为32，64个滤波器</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/conv2_weight.png" alt=""></p><p>PS：写博客的时候我看这个图是有点晕的，有点一下子转不过弯，这里说明一下图</p><p>每个滤波器的大小5*5</p><p>conv1：总共有32个滤波器，滤波器的通道数为1，所以所有滤波器（32）放在一张图上</p><p>conv2：总共有64个滤波器，滤波器的通道数为32，所以每张小图是所有滤波器（64）对应的一个通道</p><p><br></p><h3 id="5-卷积结果可视化"><a href="#5-卷积结果可视化" class="headerlink" title="5. 卷积结果可视化"></a>5. 卷积结果可视化</h3><h4 id="1-conv1-1"><a href="#1-conv1-1" class="headerlink" title="1) conv1"></a>1) conv1</h4><p>卷积后的厚度为32</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/conv2_output.png" alt=""></p><h4 id="2-conv2-1"><a href="#2-conv2-1" class="headerlink" title="2) conv2"></a>2) conv2</h4><p>卷积后的厚度为64</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/conv1_output.png" alt=""></p><p><br></p><p>写在最后：</p><p>三次练习坐下来TensorFlow总算是入门了，下面开启CNN的经典网络的学习 ~ </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;两层CNN神经网络实现MNIST的分类&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;version 1：和之前的神经网络训练MNIST数据集实现分类，整体过程差不多，只不过将计算换成卷积&lt;/p&gt;
&lt;p&gt;version 2 ：相比 version 1多了 &lt;strong&gt;权重和卷积结果的图像可视化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;整个网络框架：image —&amp;gt;conv1(32 kernels) —&amp;gt;max pool1 —&amp;gt;conv2 (64 kernels) —&amp;gt;max pool2 &lt;/p&gt;
&lt;p&gt;—&amp;gt;fc1 —&amp;gt;fc2 —&amp;gt;classifier&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Ipython 与 Jupyter Notebook</title>
    <link href="http://yoursite.com/2018/07/29/Ipython-%E4%B8%8E-Jupyter-Notebook/"/>
    <id>http://yoursite.com/2018/07/29/Ipython-与-Jupyter-Notebook/</id>
    <published>2018-07-29T11:19:01.000Z</published>
    <updated>2018-07-29T12:07:43.046Z</updated>
    
    <content type="html"><![CDATA[<p>PS：为了理解 Jupyter Notebook 所以展开来查了很多资料，也不知道自己的理解到底对不对</p><p><a href="https://blog.windrunner.me/python/jupyter.html" target="_blank" rel="noopener">https://blog.windrunner.me/python/jupyter.html</a></p><p><a href="http://smilejay.com/2012/10/interactive-shell-login-shell/" target="_blank" rel="noopener">http://smilejay.com/2012/10/interactive-shell-login-shell/</a></p><p><a href="https://blog.csdn.net/wkw1125/article/details/53932945" target="_blank" rel="noopener">https://blog.csdn.net/wkw1125/article/details/53932945</a></p><p><br></p><a id="more"></a><h2 id="IPython-和-python-shell-的区别"><a href="#IPython-和-python-shell-的区别" class="headerlink" title="IPython 和 python shell 的区别"></a>IPython 和 python shell 的区别</h2><p>IPython（Interactive Python shell），是 <strong>python 的交互式 shell</strong>，比 Python shell 功能更强大</p><p><br></p><h3 id="1-交互式-Interactive-是什么意思？"><a href="#1-交互式-Interactive-是什么意思？" class="headerlink" title="1. 交互式 Interactive 是什么意思？"></a>1. 交互式 Interactive 是什么意思？</h3><p>交互式 Interactive Shell</p><p>在终端上执行，shell等待输入，并且立即执行提交的命令，退出后，shell也终止</p><p>非交互式 Non Interactive Shell</p><p>读取存放在文件中的命令，从可以第一条命令执行到最后一条然后退出，不与你进行任何交互</p><p><br></p><h3 id="2-shell-是什么？"><a href="#2-shell-是什么？" class="headerlink" title="2. shell 是什么？"></a>2. shell 是什么？</h3><p>shell： <strong>命令解释器</strong>，处于内核和用户之间，<strong>把用户的指令传递给内核并且把执行结果回显给用户</strong></p><p>命令解释器 shell ，解释执行脚本程序 shell script</p><p>（1）windows：</p><ul><li>explorer.exe（资源管理器）：是windows的图形shell</li><li>cmd（Command shell）：是windows的命令行shell</li><li>windows Power Shell：相比CMD功能更强大</li></ul><p>（2）linux/unix：</p><p>Shell有多种实现，多数Linux发行版本默认是bash</p><p><br></p><h3 id="3-脚本语言是什么？"><a href="#3-脚本语言是什么？" class="headerlink" title="3. 脚本语言是什么？"></a>3. 脚本语言是什么？</h3><ul><li><strong>编程语言：编写-编译-链接-运行</strong></li><li><strong>脚本语言：解释-执行</strong></li></ul><p>脚本语言需要通过对应的解释器解释执行。如Perl、Python、Ruby、JavaScript等都是脚本语言，每种脚本语言都需要其对应的解释器。shell script 也属于一种比较特殊的脚本语言。</p><p><br></p><p><br></p><h2 id="Jupyter-Notebook"><a href="#Jupyter-Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h2><h3 id="1-Jupyter-Notebook-是什么？"><a href="#1-Jupyter-Notebook-是什么？" class="headerlink" title="1. Jupyter Notebook 是什么？"></a>1. Jupyter Notebook 是什么？</h3><p>Jupyter Notebook 是一个<strong>交互式笔记本</strong>，支持运行 40 多种编程语言。</p><p>Jupyter Notebook 的本质是一个 Web 应用程序，<strong>能让用户将说明文本、数学方程、代码和可视化内容全部组合到一个易于共享的文档中</strong>。</p><p>PS：个人理解 Jupyter Notebook 可以看成 txt 文件，就是笔记本，<strong>方便记录</strong></p><p><br></p><h3 id="2-Jupyter-Notebook-哪里方便？"><a href="#2-Jupyter-Notebook-哪里方便？" class="headerlink" title="2.  Jupyter Notebook 哪里方便？"></a>2.  Jupyter Notebook 哪里方便？</h3><p>（1）不用 Jupyter Notebook 之前，说明和代码是分开的</p><p>在IDE（集成开发环境）如Pycharm中写代码，然后在word中写文档说明项目。通常是写完代码，再写文档，有的时候需要重头回顾一遍代码。</p><p>（2）不用 Jupyter Notebook 之前，结果可视化不方便</p><p>为了得到数据分析的中间结果，需要重新跑代码，然后把结果截图放到文档里。</p><p><br></p><h3 id="3-ipynb是什么？"><a href="#3-ipynb是什么？" class="headerlink" title="3. ipynb是什么？"></a>3. ipynb是什么？</h3><p>jupyter保存的文件的扩展名是  .ipynb，<strong>实际上这是一个json文件</strong></p><p><br></p><h3 id="4-什么是JSON？"><a href="#4-什么是JSON？" class="headerlink" title="4. 什么是JSON？"></a>4. 什么是JSON？</h3><p>（1）JSON是一种数据结构，方便数据的传输</p><p>（2）JSON 作为数据的交换格式，在客户端和服务器端完成数据交换的原理</p><ul><li>如果是客户端请求数据，那么服务器端就<strong>将Java对象先转换成 JSON字符串</strong></li><li>经响应把字符串传到客户端之后，客户端就会接收到这个转换结果</li><li>但 JavaScript 要求把这个字符串变成对象格式才更方便访问</li><li>所以在客户端的<strong>JavaScript代码中又需要将这个JSON字符串变成JavaScript能够识别的对象</strong></li></ul><p><br></p><h3 id="5-从-JSON-理解-Jupyter-Notebook"><a href="#5-从-JSON-理解-Jupyter-Notebook" class="headerlink" title="5. 从 JSON 理解 Jupyter Notebook"></a>5. 从 JSON 理解 Jupyter Notebook</h3><p>下面是我个人的想法，从 JSON 理解 Jupyter Notebook</p><p>就是本地服务的网页界面的实现，你的. ipynb文件放在你的本地服务器就是你的电脑上，而浏览器就是客户端要显示该文件的内容，这就需要两方通信进行数据传输</p><p><br></p><p><br></p><p>PS：之前写的 XML、YML 、JSON </p><p><a href="https://sophia0130.github.io/2018/05/14/XML%E3%80%81YAML%E3%80%81JSON/" target="_blank" rel="noopener">https://sophia0130.github.io/2018/05/14/XML%E3%80%81YAML%E3%80%81JSON/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：为了理解 Jupyter Notebook 所以展开来查了很多资料，也不知道自己的理解到底对不对&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.windrunner.me/python/jupyter.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.windrunner.me/python/jupyter.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://smilejay.com/2012/10/interactive-shell-login-shell/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://smilejay.com/2012/10/interactive-shell-login-shell/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/wkw1125/article/details/53932945&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/wkw1125/article/details/53932945&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
</feed>
