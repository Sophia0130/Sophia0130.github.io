<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>绿小蕤</title>
  <icon>https://www.gravatar.com/avatar/e4d7a8bd1cb84fb3b4123916b4ea2f6b</icon>
  <subtitle>好逸恶劳,贪生怕死</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-08-02T12:25:20.521Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>绿小蕤</name>
    <email>528036346@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>cs231n笔记（六）——循环神经网络</title>
    <link href="http://yoursite.com/2018/08/02/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2018/08/02/cs231n笔记（六）——循环神经网络/</id>
    <published>2018-08-02T12:18:15.000Z</published>
    <updated>2018-08-02T12:25:20.521Z</updated>
    
    <content type="html"><![CDATA[<p>循环神经网络属于时间递归神经网络，主要解决<strong>序列</strong>数据的处理，比如文本、语音、视频等等</p><p>PS：因为我现在在做图像，CNN用的比较多，所以RNN和LSTM的学习一笔带过</p><a id="more"></a><p> <br></p><h2 id="一、序列数据与词嵌入"><a href="#一、序列数据与词嵌入" class="headerlink" title="一、序列数据与词嵌入"></a>一、序列数据与词嵌入</h2><p>序列数据：比如文本、语音、视频等等。这类数据的<strong>样本间存在顺序关系</strong>，每个样本和它之前的样本存在关联。比如说，在文本中，一个词和它前面的词是有关联的；在气象数据中，一天的气温和前几天的气温是有关联的。一组观察数据定义为一个序列，从分布中可以观察出多个序列。</p><p>词嵌入（ word embedding） ：不能直接将文字符号输入网络，所以需要将词语转换为数值形式</p><p>之前写过一篇关于 word2vec 的博客：</p><p><a href="https://sophia0130.github.io/2018/08/01/NLP%E2%80%94%E2%80%94word2vec/" target="_blank" rel="noopener">https://sophia0130.github.io/2018/08/01/NLP%E2%80%94%E2%80%94word2vec/</a></p><p><br></p><h2 id="二、RNN（recurrent-neural-networks）"><a href="#二、RNN（recurrent-neural-networks）" class="headerlink" title="二、RNN（recurrent neural networks）"></a>二、RNN（recurrent neural networks）</h2><p><img src="http://7xo0y8.com1.z0.glb.clouddn.com/RNN.png" alt="RNN_3"></p><p><br></p><h3 id="1-什么是RNN？"><a href="#1-什么是RNN？" class="headerlink" title="1.什么是RNN？"></a>1.什么是RNN？</h3><p>将循环神经网络进行展开成神经网络，隐含层h的边实际上是和上一时刻的h相连，这是一个<strong>时间序列</strong></p><p>输入单元(Input units)：{x0,x1,…,xt,xt+1,…}</p><p>输出单元(Output units)：{y0,y1,…,yt,yt+1.,..}</p><p>隐藏单元(Hidden units)：{h0,h1,…,ht,ht+1,…}</p><p><br></p><p>1）<strong>循环函数</strong>（recurrence formula ）：ht = Fθ (ht−1,xt)</p><p>2）ht：隐藏层的第t步的状态，它是网络的记忆单元，取决于当前输入层的输入与上一步隐藏层的状态</p><p>3）yt：第t步的输出，yt = softmax(Vst)</p><p>不同于 ht包含了前面所有步的隐藏层状态，输出层的输出yt只与当前步的ht有关</p><p><br></p><h3 id="2-RNN-的参数共享"><a href="#2-RNN-的参数共享" class="headerlink" title="2.RNN 的参数共享"></a>2.RNN 的参数共享</h3><p>RNN看上去和HMM很像。两者最大的区别在于，<strong>RNN的参数是跨时刻共享</strong>。也就是说，任意时刻的网络参数都是相同的。</p><p>共享参数使得模型的复杂度减少，这和CNN是相通的，CNN在二维数据的空间位置之间共享卷积核参数，而RNN则是在序列数据的时刻之间共享参数。</p><p><br></p><h3 id="3-输入输出的模式"><a href="#3-输入输出的模式" class="headerlink" title="3.输入输出的模式"></a>3.输入输出的模式</h3><p>one-to-many</p><p>many-to-one</p><p>sequence to sequence：many-to-one + one-to-many</p><p><br></p><h2 id="三、梯度消失和梯度爆炸"><a href="#三、梯度消失和梯度爆炸" class="headerlink" title="三、梯度消失和梯度爆炸"></a>三、梯度消失和梯度爆炸</h2><h2 id="Gradient-Vanishing-Exploding"><a href="#Gradient-Vanishing-Exploding" class="headerlink" title="(Gradient Vanishing/Exploding )"></a>(Gradient Vanishing/Exploding )</h2><p>对于深层网络（其层数过深）或循环神经网络（其递归结构），反向传播的连乘使得误差梯度<strong>累积</strong>，然后导致网络权重的大幅更新，并因此使网络变得不稳定。</p><p><br></p><h3 id="1-梯度消失"><a href="#1-梯度消失" class="headerlink" title="1. 梯度消失"></a>1. 梯度消失</h3><p>通常神经网络所用的激活函数是sigmoid函数，这个函数有个特点，就是能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))，因此两个0到1之间的数相乘，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新。</p><p><br></p><h3 id="2-梯度爆炸"><a href="#2-梯度爆炸" class="headerlink" title="2. 梯度爆炸"></a>2. 梯度爆炸</h3><p>由于初始化权值过大，反向传播时会导致误差梯度在更新中累积，变成非常大的梯度，对于比较浅的层数，其网络权重的大幅更新，使网络变得不稳定。</p><p><br></p><h3 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3.解决方法"></a>3.解决方法</h3><p>使用 ReLU 激活函数</p><p>使用长短期记忆网络（LSTM）</p><p>使用梯度截断（Gradient Clipping）</p><p>使用权重正则化（Weight Regularization）</p><p><br></p><h2 id="四、LSTM-长短时记忆（Long-Short-Term-Memory）"><a href="#四、LSTM-长短时记忆（Long-Short-Term-Memory）" class="headerlink" title="四、LSTM 长短时记忆（Long-Short Term Memory）"></a>四、LSTM 长短时记忆（Long-Short Term Memory）</h2><h3 id="1-LSTM的作用"><a href="#1-LSTM的作用" class="headerlink" title="1. LSTM的作用"></a>1. LSTM的作用</h3><p>权重指数级爆炸或消失，使得普通 RNN 没有办法回忆起久远记忆，而结合不同的LSTM可以很好解决这个问题</p><p><br></p><h3 id="2-LSTM-多了三个控制器"><a href="#2-LSTM-多了三个控制器" class="headerlink" title="2. LSTM 多了三个控制器"></a>2. LSTM 多了三个控制器</h3><p>概念理解 ：<a href="https://blog.csdn.net/hust_tsb/article/details/79485268" target="_blank" rel="noopener">https://blog.csdn.net/hust_tsb/article/details/79485268</a></p><p><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181722_631.png" alt="img"></p><ul><li>输入门（input gate）：输入门打开，数据就写入记忆细胞（门的打开和关闭通过学习得到）</li></ul><ul><li><p>遗忘门（forget gate)：是否将记忆中的值保留，刷新记忆</p><p>LSTM 模型的关键之一就在于这个“遗忘门”， 它能够控制训练时候梯度在这里的收敛性（从而避免了 RNN 中的梯度 vanishing/exploding 问题），同时也能够保持长期的记忆性。</p></li></ul><ul><li>输出门（output gate）：是否将记忆中的值读出（通过学习）</li></ul><p><br></p><h3 id="3-LSTM核心计算"><a href="#3-LSTM核心计算" class="headerlink" title="3. LSTM核心计算"></a>3. LSTM核心计算</h3><p>（1）输入时选择忘记过去某些信息</p><p><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_259.png" alt="img"></p><p><br></p><p>（2）记忆现在的信息</p><p><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_897.png" alt="img"></p><p>包含两个部分：</p><ul><li>sigmoid 层：决定什么值我们将要更新</li></ul><ul><li>tanh 层：<strong>创建一个新的候选向量</strong> Ct~，被加入到状态中。 语言模型的例子中，我们希望增加新的代词的类别到细胞状态中，来替代旧的需要忘记的代词。</li></ul><p><br></p><p>（3）将输入和更新后的细胞状态合并</p><p><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_883.png" alt="这里写图片描述"></p><p>更新旧细胞状态，Ct-1到Ct</p><p>旧状态与ft相乘，丢弃掉我们确定需要丢弃的信息，接着加上新的候选值it∗Ct~</p><p><br></p><p>（4）output gate：输出结果</p><p><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_463.png" alt="img"></p><p><br></p><h3 id="4-LSTM模型流程"><a href="#4-LSTM模型流程" class="headerlink" title="4.LSTM模型流程"></a>4.LSTM模型流程</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/rnn.JPG" alt="img"></p><p>（1）输入节点（gc）：接受上一个时刻隐藏节点的输出以及当前的输入作为输入，然后通过一个tanh激活函数</p><p>（2）输入门（ic）：将输入节点（gc）的输出相乘与将输入门（ic）的输出（输入门为 sigmoid，其输出为0-1之间，以起控制信息量的作用）</p><p>（3）忘记门（fc）：将上一个时刻的内部状态节点的输出（sc）与忘记门（fc）的输出相乘（忘记门为 sigmoid，其输出为0-1之间，起控制信息量的作用）</p><p>（4）内部状态节点（sc）：被输入门过滤的当前输入和经过忘记门的上一个时刻内部状态节点输出之</p><p>（5）输出门（oc）：将内部状态节点的输出（sc）与输出门（oc）相乘（输出门为 sigmoid，其输出为0-1之间，起控制信息量的作用）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;循环神经网络属于时间递归神经网络，主要解决&lt;strong&gt;序列&lt;/strong&gt;数据的处理，比如文本、语音、视频等等&lt;/p&gt;
&lt;p&gt;PS：因为我现在在做图像，CNN用的比较多，所以RNN和LSTM的学习一笔带过&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow（四）——RNN实现MNIST的分类</title>
    <link href="http://yoursite.com/2018/08/02/TensorFlow%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94RNN%E5%AE%9E%E7%8E%B0MNIST%E7%9A%84%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/08/02/TensorFlow（四）——RNN实现MNIST的分类/</id>
    <published>2018-08-02T12:17:20.000Z</published>
    <updated>2018-08-02T12:45:02.845Z</updated>
    
    <content type="html"><![CDATA[<p>PS：最近得开始做项目了，所以RNN只能草率收尾了，很多东西都不是很懂 ~ 只能先这样子了，后面如果有机会做语音和视频，还是得好好看一下</p><a id="more"></a><p><br></p><p>关于cell要注意：<br>理论上进入cell的数据就是单纯的数据，写代码的时候发现并非如此<br>这里，进入cell的数据其实是W*x+b，而非x</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/RNN.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：最近得开始做项目了，所以RNN只能草率收尾了，很多东西都不是很懂 ~ 只能先这样子了，后面如果有机会做语音和视频，还是得好好看一下&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>NLP——word2vec</title>
    <link href="http://yoursite.com/2018/08/01/NLP%E2%80%94%E2%80%94word2vec/"/>
    <id>http://yoursite.com/2018/08/01/NLP——word2vec/</id>
    <published>2018-08-01T02:44:45.000Z</published>
    <updated>2018-08-02T01:57:56.357Z</updated>
    
    <content type="html"><![CDATA[<p>PS：因为TensorFlow的官网教程里有，所以就顺带了解一下NLP，但不做编程训练，我还是要主攻图像</p><p>这篇博文比较清楚得解释了word2vec模型是个什么 ：<a href="https://zhuanlan.zhihu.com/p/26306795" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26306795</a></p><p><br></p><a id="more"></a><h1 id="一、word2vec模型是什么"><a href="#一、word2vec模型是什么" class="headerlink" title="一、word2vec模型是什么"></a>一、word2vec模型是什么</h1><p>word2vec模型：该模型是用于通过学习得到<strong>文字符号的向量表示</strong>，即是模型训练完后神经网络的权重。</p><p><br></p><h2 id="1-NLP-自然语言处理"><a href="#1-NLP-自然语言处理" class="headerlink" title="1. NLP (自然语言处理)"></a>1. NLP (自然语言处理)</h2><p>PS：NLP具体是做什么工作的  <a href="https://www.zhihu.com/question/19895141/answer/149475410" target="_blank" rel="noopener">https://www.zhihu.com/question/19895141/answer/149475410</a></p><p>1.<strong>句法语义分析</strong>：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。</p><p>2.<strong>信息抽取</strong>：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、专有名词等等</p><p>3.<strong>文本挖掘</strong>（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。</p><p>4.<strong>机器翻译</strong>：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。</p><p>5.<strong>信息检索</strong>：对大规模的文档进行索引。可</p><p>6.<strong>问答系统</strong>： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。</p><p><br></p><h2 id="2-词嵌入（-word-embedding"><a href="#2-词嵌入（-word-embedding" class="headerlink" title="2. 词嵌入（ word embedding)"></a>2. 词嵌入（ word embedding)</h2><p>词嵌入（ word embedding) ：将词语转换为数值形式</p><p>举个简单例子：如果需要判断一个词的词性，是动词还是名词。</p><p>用机器学习的思路解决，通过一系列样本(x,y)， x 是词语，y 是它们的词性，构建 f(x)：x-&gt;y 的映射。</p><p>但数学模型 f 只接受数值型输入，而 NLP 里的词语是符号形式（比如中文、英文、拉丁文），所以需要把符号转换成数值，或者说是嵌入到一个数学空间里，这种嵌入方式就叫词嵌入（word embedding）。</p><p><br></p><h2 id="3-one-hot"><a href="#3-one-hot" class="headerlink" title="3. one hot"></a>3. one hot</h2><p>one hot：属于词嵌入（ word embedding）的一种，当某一位为一的时候其他位都为零，这个向量就代表一个单词</p><p>缺点： </p><ul><li>由于向量长度是根据单词个数来的，如果有新词出现，这个向量还得增加</li><li>主观性太强</li><li>很难计算单词之间的相似性 </li></ul><p><br></p><h2 id="4-Word2vec"><a href="#4-Word2vec" class="headerlink" title="4. Word2vec"></a>4. Word2vec</h2><p>Word2vec：属于词嵌入（ word embedding）的一种，通过神经网或者深度学习对词进行训练，得到一个指定维度的向量，将练完后模型参数作为输入词语的向量化表示</p><p>在 NLP 中，把 x 看做一个句子里的一个词语，y 是这个词语的上下文词语，那么这里的 f，便是 NLP 中经常出现的『语言模型』（language model），这个模型的目的，就是判断 (x,y) 这个样本，是否符合自然语言的法则，即词语x和词语y放在一起，是不是人话。</p><p>Word2vec 正是来源于这个思想，它只关心模型训练完后模型参数（这里特指神经网络的权重），并将这些参数，作为输入 x 的某种向量化的表示，这个向量便叫做词向量，而这里的向量其实是经过降维。</p><p><br></p><p><br></p><h1 id="二、语言模型"><a href="#二、语言模型" class="headerlink" title="二、语言模型"></a>二、语言模型</h1><p>Word2vec是种高效率词嵌套学习的预测模型，其两种变体为：</p><ul><li>Skip-gram 模型：将一个词语作为输入，来预测它周围的上下文</li><li>CBOW 模型：将一个词语的上下文作为输入，来预测这个词语本身</li></ul><p><br></p><h2 id="1-Skip-gram-和-CBOW-的简单情形"><a href="#1-Skip-gram-和-CBOW-的简单情形" class="headerlink" title="1. Skip-gram 和 CBOW 的简单情形"></a>1. Skip-gram 和 CBOW 的简单情形</h2><p>用当前词 x 预测它的下一个词 y（Skip-gram）</p><p>用上下文词 x 预测当前词 y（CBOW）  </p><p><strong>（1）one-hot encoder： x 的输入形式</strong></p><p>假设全世界所有的词语总共有 V 个，这 V 个词语有自己的<strong>先后顺序</strong>，假设『吴彦祖』这个词是第1个词，就可以表示为一个 V 维全零向量、把第1个位置的0变成1，『我』这个单词是第2个词，表示为 V 维全零向量、把第2个位置的0变成1。每个词语都可以找到属于自己的唯一表示。</p><p><strong>（2）Skip-gram 的网络结构：x 就是 one-hot encoder 形式的输入，y 是在这 V 个词上输出的概率</strong></p><p><img src="https://pic4.zhimg.com/80/v2-a1a73c063b32036429fbd8f1ef59034b_hd.jpg" alt="img"></p><p><strong>（3）Word2vec ：one-hot encoder形式的输入x，存在与这个位置相对应的被激活的权重vx</strong></p><p>当模型训练完后，最后得到的其实是<strong>神经网络的权重</strong>，比如现在输入一个 x 的 one-hot encoder [1,0,0,…,0]，对应词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，从而这些权重组成一个向量 vx 来表示x，而每个词语的 one-hot encoder 里面 1 的位置是不同的，所以被激活的向量 vx 就可以用来唯一表示 x。</p><p>词向量的维度与隐含层节点数一致，且一般情况下要远远小于词语总数 V 的大小，所以 Word2vec 本质上是一种<strong>降维</strong>操作</p><p>（<strong>！重点理解上面这段话！</strong>）</p><p><br></p><h2 id="2-Skip-gram-更一般的情形"><a href="#2-Skip-gram-更一般的情形" class="headerlink" title="2. Skip-gram 更一般的情形"></a>2. Skip-gram 更一般的情形</h2><p>需要预测的上下文 y 有多个词时</p><p><img src="https://pic2.zhimg.com/80/v2-ca81e19caa378cee6d4ba6d867f4fc7c_hd.jpg" alt="img"></p><p><br></p><h2 id="3-CBOW-更一般的情形"><a href="#3-CBOW-更一般的情形" class="headerlink" title="3. CBOW 更一般的情形"></a>3. CBOW 更一般的情形</h2><p>输入的上下文 x有多个词时</p><p><img src="https://pic3.zhimg.com/80/v2-d1ca2547dfb91bf6a26c60782a26aa02_hd.jpg" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：因为TensorFlow的官网教程里有，所以就顺带了解一下NLP，但不做编程训练，我还是要主攻图像&lt;/p&gt;
&lt;p&gt;这篇博文比较清楚得解释了word2vec模型是个什么 ：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/26306795&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/26306795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow（三）——两层CNN神经网络实现MNIST的分类</title>
    <link href="http://yoursite.com/2018/07/31/TensorFlow%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E4%B8%A4%E5%B1%82CNN%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0MNIST%E7%9A%84%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/07/31/TensorFlow（三）——两层CNN神经网络实现MNIST的分类/</id>
    <published>2018-07-31T02:02:49.000Z</published>
    <updated>2018-07-31T05:25:56.920Z</updated>
    
    <content type="html"><![CDATA[<p><strong>两层CNN神经网络实现MNIST的分类</strong></p><p>version 1：和之前的神经网络训练MNIST数据集实现分类，整体过程差不多，只不过将计算换成卷积</p><p>version 2 ：相比 version 1多了 <strong>权重和卷积结果的图像可视化</strong></p><p>整个网络框架：image —&gt;conv1(32 kernels) —&gt;max pool1 —&gt;conv2 (64 kernels) —&gt;max pool2 </p><p>—&gt;fc1 —&gt;fc2 —&gt;classifier</p><p><br></p><a id="more"></a><h1 id="Version-1"><a href="#Version-1" class="headerlink" title="Version 1"></a>Version 1</h1><p>version 1和之前的神经网络训练MNIST数据集实现分类，整体过程差不多，只不过将计算换成卷积</p><p><br></p><h3 id="1-训练"><a href="#1-训练" class="headerlink" title="1. 训练"></a>1. 训练</h3><p>可以看到loss的值是有震荡的，所以超参数的选取需要改，我估计还是学习率</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_cnn1_train.jpg" alt=""></p><p><br></p><h3 id="2-测试"><a href="#2-测试" class="headerlink" title="2. 测试"></a>2. 测试</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_cnn1_test.jpg" alt=""></p><p><br></p><h3 id="3-代码解读"><a href="#3-代码解读" class="headerlink" title="3. 代码解读"></a>3. 代码解读</h3><h4 id="1-conv2d"><a href="#1-conv2d" class="headerlink" title="1) conv2d"></a>1) conv2d</h4><p><code>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)</code></p><p>input的shape：[batch, in_height, in_width, in_channels] </p><p>filter的shape：[filter_height, filter_width, in_channels, out_channels]</p><p>strides=[1,x,y,1] strides[0]和strides[3]默认值是1，中间两个表示在x方向移动的步数，y方向移动的步数</p><p><br></p><h4 id="2-max-pool"><a href="#2-max-pool" class="headerlink" title="2) max_pool"></a>2) max_pool</h4><p><code>tf.nn.max_pool(value, ksize, strides, padding, name=None)</code></p><p>value的shape：[batch, in_height, in_width, in_channels] </p><p>ksize的shape：池化窗口的大小，[1, height, width, 1]，因为不想在batch和channels上做池化，所以这两个维度设为了1</p><p>strides=[1,x,y,1] strides[0]和strides[3]默认值是1，中间两个表示在x方向移动的步数，y方向移动的步数</p><p>（一般strides=[1,2,2,1]）</p><p><br></p><h4 id="3-dropout"><a href="#3-dropout" class="headerlink" title="3)dropout"></a>3)dropout</h4><p><code>dropout(x, keep_prob, noise_shape=None, seed=None, name=None)</code></p><p>x：输入用于训练的数据</p><p>keep_prob：神经元保留概率</p><p>（需要对随机失活做scaling 缩放： l /= keep_prob  <strong>这一步很重要</strong> ）</p><p><br></p><p><strong>关于 keep_prob 需要注意</strong> </p><p>只有训练的时候 keep_prob &lt;1 才进行 dropout</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(train,feed_dict =&#123;x_input:x_train_batch, y_input:y_train_batch, keep_prob:<span class="number">0.8</span>&#125;,  options = run_options,run_metadata = run_metadata)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(loss_cross_entropy,feed_dict = &#123;x_input:x_train_batch, y_input:y_train_batch, keep_prob:<span class="number">1.0</span>&#125;,options=run_options,run_metadata=run_metadata)</span><br></pre></td></tr></table></figure><p><br></p><h3 id="4-tensorboard"><a href="#4-tensorboard" class="headerlink" title="4. tensorboard"></a>4. tensorboard</h3><h4 id="1-计算图"><a href="#1-计算图" class="headerlink" title="1) 计算图"></a>1) 计算图</h4><p><img src="http://p8ge6t5tt.bkt.clouddn.com/cnn_graph.png" alt=""></p><h4 id="2-scalar"><a href="#2-scalar" class="headerlink" title="2)scalar"></a>2)scalar</h4><p>可以看出震荡还是蛮大的，是超参数没设置好</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/cnn_scalar.png" alt=""> </p><p><br></p><p><br></p><p>PS：相比version 1多了 <strong>权重和卷积结果的图像可视化</strong></p><p>想做卷积层输出结果的图像输出，查了很多资料，总算找到了一个合适的，在此基础上做了些修改</p><p>感觉是个很低调的大神分享的代码，GitHub ：<a href="https://github.com/grishasergei/conviz" target="_blank" rel="noopener">https://github.com/grishasergei/conviz</a></p><p>权值可视化那部分改了很久，一直都报错，差点放弃了，还好坚持print了几个变量的shape，果然我还是对各数据的结构掌握不全，最后做出来的那一刻，真的开心到尖叫，旋转，跳跃，也很感谢大神的分享</p><p><br></p><h3 id="1-训练-1"><a href="#1-训练-1" class="headerlink" title="1. 训练"></a>1. 训练</h3><p>和version 1 一样，losss值震荡，等把几个经典网络学完，就得开始试着自己调参，分析结果，调参……</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_cnn2_train.jpg" alt=""></p><h3 id="2-测试-1"><a href="#2-测试-1" class="headerlink" title="2. 测试"></a>2. 测试</h3><p>与大神分享的代码相比，在权值可视化那部分改动很大，主要是conv_weight[] 这个量在原代码里那样用会在shape上出现错误，所以就改成下面这种</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_cnn2_test.jpg" alt=""></p><p><br></p><h3 id="3-修改"><a href="#3-修改" class="headerlink" title="3. 修改"></a>3. 修改</h3><ol><li><code>x_ = tf.reshape(x_, tf.pack([-1, n]))</code> 改为 <code>x_ = tf.reshape(x_, tf.stack([-1, n]))</code></li><li><code>while step * batch_size &lt; training_iters</code> 改为 <code>for step in range(1,1001)</code></li><li>增加了保存模型的那部分Saver</li><li>session部分的权值可视化</li></ol><p><br></p><h3 id="4-权值可视化"><a href="#4-权值可视化" class="headerlink" title="4. 权值可视化"></a>4. 权值可视化</h3><h4 id="1-conv1"><a href="#1-conv1" class="headerlink" title="1) conv1"></a>1) conv1</h4><p>输入数据的厚度为1，32个滤波器</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/conv1_weight.png" alt=""></p><h4 id="2-conv2"><a href="#2-conv2" class="headerlink" title="2) conv2"></a>2) conv2</h4><p>输入数据的厚度为32，64个滤波器</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/conv2_weight.png" alt=""></p><p>PS：写博客的时候我看这个图是有点晕的，有点一下子转不过弯，这里说明一下图</p><p>每个滤波器的大小5*5</p><p>conv1：总共有32个滤波器，滤波器的通道数为1，所以所有滤波器（32）放在一张图上</p><p>conv2：总共有64个滤波器，滤波器的通道数为32，所以每张小图是所有滤波器（64）对应的一个通道</p><p><br></p><h3 id="5-卷积结果可视化"><a href="#5-卷积结果可视化" class="headerlink" title="5. 卷积结果可视化"></a>5. 卷积结果可视化</h3><h4 id="1-conv1-1"><a href="#1-conv1-1" class="headerlink" title="1) conv1"></a>1) conv1</h4><p>卷积后的厚度为32</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/conv2_output.png" alt=""></p><h4 id="2-conv2-1"><a href="#2-conv2-1" class="headerlink" title="2) conv2"></a>2) conv2</h4><p>卷积后的厚度为64</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/conv1_output.png" alt=""></p><p><br></p><p>写在最后：</p><p>三次练习坐下来TensorFlow总算是入门了，下面开启CNN的经典网络的学习 ~ </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;两层CNN神经网络实现MNIST的分类&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;version 1：和之前的神经网络训练MNIST数据集实现分类，整体过程差不多，只不过将计算换成卷积&lt;/p&gt;
&lt;p&gt;version 2 ：相比 version 1多了 &lt;strong&gt;权重和卷积结果的图像可视化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;整个网络框架：image —&amp;gt;conv1(32 kernels) —&amp;gt;max pool1 —&amp;gt;conv2 (64 kernels) —&amp;gt;max pool2 &lt;/p&gt;
&lt;p&gt;—&amp;gt;fc1 —&amp;gt;fc2 —&amp;gt;classifier&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Ipython 与 Jupyter Notebook</title>
    <link href="http://yoursite.com/2018/07/29/Ipython-%E4%B8%8E-Jupyter-Notebook/"/>
    <id>http://yoursite.com/2018/07/29/Ipython-与-Jupyter-Notebook/</id>
    <published>2018-07-29T11:19:01.000Z</published>
    <updated>2018-07-29T12:07:43.046Z</updated>
    
    <content type="html"><![CDATA[<p>PS：为了理解 Jupyter Notebook 所以展开来查了很多资料，也不知道自己的理解到底对不对</p><p><a href="https://blog.windrunner.me/python/jupyter.html" target="_blank" rel="noopener">https://blog.windrunner.me/python/jupyter.html</a></p><p><a href="http://smilejay.com/2012/10/interactive-shell-login-shell/" target="_blank" rel="noopener">http://smilejay.com/2012/10/interactive-shell-login-shell/</a></p><p><a href="https://blog.csdn.net/wkw1125/article/details/53932945" target="_blank" rel="noopener">https://blog.csdn.net/wkw1125/article/details/53932945</a></p><p><br></p><a id="more"></a><h2 id="IPython-和-python-shell-的区别"><a href="#IPython-和-python-shell-的区别" class="headerlink" title="IPython 和 python shell 的区别"></a>IPython 和 python shell 的区别</h2><p>IPython（Interactive Python shell），是 <strong>python 的交互式 shell</strong>，比 Python shell 功能更强大</p><p><br></p><h3 id="1-交互式-Interactive-是什么意思？"><a href="#1-交互式-Interactive-是什么意思？" class="headerlink" title="1. 交互式 Interactive 是什么意思？"></a>1. 交互式 Interactive 是什么意思？</h3><p>交互式 Interactive Shell</p><p>在终端上执行，shell等待输入，并且立即执行提交的命令，退出后，shell也终止</p><p>非交互式 Non Interactive Shell</p><p>读取存放在文件中的命令，从可以第一条命令执行到最后一条然后退出，不与你进行任何交互</p><p><br></p><h3 id="2-shell-是什么？"><a href="#2-shell-是什么？" class="headerlink" title="2. shell 是什么？"></a>2. shell 是什么？</h3><p>shell： <strong>命令解释器</strong>，处于内核和用户之间，<strong>把用户的指令传递给内核并且把执行结果回显给用户</strong></p><p>命令解释器 shell ，解释执行脚本程序 shell script</p><p>（1）windows：</p><ul><li>explorer.exe（资源管理器）：是windows的图形shell</li><li>cmd（Command shell）：是windows的命令行shell</li><li>windows Power Shell：相比CMD功能更强大</li></ul><p>（2）linux/unix：</p><p>Shell有多种实现，多数Linux发行版本默认是bash</p><p><br></p><h3 id="3-脚本语言是什么？"><a href="#3-脚本语言是什么？" class="headerlink" title="3. 脚本语言是什么？"></a>3. 脚本语言是什么？</h3><ul><li><strong>编程语言：编写-编译-链接-运行</strong></li><li><strong>脚本语言：解释-执行</strong></li></ul><p>脚本语言需要通过对应的解释器解释执行。如Perl、Python、Ruby、JavaScript等都是脚本语言，每种脚本语言都需要其对应的解释器。shell script 也属于一种比较特殊的脚本语言。</p><p><br></p><p><br></p><h2 id="Jupyter-Notebook"><a href="#Jupyter-Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h2><h3 id="1-Jupyter-Notebook-是什么？"><a href="#1-Jupyter-Notebook-是什么？" class="headerlink" title="1. Jupyter Notebook 是什么？"></a>1. Jupyter Notebook 是什么？</h3><p>Jupyter Notebook 是一个<strong>交互式笔记本</strong>，支持运行 40 多种编程语言。</p><p>Jupyter Notebook 的本质是一个 Web 应用程序，<strong>能让用户将说明文本、数学方程、代码和可视化内容全部组合到一个易于共享的文档中</strong>。</p><p>PS：个人理解 Jupyter Notebook 可以看成 txt 文件，就是笔记本，<strong>方便记录</strong></p><p><br></p><h3 id="2-Jupyter-Notebook-哪里方便？"><a href="#2-Jupyter-Notebook-哪里方便？" class="headerlink" title="2.  Jupyter Notebook 哪里方便？"></a>2.  Jupyter Notebook 哪里方便？</h3><p>（1）不用 Jupyter Notebook 之前，说明和代码是分开的</p><p>在IDE（集成开发环境）如Pycharm中写代码，然后在word中写文档说明项目。通常是写完代码，再写文档，有的时候需要重头回顾一遍代码。</p><p>（2）不用 Jupyter Notebook 之前，结果可视化不方便</p><p>为了得到数据分析的中间结果，需要重新跑代码，然后把结果截图放到文档里。</p><p><br></p><h3 id="3-ipynb是什么？"><a href="#3-ipynb是什么？" class="headerlink" title="3. ipynb是什么？"></a>3. ipynb是什么？</h3><p>jupyter保存的文件的扩展名是  .ipynb，<strong>实际上这是一个json文件</strong></p><p><br></p><h3 id="4-什么是JSON？"><a href="#4-什么是JSON？" class="headerlink" title="4. 什么是JSON？"></a>4. 什么是JSON？</h3><p>（1）JSON是一种数据结构，方便数据的传输</p><p>（2）JSON 作为数据的交换格式，在客户端和服务器端完成数据交换的原理</p><ul><li>如果是客户端请求数据，那么服务器端就<strong>将Java对象先转换成 JSON字符串</strong></li><li>经响应把字符串传到客户端之后，客户端就会接收到这个转换结果</li><li>但 JavaScript 要求把这个字符串变成对象格式才更方便访问</li><li>所以在客户端的<strong>JavaScript代码中又需要将这个JSON字符串变成JavaScript能够识别的对象</strong></li></ul><p><br></p><h3 id="5-从-JSON-理解-Jupyter-Notebook"><a href="#5-从-JSON-理解-Jupyter-Notebook" class="headerlink" title="5. 从 JSON 理解 Jupyter Notebook"></a>5. 从 JSON 理解 Jupyter Notebook</h3><p>下面是我个人的想法，从 JSON 理解 Jupyter Notebook</p><p>就是本地服务的网页界面的实现，你的. ipynb文件放在你的本地服务器就是你的电脑上，而浏览器就是客户端要显示该文件的内容，这就需要两方通信进行数据传输</p><p><br></p><p><br></p><p>PS：之前写的 XML、YML 、JSON </p><p><a href="https://sophia0130.github.io/2018/05/14/XML%E3%80%81YAML%E3%80%81JSON/" target="_blank" rel="noopener">https://sophia0130.github.io/2018/05/14/XML%E3%80%81YAML%E3%80%81JSON/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：为了理解 Jupyter Notebook 所以展开来查了很多资料，也不知道自己的理解到底对不对&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.windrunner.me/python/jupyter.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.windrunner.me/python/jupyter.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://smilejay.com/2012/10/interactive-shell-login-shell/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://smilejay.com/2012/10/interactive-shell-login-shell/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/wkw1125/article/details/53932945&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/wkw1125/article/details/53932945&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow（二）——单层神经网络实现MNIST的分类</title>
    <link href="http://yoursite.com/2018/07/26/TensorFlow%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0MNIST%E7%9A%84%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/07/26/TensorFlow（二）——单层神经网络实现MNIST的分类/</id>
    <published>2018-07-26T12:27:03.000Z</published>
    <updated>2018-07-31T07:41:14.748Z</updated>
    
    <content type="html"><![CDATA[<p>PS：<a href="https://playground.tensorflow.org" target="_blank" rel="noopener">https://playground.tensorflow.org</a> 推荐一个很有意思的网站，够你玩一个下午的，并且能够帮助你更好理解什么是调参 ~</p><p><br></p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>MNIST 手写数字库</p><p>单层神经网络实现MNIST的分类</p><ul><li>训练训练集</li><li>测试测试集</li></ul><ul><li>可视化  tensorboard<ul><li>模型参数</li><li>loss、accuracy</li><li>网络流程图</li></ul></li><li>参数的保存 Saver</li><li>Timeline</li></ul><a id="more"></a><p><br></p><h2 id="一、MNIST-手写数字数据库"><a href="#一、MNIST-手写数字数据库" class="headerlink" title="一、MNIST 手写数字数据库"></a>一、MNIST 手写数字数据库</h2><h3 id="1-数据集"><a href="#1-数据集" class="headerlink" title="1.数据集"></a>1.数据集</h3><p>分为60000行的训练数据集（<code>mnist.train</code>）和10000行的测试数据集（<code>mnist.test</code>）</p><p>每张图片的像素 28x28 被展开为长度 784 的行向量</p><p>训练数据：</p><p>图片：<code>mnist.train.images</code> 是一个形状为 <code>[60000, 784]</code> </p><p>图片的标签：<code>mnist.train.labels</code> 是一个 <code>[60000, 10]</code>    <strong>使用的是one-hot 独热码</strong></p><p>one-hot：使用N位状态寄存器来对N个状态进行编码，并且在任意时候，只有一位有效。最大优势在于状态比较时仅仅需要比较一个位，运算简单，但是占用资源多</p><p><br></p><h3 id="2-MNIST手写数字数据集的下载"><a href="#2-MNIST手写数字数据集的下载" class="headerlink" title="2.MNIST手写数字数据集的下载"></a>2.MNIST手写数字数据集的下载</h3><p>使用下面的代码加载失败</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.examples.tutorials.mnist.input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</span><br></pre></td></tr></table></figure><p><em>由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败</em></p><p>采取下面的办法解决  <a href="https://blog.csdn.net/Li_haiyu/article/details/78465806" target="_blank" rel="noopener">https://blog.csdn.net/Li_haiyu/article/details/78465806</a></p><p><br></p><h3 id="3-PCA-可视化-MNIST-手写数字识别数据集"><a href="#3-PCA-可视化-MNIST-手写数字识别数据集" class="headerlink" title="3.PCA 可视化 MNIST 手写数字识别数据集"></a>3.PCA 可视化 MNIST 手写数字识别数据集</h3><p><a href="http://projector.tensorflow.org/" target="_blank" rel="noopener">http://projector.tensorflow.org/</a></p><p>PCA 是一种常用的数据降维方法，可以将高维数据在二维或者三维可视化呈现</p><p><br></p><h3 id="4-MNIST的可视化"><a href="#4-MNIST的可视化" class="headerlink" title="4.MNIST的可视化"></a>4.MNIST的可视化</h3><p>两种方法：</p><ul><li>tensorflow 中的 MNIST 模块读取图片（不需要将文件解压缩）</li><li>struct模块从二进制文件中读取图片（需要将文件解压缩）</li></ul><p><a href="http://www.cnblogs.com/youmuchen/p/6713241.html" target="_blank" rel="noopener">http://www.cnblogs.com/youmuchen/p/6713241.html</a></p><p><a href="https://www.cnblogs.com/zhouyang209117/p/6436751.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhouyang209117/p/6436751.html</a></p><p><br></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/MNIST_show.jpg" alt=""></p><p><br></p><p><br></p><h2 id="二、代码实现"><a href="#二、代码实现" class="headerlink" title="二、代码实现"></a>二、代码实现</h2><h3 id="1-代码流程"><a href="#1-代码流程" class="headerlink" title="1.代码流程"></a>1.代码流程</h3><ul><li>输入训练集和测试集</li><li>构造神经网络<ul><li>输入训练样本的ｘ[55000,784]、ｙ[55000,10] 一个样本为一行</li><li>设置神经网络参数 W[784,10]、b[10]</li><li>计算 y_prediction = x*W + b</li></ul></li><li>softmax计算10种类别各自的概率</li><li>交叉熵计算loss</li><li>训练网络</li><li>用训练集计算准确率</li></ul><p><br></p><h3 id="2-附加功能"><a href="#2-附加功能" class="headerlink" title="2.附加功能"></a>2.附加功能</h3><ul><li>可视化  tensorboard<ul><li>参数</li><li>loss、accuracy</li><li>图片</li></ul></li><li>参数的保存 Saver</li><li>Timeline</li></ul><p><br></p><h3 id="3-贴代码"><a href="#3-贴代码" class="headerlink" title="3.贴代码"></a>3.贴代码</h3><ul><li><p>train_or_test == 0 时训练网络得到参数</p><p>可以看到每隔50次训练的loss值</p></li></ul><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_train.jpg" alt=""></p><ul><li><p>train_or_test == 1 用得到的模型计算测试集精确度</p><p>测试集的精确度有0.88</p></li></ul><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_test.jpg" alt=""></p><p><br></p><p><br></p><h2 id="三、tensorboard"><a href="#三、tensorboard" class="headerlink" title="三、tensorboard"></a>三、tensorboard</h2><ul><li>网络结构</li></ul><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_graph.png" alt=""></p><p><br></p><ul><li>每隔50次训练的loss</li></ul><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_loss.png" alt=""></p><p><br></p><ul><li>每隔50次测试集的精确度</li></ul><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_accuracy.png" alt=""></p><p><br></p><p><br></p><h2 id="四、Timeline"><a href="#四、Timeline" class="headerlink" title="四、Timeline"></a>四、Timeline</h2><p>不要问我这个是啥意思，我也不知道，看不懂 ~</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_timeline.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：&lt;a href=&quot;https://playground.tensorflow.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://playground.tensorflow.org&lt;/a&gt; 推荐一个很有意思的网站，够你玩一个下午的，并且能够帮助你更好理解什么是调参 ~&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;p&gt;MNIST 手写数字库&lt;/p&gt;
&lt;p&gt;单层神经网络实现MNIST的分类&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;训练训练集&lt;/li&gt;
&lt;li&gt;测试测试集&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;可视化  tensorboard&lt;ul&gt;
&lt;li&gt;模型参数&lt;/li&gt;
&lt;li&gt;loss、accuracy&lt;/li&gt;
&lt;li&gt;网络流程图&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;参数的保存 Saver&lt;/li&gt;
&lt;li&gt;Timeline&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow（一）——两层神经网络预测线性回归</title>
    <link href="http://yoursite.com/2018/07/26/TensorFlow%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E4%B8%A4%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%A2%84%E6%B5%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/07/26/TensorFlow（一）——两层神经网络预测线性回归/</id>
    <published>2018-07-26T12:25:12.000Z</published>
    <updated>2018-07-26T12:25:57.979Z</updated>
    
    <content type="html"><![CDATA[<p>PS：两层的神经网络训练线性回归，代码参考了莫烦的，GitHub上有，然后又往里面加了关于Timeline的部分</p><a id="more"></a><p><br></p><h2 id="一、代码"><a href="#一、代码" class="headerlink" title="一、代码"></a>一、代码</h2><p><img src="http://p8ge6t5tt.bkt.clouddn.com/nonlinerregression.jpg" alt=""></p><p><br></p><p><br></p><h2 id="二、tensorboard"><a href="#二、tensorboard" class="headerlink" title="二、tensorboard"></a>二、tensorboard</h2><p>PS ：真心不是很会看这个分析图，可以看看下面这篇博客</p><p><a href="https://blog.csdn.net/u010099080/article/details/77426577" target="_blank" rel="noopener">https://blog.csdn.net/u010099080/article/details/77426577</a></p><p><br></p><h3 id="SCALAR"><a href="#SCALAR" class="headerlink" title="SCALAR"></a>SCALAR</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/tf_scalar.jpg" alt=""></p><ol><li>图的左下角的3 个小图标，第一个：查看大图，第二个：是否对 y 轴对数化，第三个：是放大和复原</li><li>术语解释：</li></ol><p>Smoothing ： 指的是作图时曲线的平滑程度，使用类似指数平滑的处理方法，默认是 0.6。</p><p>STEP：横轴显示的是训练迭代次数</p><p>RELATIVE：训练的相对时间，相对于训练开始的时间</p><p>WALL：训练的绝对时间</p><p><br></p><p><br></p><h3 id="DISTRIBUTIONS"><a href="#DISTRIBUTIONS" class="headerlink" title="DISTRIBUTIONS"></a>DISTRIBUTIONS</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/tf_distribution.jpg" alt=""></p><p>从上到下的折线，随着横轴训练次数，纵轴参数的多分位数图</p><p>从上到下表示不同的分位数  [maximum, 93%, 84%, 69%, 50%, 31%, 16%, 7%, minimum]</p><p><br></p><p><br></p><h3 id="HISTOGRAMS"><a href="#HISTOGRAMS" class="headerlink" title="HISTOGRAMS"></a>HISTOGRAMS</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/tf_histogram.jpg" alt=""></p><ol><li>HISTOGRAMS 和 DISTRIBUTIONS 是对同一数据不同方式的展现。HISTOGRAMS是 频数分布直方图的堆叠，横轴表示权重值，纵轴表示训练步数。颜色越深表示时间越早，越浅表示越接近训练结束</li><li>两种模式：都是横轴为权重值，纵轴为频数</li></ol><p>OVERLAY  二维表示</p><p>OFFSET 三维表示</p><p><br></p><p><br></p><h3 id="GRAPH"><a href="#GRAPH" class="headerlink" title="GRAPH"></a>GRAPH</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/tf_graph.png" alt=""></p><ol><li><p>图像颜色两种模式：</p><p>基于结构的模式（Structure），相同的节点会有同样的颜色</p><p>基于硬件（Device），同一个硬件上的会有相同颜色</p></li><li><p>术语解释：</p><p>Run 创建了几个不同的Session（改变了超参数），对应的计算图</p><p>Session runs：一个session里，不同训练次数，对应的计算图</p></li></ol><p><br></p><p><br></p><h2 id="三、Timeline"><a href="#三、Timeline" class="headerlink" title="三、Timeline"></a>三、Timeline</h2><p>真心看不懂</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/tf_timeline.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：两层的神经网络训练线性回归，代码参考了莫烦的，GitHub上有，然后又往里面加了关于Timeline的部分&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（二）——最优化</title>
    <link href="http://yoursite.com/2018/07/26/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2018/07/26/cs231n笔记（二）——最优化/</id>
    <published>2018-07-26T12:23:52.000Z</published>
    <updated>2018-07-26T12:24:22.052Z</updated>
    
    <content type="html"><![CDATA[<p><strong>目录</strong></p><p>1.解决分类的三步骤</p><p>2.最优化</p><p>3.梯度计算</p><a id="more"></a><p><br></p><h2 id="一、分类问题三步骤"><a href="#一、分类问题三步骤" class="headerlink" title="一、分类问题三步骤"></a>一、分类问题三步骤</h2><p><strong>1. 评分函数（score function）</strong> ：将原始图像数据<strong>映射为各个类别的得分</strong>，得分高低代表图像属于该类别的可能性高低。</p><p>线性分类评分函数的线性映射：<img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%2Cb%29%3DWx_i%2Bb" alt="\displaystyle f(x_i,W,b)=Wx_i+b"></p><p><strong>2. 损失函数（loss function）</strong> ：量化分类标签的得分与真实标签之间一致性。也就是<strong>量化某个具体权重集W的质量</strong> 。</p><p><strong>3.最优化（Optimization）</strong> ：找到能够最小化损失函数值的W。</p><p><br></p><p><br></p><h2 id="二、最优化-Optimization"><a href="#二、最优化-Optimization" class="headerlink" title="二、最优化 Optimization"></a>二、最优化 Optimization</h2><h3 id="1-随机搜索"><a href="#1-随机搜索" class="headerlink" title="1.随机搜索"></a>1.随机搜索</h3><p>尝试了若干随机生成的权重矩阵W，其中某些的损失值较小，而另一些的损失值大些。我们可以把这次随机搜索中找到的最好的权重W取出，然后去跑测试集</p><p><br></p><h3 id="2-随机本地搜索"><a href="#2-随机本地搜索" class="headerlink" title="2.随机本地搜索"></a>2.随机本地搜索</h3><p>从一个随机<img src="http://www.zhihu.com/equation?tex=W" alt="W">开始，然后生成一个随机的扰动<img src="http://www.zhihu.com/equation?tex=%5Cdelta+W" alt="\delta W"> ，只有当<img src="http://www.zhihu.com/equation?tex=W%2B%5Cdelta+W" alt="W+\delta W">的损失值变低，才更新。</p><p><br></p><h3 id="3-梯度下降"><a href="#3-梯度下降" class="headerlink" title="3.梯度下降"></a>3.梯度下降</h3><p>前两个方法中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实<strong>不需要随机寻找方向</strong>，可以直接计算出最陡峭的方向，这个方向就是损失函数的梯度（gradient）。</p><p>方向导数：某个方向上的导数</p><p>梯度：是一个矢量，是方向导数中取到最大值的方向，梯度的值是方向导数的最大值。</p><p><br></p><p><br></p><h2 id="三、计算梯度"><a href="#三、计算梯度" class="headerlink" title="三、计算梯度"></a>三、计算梯度</h2><p>计算梯度有两种方法，一个是缓慢的近似方法（数值梯度法），但实现相对简单。另一个方法（分析梯度法）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。</p><p><br></p><h3 id="1-数值计算梯度法"><a href="#1-数值计算梯度法" class="headerlink" title="1.数值计算梯度法"></a>1.数值计算梯度法</h3><p>使用有限差值近似计算梯度   <img src="http://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7Bdf%28x%29%7D%7Bdx%7D%3D%5Clim_%7Bh%5Cto+0%7D%5Cfrac%7Bf%28x%2Bh%29-f%28x%29%7D%7Bh%7D" alt="\displaystyle\frac{df(x)}{dx}=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}"></p><p>对所有维度进行迭代，在每个维度上产生一个很小的变化h，计算函数在该维度上的偏导数。</p><p>注意在数学公式中，<strong>h</strong>的取值是趋近于0的，然而在实际中，用一个很小的数值（比如1e-5）。还有，实际中用中心差值公式（<strong>centered difference formula）</strong><img src="http://www.zhihu.com/equation?tex=%5Bf%28x%2Bh%29-f%28x-h%29%5D%2F2h" alt="[f(x+h)-f(x-h)]/2h">效果较好。</p><p><strong>缺点是效率低</strong>：计算数值梯度的复杂性和参数的量线性相关。如果有30730个参数，损失函数每走一步就需要计算30731次损失函数的梯度。现代神经网络很容易就有上千万的参数，显然这个策略不适合大规模数据。</p><p><br></p><h3 id="2-微分分析计算梯度"><a href="#2-微分分析计算梯度" class="headerlink" title="2.微分分析计算梯度"></a>2.微分分析计算梯度</h3><p>利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。</p><p><br></p><h3 id="3-梯度检查"><a href="#3-梯度检查" class="headerlink" title="3.梯度检查"></a>3.梯度检查</h3><p>由于微分分析计算梯度容易出错 ，实际操作时常常<strong>将分析梯度法的结果和数值梯度法的结果作比较</strong>，以此来<strong>检查其实现的正确性</strong>，这个步骤叫做<strong>梯度检查</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;目录&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1.解决分类的三步骤&lt;/p&gt;
&lt;p&gt;2.最优化&lt;/p&gt;
&lt;p&gt;3.梯度计算&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>进程和线程</title>
    <link href="http://yoursite.com/2018/07/26/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/07/26/进程和线程/</id>
    <published>2018-07-26T09:38:09.000Z</published>
    <updated>2018-07-26T09:41:15.879Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-进程和线程"><a href="#1-进程和线程" class="headerlink" title="1.进程和线程"></a>1.进程和线程</h3><ul><li><p>进程：运行中的应用程序称为进程，拥有系统资源（cpu、内存）</p></li><li><p>线程：进程中的一段代码，一个进程中可以有多段代码。本身不拥有资源（共享所在进程的资源）</p><p><br></p></li></ul><ul><li>多进程: 在操作系统中能同时运行多个任务(程序)</li><li>多线程: 在同一应用程序中有多个功能流同时执行</li></ul><p><br></p><h3 id="2-多线程并行和并发"><a href="#2-多线程并行和并发" class="headerlink" title="2.多线程并行和并发"></a>2.多线程并行和并发</h3><p>多线程并行 Parallelism：在不同CPU上（多核），同时执行多个线程</p><p>多线程并发 Concurrency ：在一个CPU上，做多线程之间的切换，如果CPU调度线程的时间足够快，就造成了多线程并发执行的假象</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-进程和线程&quot;&gt;&lt;a href=&quot;#1-进程和线程&quot; class=&quot;headerlink&quot; title=&quot;1.进程和线程&quot;&gt;&lt;/a&gt;1.进程和线程&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;进程：运行中的应用程序称为进程，拥有系统资源（cpu、内存）&lt;/p&gt;
&lt;/li&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs231n笔记（五）——卷积神经网络</title>
    <link href="http://yoursite.com/2018/07/25/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2018/07/25/cs231n笔记（五）——卷积神经网络/</id>
    <published>2018-07-25T03:35:57.000Z</published>
    <updated>2018-07-26T12:32:23.799Z</updated>
    
    <content type="html"><![CDATA[<p>PS：理解卷积和卷积神经网络</p><p><br></p><a id="more"></a><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><h3 id="一、自相关、互相关"><a href="#一、自相关、互相关" class="headerlink" title="一、自相关、互相关"></a>一、自相关、互相关</h3><p>两个相关函数都是对相似性的度量</p><ul><li><p>自相关：R(u)=f(t)*f(-t)</p><p>函数自身的周期性强弱</p></li></ul><ul><li><p>互相关：R(u)=f(t)*g(-t)</p><p>两个函数在不同的相对位置上互相匹配的程度</p></li></ul><p><br></p><h3 id="二、卷积层与全连接层的共同点和不同点"><a href="#二、卷积层与全连接层的共同点和不同点" class="headerlink" title="二、卷积层与全连接层的共同点和不同点"></a>二、卷积层与全连接层的共同点和不同点</h3><p>1.全连接层</p><p>权重看作是展开的一维图形模板，输入的图像也是展开的一维向量，两个向量做点积，若两个向量越相近，则点积的结果越大，即输入图像与图像模板的匹配程度越高。</p><p>2.卷积层</p><p>卷积神经网络中，卷积核是多维特征模板，但是不是整幅图像与模板匹配，而是图像的一部分与模板匹配，点积越大，也就是说匹配程度越大。</p><p><br></p><!--more--><h3 id="三、点积与叉积"><a href="#三、点积与叉积" class="headerlink" title="三、点积与叉积"></a>三、点积与叉积</h3><h4 id="1-点积、内积、数量积"><a href="#1-点积、内积、数量积" class="headerlink" title="1.点积、内积、数量积"></a>1.点积、内积、数量积</h4><p>$$<br>\mathbf{a}\cdot \mathbf{b} = \sum_{i=1}^n a_ib_i = a_1b_1 + a_2b_2 + \cdots + a_nb_n<br>$$</p><p>几何意义：b在a方向上的投影</p><p><br></p><h4 id="2-叉积、外积、向量积"><a href="#2-叉积、外积、向量积" class="headerlink" title="2.叉积、外积、向量积"></a>2.叉积、外积、向量积</h4><p>$$<br>\mathbf{a} \cdot \mathbf{b} = \mathbf{a} \mathbf{b}^T<br>$$</p><p>几何意义：a和b构成的平行四边形的面积</p><p><br></p><p><br></p><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p><br></p><h3 id="一、卷积层的处理"><a href="#一、卷积层的处理" class="headerlink" title="一、卷积层的处理"></a>一、卷积层的处理</h3><h4 id="1-padding：对原始图片在边界上进行填充填充"><a href="#1-padding：对原始图片在边界上进行填充填充" class="headerlink" title="1.padding：对原始图片在边界上进行填充填充"></a>1.padding：对原始图片在边界上进行填充填充</h4><p>每个方向扩展像素点数量为 p，填充后原始图片的大小为 (n+2p) <em> (n+2p)，滤波器保持 f </em> f不变，则输出图片大小为 (n+2p-f+1) * (n+2p-f+1)</p><p><br></p><ul><li><strong>Valid 卷积</strong>：不填充，直接卷积，结果大小为 (n-f+1) * (n-f+1)</li><li><strong>Same 卷积</strong>：进行填充，使得卷积后结果大小与输入一致 n+2p-f+1 = n ，得出 p =（f-1）/2</li></ul><p><br></p><h4 id="2-stride：卷积走的步长"><a href="#2-stride：卷积走的步长" class="headerlink" title="2.stride：卷积走的步长"></a>2.stride：卷积走的步长</h4><p>设步长为 s，填充长度为 p，输入图片大小为 n <em> n，滤波器大小为 f </em> f，则卷积后图片的尺寸为：</p><p>⌊（n+2p-f）/s+1⌋×⌊（n+2p-f）/s+1⌋⌊n+2p−fs+1⌋×⌊n+2p−fs+1⌋</p><p>⌊ ⌋ ：<strong>向下取整</strong>的符号</p><p><br></p><h3 id="二、卷积神经网络（CNN）中的三种层"><a href="#二、卷积神经网络（CNN）中的三种层" class="headerlink" title="二、卷积神经网络（CNN）中的三种层"></a>二、卷积神经网络（CNN）中的三种层</h3><h4 id="1-卷积层（Convolution-layer）"><a href="#1-卷积层（Convolution-layer）" class="headerlink" title="1.卷积层（Convolution layer）"></a>1.卷积层（Convolution layer）</h4><p>卷积核在上一级输入层上通过逐一滑动窗口计算而得，卷积核中的每一个参数都相当于传统神经网络中的权值参数，与对应的局部像素相连接，将卷积核的各个参数与对应的局部像素值相乘之和，通常还要再<strong>加上一个偏置参数</strong>。</p><p><br></p><h4 id="2-池化-采样层（Pooling-layer）"><a href="#2-池化-采样层（Pooling-layer）" class="headerlink" title="2.池化/采样层（Pooling layer）"></a>2.池化/采样层（Pooling layer）</h4><p>往往在卷积层后面，<strong>通过池化来降低卷积层输出的特征向量，不易出现过拟合</strong><br><br><br><strong>Max Pooling</strong>: 选择Pooling窗口中的最大值作为采样值<br><strong>Mean Pooling</strong>: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值</p><p><br></p><h4 id="3-全连接层（Fully-Connected-layer）"><a href="#3-全连接层（Fully-Connected-layer）" class="headerlink" title="3.全连接层（Fully Connected layer）"></a>3.全连接层（Fully Connected layer）</h4><p>每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来<br><br></p><h4 id="4-三种层在参数上的区别"><a href="#4-三种层在参数上的区别" class="headerlink" title="4.三种层在参数上的区别"></a>4.三种层在参数上的区别</h4><p>卷积层——仅有少量的参数<br>池化层——没有参数<br>全连接层——存在大量的参数</p><p><br></p><h4 id="5-注意"><a href="#5-注意" class="headerlink" title="5.注意"></a>5.注意</h4><ol><li>在计算神经网络的层数时，通常<strong>只统计具有权重和参数的层</strong>，因此<strong>池化层通常和之前的卷积层共同计为一层</strong>。</li><li>随着网络的深入，<strong>提取的特征图片大小将会逐渐减小，但同时通道数量应随之增加</strong></li></ol><p><br></p><p><br></p><h3 id="三、卷积运算参数少的机制"><a href="#三、卷积运算参数少的机制" class="headerlink" title="三、卷积运算参数少的机制"></a>三、卷积运算参数少的机制</h3><p>PS：卷积神经网络的参数少，从而解决了，一般神经网络处理图像数据时，需要用大大量参数的问题</p><p><br></p><p><strong>１．参数共享</strong>（Parameter sharing）：<strong>在卷积过程中，不管输入有多大，一个滤波器能对整个输入的某一特征进行探测</strong><br>２．<strong>稀疏连接</strong>（Sparsity of connections）：滤波器的尺寸限制使得，<strong>每个输出值只取决于输入在局部的一小部分的值</strong>，稀疏连接不同于全连接，每个输出综合了所有的输入特征</p><p><br></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：理解卷积和卷积神经网络&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（四）——神经网络</title>
    <link href="http://yoursite.com/2018/07/18/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2018/07/18/cs231n笔记（四）——神经网络/</id>
    <published>2018-07-18T01:46:56.000Z</published>
    <updated>2018-07-25T03:32:57.189Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络知识点大纲总结，还没有看完，所以持续更新 ~ </p><a id="more"></a><p><br></p><h2 id="一、非线性激活函数"><a href="#一、非线性激活函数" class="headerlink" title="一、非线性激活函数"></a>一、非线性激活函数</h2><h3 id="1-Sigmoid"><a href="#1-Sigmoid" class="headerlink" title="1.Sigmoid"></a>1.Sigmoid</h3><h3 id="2-tanh"><a href="#2-tanh" class="headerlink" title="2.tanh"></a>2.tanh</h3><h3 id="3-ReLU"><a href="#3-ReLU" class="headerlink" title="3.ReLU"></a>3.ReLU</h3><h3 id="4-Leaky-ReLU"><a href="#4-Leaky-ReLU" class="headerlink" title="4.Leaky ReLU"></a>4.Leaky ReLU</h3><h3 id="5-PReLU"><a href="#5-PReLU" class="headerlink" title="5.PReLU"></a>5.PReLU</h3><h3 id="6-EReLU"><a href="#6-EReLU" class="headerlink" title="6.EReLU"></a>6.EReLU</h3><h3 id="7-maxout"><a href="#7-maxout" class="headerlink" title="7.maxout"></a>7.maxout</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B01.jpg" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B02.png" alt=""></p><p><strong>使用非线性激活函数的原因</strong></p><p>激活函数是线性，比如，f(x) = x，那么每一层输出都是上层输入的线性函数，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果一样</p><p><br></p><p><strong>使用非线性函数的注意点</strong></p><p>ReLU非线性函数。注意设置好<strong>学习率</strong>，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰，就试试Leaky ReLU或者Maxout，不要再用sigmoid。</p><p><br></p><p><br></p><h2 id="二、数据预处理"><a href="#二、数据预处理" class="headerlink" title="二、数据预处理"></a>二、数据预处理</h2><h3 id="1-均值减法（Mean-subtraction）"><a href="#1-均值减法（Mean-subtraction）" class="headerlink" title="1.均值减法（Mean subtraction）"></a>1.均值减法（Mean subtraction）</h3><p>每个独立特征减去平均值，从几何上可以理解为在每个维度上都<strong>将数据云的中心都迁移到原点</strong>。</p><p><br></p><h3 id="2-归一化（Normalization）"><a href="#2-归一化（Normalization）" class="headerlink" title="2.归一化（Normalization）"></a>2.归一化（Normalization）</h3><p>做零中心化（zero-centered）处理，然后每个维度都除以其标准差。</p><p>第一种是对整幅图像做零中心化（zero-centered）处理，然后每个维度都除以其标准差</p><p>第二种是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。</p><p><br></p><h3 id="2-PCA和白化（Whitening）"><a href="#2-PCA和白化（Whitening）" class="headerlink" title="2.PCA和白化（Whitening）"></a>2.PCA和白化（Whitening）</h3><p><strong>注意：</strong></p><p>1.进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。</p><p>例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。<strong>应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。</strong></p><p>2.实际上在卷积神经网络中并不会采用PCA和白化变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。</p><p><br></p><p><br></p><h2 id="三、权重初始化"><a href="#三、权重初始化" class="headerlink" title="三、权重初始化"></a>三、权重初始化</h2><h3 id="1-错误：全零初始化"><a href="#1-错误：全零初始化" class="headerlink" title="1.错误：全零初始化"></a>1.错误：全零初始化</h3><p>具有对称性</p><p><br></p><h3 id="2-随机数初始化"><a href="#2-随机数初始化" class="headerlink" title="2.随机数初始化"></a>2.随机数初始化</h3><p><strong>初始化为小数</strong>：权重太小，随着网络的层次加深，激励函数的结果接近于0，也就是下一层的输入接近于0，那么在反向传播的时候就会计算出非常小的梯度。</p><p><strong>初始化为太大的数</strong>：会发生饱和使梯度消失</p><p><br></p><h3 id="3-Xavier-initialization"><a href="#3-Xavier-initialization" class="headerlink" title="3.Xavier initialization"></a>3.Xavier initialization</h3><p>Var(wi)=1/n[l-1]     n输入的神经元个数</p><p>为了得到较小的 wi，设置Var(wi)=1/n。就是将wi的方差设为1/n，标准差设为sqrt(1/n) </p><p><br></p><h3 id="4-He-Initialization"><a href="#4-He-Initialization" class="headerlink" title="4.He Initialization"></a>4.He Initialization</h3><p>Var(wi)=2/n[l-1]     n输入的神经元个数</p><p><br></p><p><br></p><h2 id="四、Batch-Normalization（BN）"><a href="#四、Batch-Normalization（BN）" class="headerlink" title="四、Batch Normalization（BN）"></a>四、Batch Normalization（BN）</h2><p>1.对各神经元的输入做类似的标准化处理，提高神经网络训练速度</p><p>2.减小不同的样本分布（Covariate Shift ）对后面几层网络的影响，使整体网络更加健壮。</p><p><br></p><p><br></p><h2 id="五、过拟合Overfitting的解决方式正则化-Regularization"><a href="#五、过拟合Overfitting的解决方式正则化-Regularization" class="headerlink" title="五、过拟合Overfitting的解决方式正则化 Regularization"></a>五、过拟合Overfitting的解决方式正则化 Regularization</h2><h3 id="方法一：L2-Regularization-正则化"><a href="#方法一：L2-Regularization-正则化" class="headerlink" title="方法一：L2  Regularization 正则化"></a>方法一：L2  Regularization 正则化</h3><p>PS：<strong>过拟合时的权重很大，正则化就是让权重不那么大</strong></p><p>1.<strong>过拟合的时候权重会很大</strong></p><p>过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，从而使得很小的区间里，函数值的变化很剧烈，即导数值的绝对值非常大。由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。</p><p>2.正则化防止过拟合的原因</p><p>正则化项对于原权重是个衰减项，从而使得权重不会变得太大</p><p><br></p><h3 id="方法二：droupout"><a href="#方法二：droupout" class="headerlink" title="方法二：droupout"></a>方法二：droupout</h3><p>随机失活是<strong>为每个神经元结点设置一个随机消除的概率</strong>，多地被使用在计算机视觉领域。</p><p><strong>注意，在测试阶段不要使用 dropout</strong></p><p><br></p><h3 id="方法三：数据扩增（Data-Augmentation）"><a href="#方法三：数据扩增（Data-Augmentation）" class="headerlink" title="方法三：数据扩增（Data Augmentation）"></a>方法三：数据扩增（Data Augmentation）</h3><p><br></p><h3 id="方法四：早停止法（Early-Stopping）"><a href="#方法四：早停止法（Early-Stopping）" class="headerlink" title="方法四：早停止法（Early Stopping）"></a>方法四：早停止法（Early Stopping）</h3><p><br></p><h3 id="方法五：Drop-Connect"><a href="#方法五：Drop-Connect" class="headerlink" title="方法五：Drop Connect"></a>方法五：Drop Connect</h3><p><br></p><h3 id="方法六：Fractional-Max-Pooling"><a href="#方法六：Fractional-Max-Pooling" class="headerlink" title="方法六：Fractional Max Pooling"></a>方法六：Fractional Max Pooling</h3><p><br></p><h3 id="方法七：Stochastic-Depth"><a href="#方法七：Stochastic-Depth" class="headerlink" title="方法七：Stochastic Depth"></a>方法七：Stochastic Depth</h3><p><br></p><p><br></p><h2 id="六、超参数的选择"><a href="#六、超参数的选择" class="headerlink" title="六、超参数的选择"></a>六、超参数的选择</h2><h3 id="1-交叉验证"><a href="#1-交叉验证" class="headerlink" title="1.交叉验证"></a>1.交叉验证</h3><p><br></p><h3 id="2-随机搜索优于网格搜索"><a href="#2-随机搜索优于网格搜索" class="headerlink" title="2.随机搜索优于网格搜索"></a>2.随机搜索优于网格搜索</h3><p><br></p><h3 id="3-为参数选择合适的范围"><a href="#3-为参数选择合适的范围" class="headerlink" title="3.为参数选择合适的范围"></a>3.为参数选择合适的范围</h3><p>有一些超参数做均匀随机取值是不合适的，需要按照一定的比例在不同的小范围内进行均匀随机取值。</p><p><br></p><h3 id="4-计算资源来决定训练模型的方式"><a href="#4-计算资源来决定训练模型的方式" class="headerlink" title="4.计算资源来决定训练模型的方式"></a>4.计算资源来决定训练模型的方式</h3><ul><li>Panda（熊猫方式）：<strong>受计算能力所限</strong>，同时试验大量模型比较困难。可以专注于<strong>试验一个或一小批模型</strong>，初始化，试着让其工作运转，观察它的表现，不断调整参数。</li><li>Caviar（鱼子酱方式）：拥有足够的计算能力去<strong>平行试验很多模型</strong>，尝试很多不同的超参数，选取效果最好的模型。</li></ul><p><br></p><p><br></p><h2 id="七、梯度下降的更新方法"><a href="#七、梯度下降的更新方法" class="headerlink" title="七、梯度下降的更新方法"></a>七、梯度下降的更新方法</h2><p><strong>加速神经网络的训练</strong></p><h3 id="1-SGD（随机梯度下降），Momentum动量，Nesterov动量"><a href="#1-SGD（随机梯度下降），Momentum动量，Nesterov动量" class="headerlink" title="1.SGD（随机梯度下降），Momentum动量，Nesterov动量"></a>1.SGD（随机梯度下降），Momentum动量，Nesterov动量</h3><p><br></p><h3 id="2-学习率退火"><a href="#2-学习率退火" class="headerlink" title="2.学习率退火"></a>2.学习率退火</h3><p><br></p><h3 id="3-二阶方法"><a href="#3-二阶方法" class="headerlink" title="3.二阶方法"></a>3.二阶方法</h3><p><br></p><h3 id="4-逐参数适应学习率方法（Adagrad，RMSProp）"><a href="#4-逐参数适应学习率方法（Adagrad，RMSProp）" class="headerlink" title="4.逐参数适应学习率方法（Adagrad，RMSProp）"></a>4.逐参数适应学习率方法（Adagrad，RMSProp）</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;神经网络知识点大纲总结，还没有看完，所以持续更新 ~ &lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（三）——分段式反向传播求梯度</title>
    <link href="http://yoursite.com/2018/07/16/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E5%88%86%E6%AE%B5%E5%BC%8F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B1%82%E6%A2%AF%E5%BA%A6/"/>
    <id>http://yoursite.com/2018/07/16/cs231n笔记（三）——分段式反向传播求梯度/</id>
    <published>2018-07-16T08:50:13.000Z</published>
    <updated>2018-07-22T02:58:32.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>1.反向传播</p><p>反向传播是利用<strong>链式法则</strong> ，递归计算梯度的方法</p><p>2.<strong>分段反向传播求梯度</strong>  (重点)</p><p><strong>不需要关于输入变量的明确的函数来计算导数</strong>，因为需要求的是梯度值。只需要将表达式分成不同的可以求导的模块，然后在反向传播中一步一步地计算<strong>梯度</strong>。</p><p>3.加法、乘法和取最大值在反向传播中关于梯度的理解</p><p>4.用向量化计算梯度</p><a id="more"></a><p><br></p><h2 id="一、反向传播"><a href="#一、反向传播" class="headerlink" title="一、反向传播"></a>一、反向传播</h2><p>反向传播是利用<strong>链式法则</strong>递归计算表达式的梯度的方法，链式法则是指用相乘将梯度表达式链接起来。</p><p><br></p><p><br></p><h2 id="二、分段反向传播求梯度"><a href="#二、分段反向传播求梯度" class="headerlink" title="二、分段反向传播求梯度"></a>二、分段反向传播求梯度</h2><p>不需要关于输入变量的明确的函数来计算导数，需要求的是梯度值。</p><p>分段计算是指将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。实际上<strong>并不需要关于输入变量的明确的函数来计算导数</strong>，只需要将表达式分成不同的可以求导的模块，然后在反向传播中一步一步地计算<strong>梯度</strong>。</p><p>假设有如下函数：</p><p><img src="http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x%2Cy%29%3D%5Cfrac%7Bx%2B%5Csigma%28y%29%7D%7B%5Csigma%28x%29%2B%28x%2By%29%5E2%7D" alt="\displaystyle f(x,y)=\frac{x+\sigma(y)}{\sigma(x)+(x+y)^2}"></p><p>（1）前向传播计算每个门的输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = 3 # 例子数值</span><br><span class="line">y = -4</span><br><span class="line"></span><br><span class="line"># 前向传播</span><br><span class="line">sigy = 1.0 / (1 + math.exp(-y)) # 分子中的sigmoi          #(1)</span><br><span class="line">num = x + sigy # 分子                                    #(2)</span><br><span class="line">sigx = 1.0 / (1 + math.exp(-x)) # 分母中的sigmoid         #(3)</span><br><span class="line">xpy = x + y                                              #(4)</span><br><span class="line">xpysqr = xpy**2                                          #(5)</span><br><span class="line">den = sigx + xpysqr # 分母                                #(6)</span><br><span class="line">invden = 1.0 / den                                       #(7)</span><br><span class="line">f = num * invden #                                  #(8)</span><br></pre></td></tr></table></figure><p>前向传播时创建了多个中间变量，每个都是比较简单的表达式，它们计算局部梯度的方法是已知的。当对前向传播时产生每个变量(<strong>sigy, num, sigx, xpy, xpysqr, den, invden</strong>)进行回传。我们会有同样数量的变量，但是都以<strong>d</strong>开头，需要存储对应变量的梯度。然后根据使用链式法则乘以上游梯度。</p><p>（2）反向传播计算每个门的梯度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 回传 f = num * invden</span><br><span class="line">dnum = invden # 分子的梯度                                         #(8)</span><br><span class="line">dinvden = num                                                     #(8)</span><br><span class="line"># 回传 invden = 1.0 / den </span><br><span class="line">dden = (-1.0 / (den**2)) * dinvden                                #(7)</span><br><span class="line"># 回传 den = sigx + xpysqr</span><br><span class="line">dsigx = (1) * dden                                                #(6)</span><br><span class="line">dxpysqr = (1) * dden                                              #(6)</span><br><span class="line"># 回传 xpysqr = xpy**2</span><br><span class="line">dxpy = (2 * xpy) * dxpysqr                                        #(5)</span><br><span class="line"># 回传 xpy = x + y</span><br><span class="line">dx = (1) * dxpy                                                   #(4)</span><br><span class="line">dy = (1) * dxpy                                                   #(4)</span><br><span class="line"># 回传 sigx = 1.0 / (1 + math.exp(-x))</span><br><span class="line">dx += ((1 - sigx) * sigx) * dsigx # Notice += !!                  #(3)</span><br><span class="line"># 回传 num = x + sigy</span><br><span class="line">dx += (1) * dnum                                                  #(2)</span><br><span class="line">dsigy = (1) * dnum                                                #(2)</span><br><span class="line"># 回传 sigy = 1.0 / (1 + math.exp(-y))</span><br><span class="line">dy += ((1 - sigy) * sigy) * dsigy                                 #(1)</span><br></pre></td></tr></table></figure><p><strong>（3）注意</strong></p><p><strong>对前向传播变量进行缓存</strong>：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。</p><p><strong>变量在不同分支的梯度要累加</strong>：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。<strong>如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加</strong>。看（3）理解。</p><p><br></p><p><br></p><h2 id="三、加法、乘法和取最大值在反向传播中关于梯度的理解"><a href="#三、加法、乘法和取最大值在反向传播中关于梯度的理解" class="headerlink" title="三、加法、乘法和取最大值在反向传播中关于梯度的理解"></a>三、加法、乘法和取最大值在反向传播中关于梯度的理解</h2><p>神经网络中最常用的加法、乘法和取最大值这三个门单元，它们在反向传播过程中的行为都有非常简单的解释。</p><p>2<em>[x\</em>y+max(w,z)]</p><p><img src="https://pic2.zhimg.com/80/39162d0c528144362cc09f1965d710d1_hd.jpg" alt="img"></p><p><strong>加法门单元</strong>：把输出的梯度相等地分发给它所有的输入。上例中，加法门把梯度2.00相等地路由给了两个输入。</p><p><strong>取最大值门单元</strong>对：将输出梯度转给前向传播中值最大的那个输入，其余的输入的梯度为0。上例中，取最大值门将梯度2.00转给了z变量，因为z的值比w高，于是w的梯度保持为0。</p><p><strong>乘法门单元</strong>：局部梯度就是交换之后的输入值，然后根据链式法则乘以输出值的梯度。上例中，x的梯度是-4.00x2.00=-8.00。</p><p>注意一种比较特殊的情况，如果乘法门单元的其中一个输入非常小，而另一个输入非常大，它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本<img src="http://www.zhihu.com/equation?tex=x_i" alt="x_i">乘以1000，那么权重的梯度将会增大1000倍，这样就必须降低学习率来弥补，所以数据预处理很重要。</p><p><br></p><p><br></p><h2 id="四、用向量化计算梯度"><a href="#四、用向量化计算梯度" class="headerlink" title="四、用向量化计算梯度"></a>四、用向量化计算梯度</h2>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;p&gt;1.反向传播&lt;/p&gt;
&lt;p&gt;反向传播是利用&lt;strong&gt;链式法则&lt;/strong&gt; ，递归计算梯度的方法&lt;/p&gt;
&lt;p&gt;2.&lt;strong&gt;分段反向传播求梯度&lt;/strong&gt;  (重点)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;不需要关于输入变量的明确的函数来计算导数&lt;/strong&gt;，因为需要求的是梯度值。只需要将表达式分成不同的可以求导的模块，然后在反向传播中一步一步地计算&lt;strong&gt;梯度&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;3.加法、乘法和取最大值在反向传播中关于梯度的理解&lt;/p&gt;
&lt;p&gt;4.用向量化计算梯度&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（一）——线性回归、逻辑回归、多分类</title>
    <link href="http://yoursite.com/2018/07/15/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E3%80%81%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E3%80%81%E5%A4%9A%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/07/15/cs231n笔记（一）——线性回归、逻辑回归、多分类/</id>
    <published>2018-07-15T13:23:33.000Z</published>
    <updated>2018-07-22T02:57:45.048Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>一、线性回归</p><p>二、逻辑回归</p><p>三、多分类</p><p>1.根据像素差异分类</p><p>2.线性分类</p><p>   （1）多类SVM分类器-折叶损失</p><p>   （2）softmax分类器-交叉熵损失</p><a id="more"></a><p><br></p><h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p><br></p><p>即对于多维空间中存在的样本点，我们用特征的线性组合去拟合空间中点的分布和轨迹，用于对连续值结果进行预测。</p><p>线性回归的代价函数</p><p><img src="https://camo.githubusercontent.com/90f8b2963cbd56650a13d0568166905d9805f480/687474703a2f2f696d616765732e636e6974626c6f672e636f6d2f626c6f672f3537353537322f3230313331312f30393038313430352d39623439326363393533376434653662623461393739616166363430653836322e706e67" alt="img"></p><p><br></p><p><br></p><h1 id="逻辑回归（二分类）"><a href="#逻辑回归（二分类）" class="headerlink" title="逻辑回归（二分类）"></a>逻辑回归（二分类）</h1><p><br></p><p>1.过程</p><ul><li>输入特征x</li><li>用激活函数计算逻辑回归的输出 hθ(x)</li><li>计算hθ(x)的代价函数Jθ(x)</li><li>用梯度下降法结合Jθ(x)调整参数θ使得Jθ(x)最小</li><li>得出最终结果θ</li></ul><p>2.激活函数</p><p>3.逻辑回归的代价函数</p><p><img src="https://camo.githubusercontent.com/974a671827e2e3d36cd26c76c05ad600ec5502bc/687474703a2f2f696d616765732e636e6974626c6f672e636f6d2f626c6f672f3537353537322f3230313331312f30393038313433372d33306165393937633465633834303161396461663237366365646537346263372e706e67" alt="img"></p><p><img src="https://camo.githubusercontent.com/7766af8da33ce8b9bf8ca7b2c6308e78e5656693/687474703a2f2f696d616765732e636e6974626c6f672e636f6d2f626c6f672f3537353537322f3230313331312f30393038313731332d30613263623061333134613234333431396135623331623366333732393133342e706e67" alt="img"></p><p>不用线性回归的代价函数，是因为会有“非凸”函数的问题，就是这个函数有很多个局部最低点。</p><p><br></p><p><br></p><h1 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h1><p><br></p><h2 id="（一）通过像素差异分类"><a href="#（一）通过像素差异分类" class="headerlink" title="（一）通过像素差异分类"></a>（一）通过像素差异分类</h2><p>1.NN 最近邻分类器</p><p><strong>（1）Nearest Neighbor ：</strong>拿着测试图片和训练集中每一张图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。</p><p><strong>（2）k-Nearest Neighbor分类器</strong>：找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。</p><p>2.像素差异分类的缺点：图像更多的是按照背景和颜色被分类，而不是语义主体本身。</p><p><br></p><h2 id="（二）线性分类"><a href="#（二）线性分类" class="headerlink" title="（二）线性分类"></a>（二）线性分类</h2><p><br></p><h3 id="一、分类三个步骤"><a href="#一、分类三个步骤" class="headerlink" title="一、分类三个步骤"></a>一、分类三个步骤</h3><p>由三部分组成：</p><p><strong>1. 评分函数（score function）</strong> ：将原始图像数据映射为各个类别的得分，得分高低代表图像属于该类别的可能性高低。</p><p>线性分类评分函数为线性映射：<img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%2Cb%29%3DWx_i%2Bb" alt="\displaystyle f(x_i,W,b)=Wx_i+b"></p><p><strong>2. 损失函数（loss function）</strong> ：量化分类标签的得分与真实标签之间一致性。也就是<strong>量化某个具体权重集W的质量</strong> 。</p><p><strong>3.最优化（Optimization）</strong> ：找到能够最小化损失函数值的W。</p><p><br></p><h3 id="二、损失函数正则化"><a href="#二、损失函数正则化" class="headerlink" title="二、损失函数正则化"></a>二、损失函数正则化</h3><p>1.为什么正则化</p><p>向损失函数增加一个正则化惩罚（regularization penalty）<img src="https://www.zhihu.com/equation?tex=R%28W%29" alt="R(W)">部分。可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响，并避免过拟合。</p><p>注意：通常只对权重<img src="https://www.zhihu.com/equation?tex=W" alt="W">正则化，而不正则化偏差<img src="https://www.zhihu.com/equation?tex=b" alt="b">。</p><p>2.完整的损失函数</p><p>数据损失（data loss），即所有样例的的平均损失<img src="https://www.zhihu.com/equation?tex=L_i" alt="L_i">，和正则化损失（regularization loss）组成</p><p><img src="https://www.zhihu.com/equation?tex=L%3D%5Cdisplaystyle+%5Cunderbrace%7B+%5Cfrac%7B1%7D%7BN%7D%5Csum_i+L_i%7D_%7Bdata+%5C++loss%7D%2B%5Cunderbrace%7B%5Clambda+R%28W%29%7D_%7Bregularization+%5C+loss%7D" alt="L=\displaystyle \underbrace{ \frac{1}{N}\sum_i L_i}_{data \  loss}+\underbrace{\lambda R(W)}_{regularization \ loss}"></p><p><br></p><h3 id="三、损失函数分类"><a href="#三、损失函数分类" class="headerlink" title="三、损失函数分类"></a>三、损失函数分类</h3><p>1.多类SVM</p><p>（1）多类SVM</p><p><strong>希望正确分类类别<img src="https://www.zhihu.com/equation?tex=y_i" alt="y_i">的分数比不正确类别分数高，而且至少要高<img src="https://www.zhihu.com/equation?tex=%5CDelta" alt="\Delta">，如果不满足这点，就开始计算损失值。</strong></p><p>（2）损失函数</p><p><strong>折叶损失（hinge loss）</strong>：<img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cs_j-s_%7By_i%7D%2B%5CDelta%29" alt="\displaystyle L_i=\sum_{j\not=y_i}max(0,s_j-s_{y_i}+\Delta)">  </p><p><br></p><p>2.Softmax分类器</p><p>（1）Softmax分类器</p><p>是逻辑回归分类器即二分类，面对多个分类的一般化归纳。</p><p>（2）损失函数</p><p><strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong> ：<img src="http://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29" alt="\displaystyle Li=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})"></p><p>其中<img src="http://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D" alt="f_j(z)=\frac{e^{z_j}}{\sum_ke^{z_k}}">是softmax 函数</p><p>softmax 函数：<strong>将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;p&gt;一、线性回归&lt;/p&gt;
&lt;p&gt;二、逻辑回归&lt;/p&gt;
&lt;p&gt;三、多分类&lt;/p&gt;
&lt;p&gt;1.根据像素差异分类&lt;/p&gt;
&lt;p&gt;2.线性分类&lt;/p&gt;
&lt;p&gt;   （1）多类SVM分类器-折叶损失&lt;/p&gt;
&lt;p&gt;   （2）softmax分类器-交叉熵损失&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>显卡</title>
    <link href="http://yoursite.com/2018/07/15/%E6%98%BE%E5%8D%A1/"/>
    <id>http://yoursite.com/2018/07/15/显卡/</id>
    <published>2018-07-15T03:28:11.000Z</published>
    <updated>2018-07-22T03:20:49.147Z</updated>
    
    <content type="html"><![CDATA[<p>PS ：最近想入一个显示屏，结果发现自己连显卡是什么都搞不清楚，我真是菜的不行</p><p><br></p><h2 id="显卡"><a href="#显卡" class="headerlink" title="显卡"></a>显卡</h2><p>显卡是电脑进行数模信号转换的设备，承担输出显示图形的任务。</p><p>1.显存：又叫帧缓存，作用是用来<strong>存储</strong>GPU处理过或者即将提取的渲染数据</p><p>2.GPU（Graphic Processing Unit）：是显卡的核心引擎，专为图形渲染设计的，并行处理方面，浮点运算方面功能强大。</p><p>3.理解显存和GPU：GPU相当于电脑的CPU，显存相当于电脑的内存</p><a id="more"></a><p><br></p><h2 id="显卡分类"><a href="#显卡分类" class="headerlink" title="显卡分类"></a>显卡分类</h2><p>1.集显：集成显卡，显示核心集成在主板上</p><p>2.核显：就是集成在CPU里，也算是集显的一种</p><p>3.独显：相对集显而已，最明显的一个特性，就是<strong>自带显存</strong>，<strong>集显是共享内存的</strong>，性能上比集显要好很多</p><p>4.双显：一般是指笔记本同时配置有独显和集显（核显），可以在这两个显卡之间来回切换使用</p><p><br></p><h2 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h2><p>CUDA是NVIDIA推出的用于自家GPU的<strong>并行计算</strong>框架，并且CUDA只能在NVIDIA的GPU上运行。</p><p>CUDA 架构下，一个程序分为两个部份：Host 端和 device 端。Host 端是指在 CPU 上执行的部份，device 端则是在显示芯片上执行的部份。Device 端的程序又称为 “kernel”。通常 Host 端程序会将数据准备好后，复制到显卡的内存中，再由GPU执行 Device 端程序，完成后再由 host 端程序将结果从显卡的内存中取回。</p><p><br></p><p><strong>cuDNN</strong>（CUDA Deep Neural Network library）：是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。要用GPU训练模型，cuDNN不是必须的，但是一般会采用这个加速库。</p><p><br></p><h2 id="图形渲染"><a href="#图形渲染" class="headerlink" title="图形渲染"></a>图形渲染</h2><p>我的理解就是将图形尽量接近真实场景，逼真地呈现</p><p><a href="http://imgtec.eetrend.com/blog/9825" target="_blank" rel="noopener">http://imgtec.eetrend.com/blog/9825</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS ：最近想入一个显示屏，结果发现自己连显卡是什么都搞不清楚，我真是菜的不行&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;显卡&quot;&gt;&lt;a href=&quot;#显卡&quot; class=&quot;headerlink&quot; title=&quot;显卡&quot;&gt;&lt;/a&gt;显卡&lt;/h2&gt;&lt;p&gt;显卡是电脑进行数模信号转换的设备，承担输出显示图形的任务。&lt;/p&gt;
&lt;p&gt;1.显存：又叫帧缓存，作用是用来&lt;strong&gt;存储&lt;/strong&gt;GPU处理过或者即将提取的渲染数据&lt;/p&gt;
&lt;p&gt;2.GPU（Graphic Processing Unit）：是显卡的核心引擎，专为图形渲染设计的，并行处理方面，浮点运算方面功能强大。&lt;/p&gt;
&lt;p&gt;3.理解显存和GPU：GPU相当于电脑的CPU，显存相当于电脑的内存&lt;/p&gt;
    
    </summary>
    
    
      <category term="GPU" scheme="http://yoursite.com/tags/GPU/"/>
    
      <category term="硬件" scheme="http://yoursite.com/tags/%E7%A1%AC%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>程序是怎样跑起来的</title>
    <link href="http://yoursite.com/2018/07/14/%E7%A8%8B%E5%BA%8F%E6%98%AF%E6%80%8E%E6%A0%B7%E8%B7%91%E8%B5%B7%E6%9D%A5%E7%9A%84/"/>
    <id>http://yoursite.com/2018/07/14/程序是怎样跑起来的/</id>
    <published>2018-07-14T08:45:16.000Z</published>
    <updated>2018-07-22T03:20:16.711Z</updated>
    
    <content type="html"><![CDATA[<h1 id="程序是怎样跑起来的"><a href="#程序是怎样跑起来的" class="headerlink" title="程序是怎样跑起来的"></a>程序是怎样跑起来的</h1><p>PS：超级感谢师兄把这本书借给我，因为我的硬件基础实在是太差，这本书让我对硬件、操作系统有了个基本的概念。</p><p>这篇博就是记录读书笔记，有些章节我比较熟悉，就跳过了</p><a id="more"></a><p><br></p><h2 id="一、CPU"><a href="#一、CPU" class="headerlink" title="一、CPU"></a>一、CPU</h2><h3 id="1-CPU内部结构"><a href="#1-CPU内部结构" class="headerlink" title="1.CPU内部结构"></a>1.CPU内部结构</h3><ol><li>寄存器：暂存指令、数据</li><li>控制器：将内存中的指令、数据读入寄存器，根据运算结果控制计算机（如显示器的输入输出）</li><li>运算器：计算寄存器里的数据</li><li>时钟：发出CPU开始计时的时钟信号，CPU运行速度</li></ol><p>对于程序员而言，CPU的关注点在各种功能的<strong>寄存器</strong></p><h3 id="2-程序计数器"><a href="#2-程序计数器" class="headerlink" title="2.程序计数器"></a>2.程序计数器</h3><p>程序计数器是CPU寄存器的一种，CPU每执行一条指令，程序计数器的值加1，若执行指令占据多个内存地址，增加相应的数值，即程序计数器存储的是指令地址。</p><h3 id="3-条件分支和循环机制"><a href="#3-条件分支和循环机制" class="headerlink" title="3.条件分支和循环机制"></a>3.条件分支和循环机制</h3><p>程序流程：顺序执行、条件分支、循环</p><h3 id="4-函数调用机制"><a href="#4-函数调用机制" class="headerlink" title="4.函数调用机制"></a>4.函数调用机制</h3><p>（1）CALL指令：将调用函数后面的指令地址放到内存的栈里</p><p>（2）程序计数器被设定为要调用的函数的入口地址</p><p>（3）RETURN指令：调用函数执行完后，把栈里的地址取出，将程序计数器设定为该地址</p><h3 id="5-内存地址的查看"><a href="#5-内存地址的查看" class="headerlink" title="5.内存地址的查看"></a>5.内存地址的查看</h3><p>内存地址通过基址寄存器+变址寄存器来查看更方便，相当于<strong>二维数组的索引</strong>。</p><h3 id="6-CPU能直接识别和执行的只有机器语言"><a href="#6-CPU能直接识别和执行的只有机器语言" class="headerlink" title="6. CPU能直接识别和执行的只有机器语言"></a>6. CPU能直接识别和执行的只有机器语言</h3><p><br></p><p><br></p><h2 id="二、计算机信息用二进制数表示"><a href="#二、计算机信息用二进制数表示" class="headerlink" title="二、计算机信息用二进制数表示"></a>二、计算机信息用二进制数表示</h2><p>1.内存和磁盘都是用字节单位来存储和读写数据，所以字节是信息的基本单位，位是最小单位</p><p>2.计算机中的小数计算错误：是因为有些十进制的小数无法转换为二进制数</p><p>举个例子：十进制0.1转为二进制会变成0.00011001100……(1100循环) 。实际上，计算机不会这样处理小数。</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/error.JPG" alt=""></p><p>3.计算机中小数的表示：定点数和浮点数</p><ul><li>定点数：小数点需要固定到某个位置，可以自己在程序中指定，前面的是整数，后面的是小数。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-4d04c3d95b762e3b490a754131f35ea3_hd.jpg" alt="img"></p><ul><li>浮点数：由符号、尾数、基数、指数四部分组成</li></ul><p>双精度浮点数64位(double)，单精度浮点数32位(float)</p><p><img src="https://pic4.zhimg.com/80/v2-94bf60eecf4bcbb038ede2c8f18bfa93_hd.jpg" alt="img"></p><p><img src="https://pic1.zhimg.com/80/v2-b5d44cc4d1f293d07826fc052b20213c_hd.jpg" alt="img"></p><p>最高位是符号，接着是指数E，剩下的是尾数M，其中基数为2</p><p>4.正则表达式</p><p>尾数：用的是<strong>将小数点前面的值固定为1的正则表达式</strong></p><p>因为浮点数可以用不同的数值表示同一数值，所以需要制定统一的规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1011.0011</span><br><span class="line">0001.01100111 右移三位</span><br><span class="line">0110011000...00(确保长度为23，单精度浮点数的尾部部分是23)</span><br><span class="line">仅仅保存小数点后面部分，完成正则表达式（所以我总算明白了为什么这个叫尾数）</span><br></pre></td></tr></table></figure><p>5.EXCESS系统</p><p>指数：用的是EXCESS系统表，为的是<strong>表示负数时不使用符号位</strong>（通过将指数部分表示范围的中间值设为0）</p><p>举个例子：ABCDEFG　D表示0，那么E表示1，C表示-1</p><p><br></p><p><br></p><h2 id="三、内存"><a href="#三、内存" class="headerlink" title="三、内存"></a>三、内存</h2><h3 id="1-内存的物理机制"><a href="#1-内存的物理机制" class="headerlink" title="1.内存的物理机制"></a>1.内存的物理机制</h3><p>内存IC中有电源、地址信号、数据信号、控制信号 (读写操作)。</p><p> 内存，包括随机存储器（RAM），只读存储器（<strong>ROM</strong>），以及高速缓存（CACHE）。 只不过因为RAM是其中最重要的存储器。 通常所说的内即指电脑系统中的RAM。</p><p><br></p><h3 id="2-数据在内存中的存储"><a href="#2-数据在内存中的存储" class="headerlink" title="2.数据在内存中的存储"></a>2.数据在内存中的存储</h3><p>数据类型在内存中看就是，占据内存的大小(当然符号位先不管)</p><p>字节序</p><ul><li>Big endian ：按照从低地址到高地址的顺序存放数据的高位字节到低位字节</li><li>Little endian：按照从低地址到高地址的顺序存放据的低位字节到高位字节</li></ul><p><br></p><h3 id="3-指针"><a href="#3-指针" class="headerlink" title="3.指针"></a>3.指针</h3><p>存储数据的内存地址</p><p><code>char *d</code>  char 表示一次能从地址中读取一个字节的数据</p><p> <code>long *l</code> long 表示一次能从地址中读取四个字节的数据</p><p><br></p><h3 id="4-数组"><a href="#4-数组" class="headerlink" title="4.数组"></a>4.数组</h3><p>数组和内存的物理构造是一样的，数组使编程工作更高效</p><p><br></p><h3 id="5-栈、队列、环形缓冲区（不需要指定地址和索引）"><a href="#5-栈、队列、环形缓冲区（不需要指定地址和索引）" class="headerlink" title="5.栈、队列、环形缓冲区（不需要指定地址和索引）"></a>5.栈、队列、环形缓冲区（不需要指定地址和索引）</h3><p>栈：数据只能在栈顶插入(push)或删除(pop)  ，对内存中的数据读写 LIFO (last in first out)</p><p>队列：队尾插入数据，队头删除数据 ，对内存中的数据读写 FIFO (first in first out)</p><p>队列一般是以环状缓冲区的形式实现，使数据的写入和读出循环起来</p><p><br></p><h3 id="6-链表"><a href="#6-链表" class="headerlink" title="6.链表"></a>6.链表</h3><p>使元素插入、删除更容易，不像数组那样需要一部分数据整体都移动</p><p><br></p><h3 id="7-二叉查找树（数据搜索）"><a href="#7-二叉查找树（数据搜索）" class="headerlink" title="7.二叉查找树（数据搜索）"></a>7.二叉查找树（数据搜索）</h3><p>在链表的基础上插入、删除元素，考虑数据大小，将其分成左右两个方向</p><p><br></p><p><br></p><h2 id="四、磁盘"><a href="#四、磁盘" class="headerlink" title="四、磁盘"></a>四、磁盘</h2><h3 id="1-内存和磁盘"><a href="#1-内存和磁盘" class="headerlink" title="1.内存和磁盘"></a>1.内存和磁盘</h3><p>都可以用来存储程序命令和数据。</p><ul><li>内存：用电流来实现存储，高速小容量</li><li>磁盘：用磁效应来实现存储，低速高容量</li></ul><p>内存主要是指主内存 (存储CPU中运行的程序指令和数据)，磁盘主要是指硬盘</p><p><br></p><h3 id="2-存储程序方式-程序内置方式"><a href="#2-存储程序方式-程序内置方式" class="headerlink" title="2.存储程序方式 (程序内置方式)"></a>2.存储程序方式 (程序内置方式)</h3><p><strong>磁盘中存储的程序，必须要加载到内存才能运行</strong>，因为CPU运行和解析程序的时候要通过<strong>程序计数器</strong>指定内存地址来读取程序</p><p><br></p><h3 id="3-磁盘缓存-假想的磁盘"><a href="#3-磁盘缓存-假想的磁盘" class="headerlink" title="3.磁盘缓存 (假想的磁盘)"></a>3.磁盘缓存 (假想的磁盘)</h3><p>磁盘缓存：从磁盘中读取数据到内存中存储的方式，从而加快访问速度。</p><p>这种将低速设备中的数据保存到高速设备中的<strong>缓存</strong>方式，其它情况也会用到。Web浏览器通过远程Web服务器获取数据，在显示较大的图片或视频时，其实先将文件缓存在硬盘里，等到需要时从硬盘读取。(应该就是视频已经缓存到硬盘里，等你点开播放键，再从硬盘读取，这样读取速度会快)</p><p><br></p><h3 id="4-虚拟内存-假想的内存"><a href="#4-虚拟内存-假想的内存" class="headerlink" title="4.虚拟内存(假想的内存)"></a>4.虚拟内存(假想的内存)</h3><p>将磁盘的一部分作为内存使用，在<strong>内存不足时也能运行程序</strong>，其实是<strong>将实际内存与磁盘虚拟内存中的内容置换</strong>得以运行程序。</p><p>虚拟内存有分段式和分页式。<strong>分页式</strong>是指不考虑程序构造，将程序按照页(page)的大小进行分割，然后按照页大小在磁盘的虚拟内存和内存之间进行置换。</p><p>page file 1920MB</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/virtual%20memory.JPG" alt=""></p><p><br></p><h3 id="5-节约内存的编程方法"><a href="#5-节约内存的编程方法" class="headerlink" title="5.节约内存的编程方法"></a>5.节约内存的编程方法</h3><p>(1) DLL(Dynamic Link Library) 多个应用可以共用同一个DLL文件，从而节约了内存</p><p>(2) C语言编程可以通过调用_stdcall</p><p>C语言在调用函数后需要进行栈清理处理 </p><ul><li>在调用方处理：多次进行相同的栈清理处理</li><li>在被调用方处理：只进行一次栈清理</li></ul><p><br></p><h3 id="6-磁盘划分"><a href="#6-磁盘划分" class="headerlink" title="6. 磁盘划分"></a>6. 磁盘划分</h3><p>磁盘的划分有扇区和可变长</p><ul><li>扇区是硬盘上最小的读写单位</li><li>簇是<strong>文件系统</strong>的最小读写单位，是扇区的倍数，簇可以保证里面的扇区是连续的</li></ul><p><strong>文件系统</strong>：是操作系统在存储设备上组织文件的方法。Windows 98 以前所使用的是文件系统是 FAT，Windows 2000 以后的版本有所谓的 NTFS 。</p><p><br></p><h3 id="7-驱动"><a href="#7-驱动" class="headerlink" title="7.驱动"></a>7.驱动</h3><p>驱动程序就是运行在操作系统和硬件之间，用来协助<strong>操作系统控制硬件的程序</strong></p><p>操作系统只与输入输出设备的驱动打交道，比如显卡、声卡，与硬件直接打交道的是驱动程序</p><p><br></p><p><br></p><h2 id="五、程序的运行环境"><a href="#五、程序的运行环境" class="headerlink" title="五、程序的运行环境"></a>五、程序的运行环境</h2><h3 id="1-操作系统和硬件共同决定了程序的运行环境"><a href="#1-操作系统和硬件共同决定了程序的运行环境" class="headerlink" title="1. 操作系统和硬件共同决定了程序的运行环境"></a>1. 操作系统和硬件共同决定了程序的运行环境</h3><p><br></p><h3 id="2-机器语言"><a href="#2-机器语言" class="headerlink" title="2.机器语言"></a>2.机器语言</h3><p>机器语言是用<strong>二进制代码</strong>表示的计算机能直接识别和执行的一种<strong>机器指令的集合</strong>。这种指令集称为机器码或原生码。<strong>不同种类的计算机其机器语言是不相通的</strong>，按某种计算机的机器指令编制的程序不能在另一种计算机上执行。</p><p>对于<strong>源代码</strong>进行编译后得到<strong>本地代码</strong>，即机器语言的程序。</p><p><br></p><h3 id="3-不同操作系统API不同"><a href="#3-不同操作系统API不同" class="headerlink" title="3. 不同操作系统API不同"></a>3. 不同操作系统API不同</h3><p>应用软件需要根据不同的操作系统来开发，<strong>应用程序向操作系统传递指令的途径叫API</strong>，像鼠标输入、显示器输出等</p><p><br></p><h3 id="4-利用虚拟机软件运行其它操作系统下才能运行的应用"><a href="#4-利用虚拟机软件运行其它操作系统下才能运行的应用" class="headerlink" title="4. 利用虚拟机软件运行其它操作系统下才能运行的应用"></a>4. 利用虚拟机软件运行其它操作系统下才能运行的应用</h3><p><br></p><h3 id="5-Java虚拟机"><a href="#5-Java虚拟机" class="headerlink" title="5. Java虚拟机"></a>5. Java虚拟机</h3><p>Java编译后生成的不是特定CPU使用的本地代码，而是<strong>字节代码</strong>，Java虚拟机为字节代码的运行提供了运行环境，即<strong>一边编译为本地代码一边运行</strong>。</p><p>Java虚拟机看作是不依赖于特定硬件及操作系统的运行环境</p><p><br></p><h3 id="6-BIOS"><a href="#6-BIOS" class="headerlink" title="6. BIOS"></a>6. BIOS</h3><p><strong>BIOS放在内存中的ROM里</strong> ，掉电不丢失</p><p>BIOS (Basic Input Output System ) 记录了控制外围设备的程序和数据。开机后，BIOS首先确认硬件是否正常运行，没有问题后，会启动<strong>硬盘里的引导程序</strong>。</p><p><br></p><p><br></p><h2 id="六、源文件到可执行文件"><a href="#六、源文件到可执行文件" class="headerlink" title="六、源文件到可执行文件"></a>六、源文件到可执行文件</h2><h3 id="1-计算机只能运行本地代码"><a href="#1-计算机只能运行本地代码" class="headerlink" title="1.计算机只能运行本地代码"></a>1.计算机只能运行本地代码</h3><p><br></p><h3 id="2-编译器"><a href="#2-编译器" class="headerlink" title="2.编译器"></a>2.编译器</h3><p>编译器与编程语言还与CPU的种类有关，将源代码转换为本地代码 </p><p>所以对于源文件编译生成的目标文件 .obj ，其内容是本地代码</p><p><strong>交叉编译器</strong>：在一种计算机环境中运行的编译程序，能编译出<strong>另外一种环境下运行的代码</strong> </p><p><br></p><h3 id="3-链接器"><a href="#3-链接器" class="headerlink" title="3.链接器"></a>3.链接器</h3><p>编译后生成的目标文件无法直接运行，需要用链接器将多个目标文件结合，才能生成课执行文件.exe</p><p><br></p><h3 id="4-库文件"><a href="#4-库文件" class="headerlink" title="4.库文件"></a>4.库文件</h3><p>库文件：将多个目标文件集中保存到一个文件中的形式，链接时将指定的目标文件抽取出。静态数据库(.lib文件)和动态数据库(.dll文件)。</p><p><strong>静态链接</strong>：lib 即包括函数位置信息的索引，也包括实现，在编译时直接将代码加入程序当中</p><p><strong>动态链接</strong>：需要用到两个文件，一个是 lib文件，包含 dll 中的函数名称和位置。另一个是dll文件，包含实际的函数和数据，应用程序通过 lib 文件链接到 dll 文件</p><p><br></p><h3 id="5-可执行文件的运行"><a href="#5-可执行文件的运行" class="headerlink" title="5.可执行文件的运行"></a>5.可执行文件的运行</h3><p>1）可执行文件是放在硬盘中的，需要将程序加载进内存后运行</p><p>2）本地代码处理变量或函数时，跳转到的是变量或函数在内存中的地址，但是可执行文件中并没有指定变量或函数的实际内存地址。可执行文件中变量或函数的实际地址如何表示？</p><p>可执行文件中给变量或函数分配的是<strong>虚拟内存的地址</strong>，程序运行时，会将虚拟内存的地址转为实际内存的地址，这需要<strong>再配置信息</strong>（为这种地址转换提供的信息）</p><p><br></p><h3 id="6-程序加载生成堆和栈"><a href="#6-程序加载生成堆和栈" class="headerlink" title="6.程序加载生成堆和栈"></a>6.程序加载生成堆和栈</h3><p>可执行文件中包括再配置信息、函数、数据，<strong>不存在堆和栈</strong></p><p>堆和栈需要的内存空间是在可执行文件运行时分配的</p><p><br></p><h3 id="7-栈"><a href="#7-栈" class="headerlink" title="7.栈"></a>7.栈</h3><p>存储函数内部的局部变量和方法调用所用参数的内存区域</p><p><strong>自动申请和分配</strong></p><p><br></p><h3 id="8-堆"><a href="#8-堆" class="headerlink" title="8.堆"></a>8.堆</h3><p>通过malloc和new等<strong>动态申请</strong>内存的语句使用，也需要用户<strong>手动释放</strong></p><p><br></p><p><br></p><h2 id="七、操作系统和应用"><a href="#七、操作系统和应用" class="headerlink" title="七、操作系统和应用"></a>七、操作系统和应用</h2><h3 id="1-监控程序"><a href="#1-监控程序" class="headerlink" title="1.监控程序"></a>1.监控程序</h3><p>监控程序：操作系统的原型，具有加载和运行的功能，即将各种应用程序加载到内存中运行</p><p>初期的操作系统：任何程序的输入输出部分都是一样的，所以将这一部分加入到监控程序中形成初期操作系统</p><p><br></p><h3 id="2-系统调用"><a href="#2-系统调用" class="headerlink" title="2.系统调用"></a>2.系统调用</h3><p>系统调用：操作系统的硬件控制功能</p><p>高级语言的<strong>可移植性</strong>：高级语言程序编译后生成相应操作系统的系统调用，即生成利用系统调用的本地代码</p><p><br></p><h3 id="3-Windows操作系统的特征"><a href="#3-Windows操作系统的特征" class="headerlink" title="3.Windows操作系统的特征"></a>3.Windows操作系统的特征</h3><h4 id="（1）API"><a href="#（1）API" class="headerlink" title="（1）API"></a>（1）API</h4><p>API：应用程序接口，将应用程序和操作系统相连接，由多个DLL文件提供</p><h4 id="（2）GUI-图形用户界面"><a href="#（2）GUI-图形用户界面" class="headerlink" title="（2）GUI 图形用户界面"></a>（2）GUI 图形用户界面</h4><h4 id="（3）-WYSIWYG"><a href="#（3）-WYSIWYG" class="headerlink" title="（3） WYSIWYG"></a>（3） WYSIWYG</h4><p>what you see is what you get 所见即所得 ：显示器和打印机作为同等的输出设备处理</p><h4 id="（4）多任务"><a href="#（4）多任务" class="headerlink" title="（4）多任务"></a>（4）多任务</h4><p>通过时钟切割，使得多个程序同时运行，即在短时间内让多个程序切换运行，看起来就像多个程序同时运行</p><h4 id="（5）网络功能和数据库功能"><a href="#（5）网络功能和数据库功能" class="headerlink" title="（5）网络功能和数据库功能"></a>（5）网络功能和数据库功能</h4><p>叫作中间件，操作系统和中间件合起来叫系统软件，应用可以利用操作系统，也可以利用中间件的功能</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E4%B8%AD%E9%97%B4%E4%BB%B6.png" alt=""></p><h4 id="6-即插即用-PNP（plug-and-play）"><a href="#6-即插即用-PNP（plug-and-play）" class="headerlink" title="(6)即插即用 PNP（plug and play）"></a>(6)即插即用 PNP（plug and play）</h4><p>新的设备连接到计算机，系统会自动安装和设定设备的驱动程序</p><p>基本的驱动装系统的时候都是自动安装的，但是大部分的硬件驱动是要自己安装的，比如升级的显卡、声卡</p><p><br></p><p><br></p><h2 id="八、汇编语言"><a href="#八、汇编语言" class="headerlink" title="八、汇编语言"></a>八、汇编语言</h2><p>汇编语言使用助记符 </p><p>汇编语言编译为本地代码的过程叫汇编，反之，叫反汇编</p><p>汇编语言与本地代码一一对应，高级语言反编译是无法做到完全还原</p><p><br></p><p><br></p><h2 id="九、硬件"><a href="#九、硬件" class="headerlink" title="九、硬件"></a>九、硬件</h2><h3 id="1-系统调用"><a href="#1-系统调用" class="headerlink" title="1.系统调用"></a>1.系统调用</h3><p>系统调用实现对硬件的控制，而API则是调用的函数，函数实体存储在DLL文件中</p><p><br></p><h3 id="2-IN、OUT"><a href="#2-IN、OUT" class="headerlink" title="2.IN、OUT"></a>2.IN、OUT</h3><p>向外围设备进行输入输出操作的指令</p><p>IN：将指定端口号的端口数据输入到CPU寄存器</p><p>OUT：将CPU寄存器中的数据输出到指定端口号的端口</p><p><br></p><h3 id="3-I-O-控制器"><a href="#3-I-O-控制器" class="headerlink" title="3.I/O 控制器"></a>3.I/O 控制器</h3><p>主机附带了连接外围设备的连接器，连接器内部有I/O控制器，I/O控制器是用来交换主机与外围设备电流特性的IC，因为电压不同、数字信号与模拟信号的电流特性不同，主机与外围设备无法直接连接</p><p><strong>I/O控制器可以控制多个外围设备</strong></p><p><br></p><h3 id="4-端口-port"><a href="#4-端口-port" class="headerlink" title="4.端口 port"></a>4.端口 port</h3><p>端口是I/O控制器中用于临时保存数据的内存，也叫寄存器（CPU内部的寄存器主要用于数据运算，I/O寄存器主要用于临时存储数据）</p><p>端口号又叫I/O地址，用于区分端口</p><p><strong>IN OUT指令在指定了端口号的端口和CPU之间进行数据的输入输出，和通过内存地址的内存与CPU进行数据交换其实是一样的</strong> </p><p><br></p><h3 id="5-中断请求-IRQ（Interrupt-Request）"><a href="#5-中断请求-IRQ（Interrupt-Request）" class="headerlink" title="5.中断请求 IRQ（Interrupt Request）"></a>5.中断请求 IRQ（Interrupt Request）</h3><p>外围设备向CPU请求中断，使用<strong>中断编号</strong>对不同设备区分</p><p>操作系统和BIOS提供响应中断编号的<strong>中断处理程序</strong></p><p><strong>中断控制器</strong>将多个设备发出的中断请求有序的发给CPU</p><p>利用中断可以实现对数据的实时处理</p><p><br></p><h3 id="6-直接内存存取-DMA（Direct-Memory-Access）"><a href="#6-直接内存存取-DMA（Direct-Memory-Access）" class="headerlink" title="6.直接内存存取 DMA（Direct Memory Access）"></a>6.直接内存存取 DMA（Direct Memory Access）</h3><p>外围设备直接与主内存进行数据传输，<strong>不需要通过CPU</strong>，节约了时间</p><p>PS：I/O端口号、中断号、DMA通道用于识别外围设备</p><p><br></p><h3 id="7-显示机制"><a href="#7-显示机制" class="headerlink" title="7.显示机制"></a>7.显示机制</h3><p>与主内存相独立的VRAM中存储显示器显示的信息，通过中断实现数据的显示</p><p><br></p><p><br></p><h2 id="十、随机数"><a href="#十、随机数" class="headerlink" title="十、随机数"></a>十、随机数</h2><p>1.随机数：用程序来<strong>表示人类的直觉</strong>的一种方法</p><p>2.<strong>伪随机数</strong>：借助公式产生具有一定规律性的随机数</p><p>3.<strong>随机数的种子</strong></p><p>通过一个例子理解：有一种获取伪随机数的方法叫线性同余 R`=(a*R+b) mod c </p><p>R是当前随机数 R`是下一个出现的随机数</p><p>R、a、b、c的数值与当前系统时间有关，并且都有默认值，但是<strong>不调用获取当前系统时间的函数</strong>，R、a、b、c的数值使用的就是默认值，这里R、a、b、c就是随机数的种子</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;程序是怎样跑起来的&quot;&gt;&lt;a href=&quot;#程序是怎样跑起来的&quot; class=&quot;headerlink&quot; title=&quot;程序是怎样跑起来的&quot;&gt;&lt;/a&gt;程序是怎样跑起来的&lt;/h1&gt;&lt;p&gt;PS：超级感谢师兄把这本书借给我，因为我的硬件基础实在是太差，这本书让我对硬件、操作系统有了个基本的概念。&lt;/p&gt;
&lt;p&gt;这篇博就是记录读书笔记，有些章节我比较熟悉，就跳过了&lt;/p&gt;
    
    </summary>
    
    
      <category term="硬件" scheme="http://yoursite.com/tags/%E7%A1%AC%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>解决pip安装慢的问题</title>
    <link href="http://yoursite.com/2018/06/28/%E8%A7%A3%E5%86%B3pip%E5%AE%89%E8%A3%85%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/06/28/解决pip安装慢的问题/</id>
    <published>2018-06-28T11:33:08.000Z</published>
    <updated>2018-06-28T11:34:44.078Z</updated>
    
    <content type="html"><![CDATA[<p>完全转载该博主的原文： <a href="https://blog.csdn.net/furzoom/article/details/53897318" target="_blank" rel="noopener">https://blog.csdn.net/furzoom/article/details/53897318</a></p><p>由于国外官方pypi经常被墙，导致不可用，所以需要将pip源更换一下，这样就能解决被墙导致的装不上库的烦恼。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;完全转载该博主的原文： &lt;a href=&quot;https://blog.csdn.net/furzoom/article/details/53897318&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/furzoom
      
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业（七）</title>
    <link href="http://yoursite.com/2018/06/28/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%B8%83%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/28/Deep-Learning-吴恩达-作业（七）/</id>
    <published>2018-06-28T02:17:34.000Z</published>
    <updated>2018-06-28T02:22:23.093Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、对于训练集的处理"><a href="#一、对于训练集的处理" class="headerlink" title="一、对于训练集的处理"></a>一、对于训练集的处理</h2><h3 id="1-Mini-Batch"><a href="#1-Mini-Batch" class="headerlink" title="1.Mini-Batch"></a>1.Mini-Batch</h3><p>每次处理训练集的一部分进行梯度下降，避免了遍历完整个训练集才完成一次参数更新</p><h3 id="2-Batch-Gradient-Descent"><a href="#2-Batch-Gradient-Descent" class="headerlink" title="2.Batch Gradient Descent"></a>2.Batch Gradient Descent</h3><p>每次处理完所有训练集进行梯度下降</p><h3 id="3-Stochastic-Gradient-Descent"><a href="#3-Stochastic-Gradient-Descent" class="headerlink" title="3.Stochastic Gradient Descent"></a>3.Stochastic Gradient Descent</h3><p>每次处理完一个样本就进行梯度下降</p><h2 id="二、反向传播加速梯度下降的过程"><a href="#二、反向传播加速梯度下降的过程" class="headerlink" title="二、反向传播加速梯度下降的过程"></a>二、反向传播加速梯度下降的过程</h2><h3 id="1-Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）"><a href="#1-Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）" class="headerlink" title="1.Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）"></a>1.Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）</h3><h3 id="2-RMS-Prop：使波动大的维度参数更新慢，波动小的维度参数更新快"><a href="#2-RMS-Prop：使波动大的维度参数更新慢，波动小的维度参数更新快" class="headerlink" title="2.RMS Prop：使波动大的维度参数更新慢，波动小的维度参数更新快"></a>2.RMS Prop：使波动大的维度参数更新慢，波动小的维度参数更新快</h3><h3 id="3-Adam：将Momentum-和-RMS-Prop相结合"><a href="#3-Adam：将Momentum-和-RMS-Prop相结合" class="headerlink" title="3.Adam：将Momentum 和 RMS Prop相结合"></a>3.Adam：将Momentum 和 RMS Prop相结合</h3><a id="more"></a><p>下面是我的作业喽 ~</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E4%BD%9C%E4%B8%9A%E4%B8%83.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、对于训练集的处理&quot;&gt;&lt;a href=&quot;#一、对于训练集的处理&quot; class=&quot;headerlink&quot; title=&quot;一、对于训练集的处理&quot;&gt;&lt;/a&gt;一、对于训练集的处理&lt;/h2&gt;&lt;h3 id=&quot;1-Mini-Batch&quot;&gt;&lt;a href=&quot;#1-Mini-Batch&quot; class=&quot;headerlink&quot; title=&quot;1.Mini-Batch&quot;&gt;&lt;/a&gt;1.Mini-Batch&lt;/h3&gt;&lt;p&gt;每次处理训练集的一部分进行梯度下降，避免了遍历完整个训练集才完成一次参数更新&lt;/p&gt;
&lt;h3 id=&quot;2-Batch-Gradient-Descent&quot;&gt;&lt;a href=&quot;#2-Batch-Gradient-Descent&quot; class=&quot;headerlink&quot; title=&quot;2.Batch Gradient Descent&quot;&gt;&lt;/a&gt;2.Batch Gradient Descent&lt;/h3&gt;&lt;p&gt;每次处理完所有训练集进行梯度下降&lt;/p&gt;
&lt;h3 id=&quot;3-Stochastic-Gradient-Descent&quot;&gt;&lt;a href=&quot;#3-Stochastic-Gradient-Descent&quot; class=&quot;headerlink&quot; title=&quot;3.Stochastic Gradient Descent&quot;&gt;&lt;/a&gt;3.Stochastic Gradient Descent&lt;/h3&gt;&lt;p&gt;每次处理完一个样本就进行梯度下降&lt;/p&gt;
&lt;h2 id=&quot;二、反向传播加速梯度下降的过程&quot;&gt;&lt;a href=&quot;#二、反向传播加速梯度下降的过程&quot; class=&quot;headerlink&quot; title=&quot;二、反向传播加速梯度下降的过程&quot;&gt;&lt;/a&gt;二、反向传播加速梯度下降的过程&lt;/h2&gt;&lt;h3 id=&quot;1-Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）&quot;&gt;&lt;a href=&quot;#1-Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）&quot; class=&quot;headerlink&quot; title=&quot;1.Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）&quot;&gt;&lt;/a&gt;1.Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）&lt;/h3&gt;&lt;h3 id=&quot;2-RMS-Prop：使波动大的维度参数更新慢，波动小的维度参数更新快&quot;&gt;&lt;a href=&quot;#2-RMS-Prop：使波动大的维度参数更新慢，波动小的维度参数更新快&quot; class=&quot;headerlink&quot; title=&quot;2.RMS Prop：使波动大的维度参数更新慢，波动小的维度参数更新快&quot;&gt;&lt;/a&gt;2.RMS Prop：使波动大的维度参数更新慢，波动小的维度参数更新快&lt;/h3&gt;&lt;h3 id=&quot;3-Adam：将Momentum-和-RMS-Prop相结合&quot;&gt;&lt;a href=&quot;#3-Adam：将Momentum-和-RMS-Prop相结合&quot; class=&quot;headerlink&quot; title=&quot;3.Adam：将Momentum 和 RMS Prop相结合&quot;&gt;&lt;/a&gt;3.Adam：将Momentum 和 RMS Prop相结合&lt;/h3&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning 吴恩达 课程笔记大纲</title>
    <link href="http://yoursite.com/2018/06/27/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%A4%A7%E7%BA%B2/"/>
    <id>http://yoursite.com/2018/06/27/Deep-Learning-吴恩达-课程笔记大纲/</id>
    <published>2018-06-27T02:59:57.000Z</published>
    <updated>2018-07-22T03:13:28.242Z</updated>
    
    <content type="html"><![CDATA[<p>PS：Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization和Neural Networks and Deep Learning 两门课的课程笔记大纲梳理，我用我觉得的理解进行归类，方面以后查找 ~</p><a id="more"></a><h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><p><br></p><h2 id="一、以神经网络的结构看逻辑回归"><a href="#一、以神经网络的结构看逻辑回归" class="headerlink" title="一、以神经网络的结构看逻辑回归"></a>一、以神经网络的结构看逻辑回归</h2><ol><li>sigmoid函数</li><li>loss function 损失函数和 cost funcrion 代价函数</li><li>Gradient Descent 梯度下降法</li></ol><p><br></p><h2 id="二、向量化（Vectorization）-对应到硬件就是并行计算"><a href="#二、向量化（Vectorization）-对应到硬件就是并行计算" class="headerlink" title="二、向量化（Vectorization）(对应到硬件就是并行计算)"></a>二、向量化（Vectorization）(对应到硬件就是并行计算)</h2><p><br></p><h2 id="三、广播（broadcasting）"><a href="#三、广播（broadcasting）" class="headerlink" title="三、广播（broadcasting）"></a>三、广播（broadcasting）</h2><p><br></p><p><br></p><h1 id="浅层神经网络"><a href="#浅层神经网络" class="headerlink" title="浅层神经网络"></a>浅层神经网络</h1><p><br></p><h2 id="一、计算神经网络的输出-数学角度"><a href="#一、计算神经网络的输出-数学角度" class="headerlink" title="一、计算神经网络的输出 (数学角度)"></a>一、计算神经网络的输出 (数学角度)</h2><p><br></p><h2 id="二、向量化实现-代码实现"><a href="#二、向量化实现-代码实现" class="headerlink" title="二、向量化实现 (代码实现)"></a>二、向量化实现 (代码实现)</h2><p><br></p><h2 id="三、激活函数"><a href="#三、激活函数" class="headerlink" title="三、激活函数"></a>三、激活函数</h2><p><br></p><h2 id="四、神经网络的反向梯度下降-数学和代码"><a href="#四、神经网络的反向梯度下降-数学和代码" class="headerlink" title="四、神经网络的反向梯度下降(数学和代码)"></a>四、神经网络的反向梯度下降(数学和代码)</h2><p><br></p><h2 id="五、参数随机初始化"><a href="#五、参数随机初始化" class="headerlink" title="五、参数随机初始化"></a>五、参数随机初始化</h2><p><br></p><p><br></p><h1 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h1><h2 id="一、使用深层表示的原因"><a href="#一、使用深层表示的原因" class="headerlink" title="一、使用深层表示的原因"></a>一、使用深层表示的原因</h2><p><br></p><h2 id="二、搭建深层神经网络块"><a href="#二、搭建深层神经网络块" class="headerlink" title="二、搭建深层神经网络块"></a>二、搭建深层神经网络块</h2><p><br></p><h2 id="三、矩阵的维度"><a href="#三、矩阵的维度" class="headerlink" title="三、矩阵的维度"></a>三、矩阵的维度</h2><p><br></p><p><br></p><hr><h1 id="优化（一）-—-过拟合"><a href="#优化（一）-—-过拟合" class="headerlink" title="优化（一） — 过拟合"></a>优化（一） — 过拟合</h1><p><br></p><h2 id="一、数据划分：训练-验证-测试集（训练之前）"><a href="#一、数据划分：训练-验证-测试集（训练之前）" class="headerlink" title="一、数据划分：训练 / 验证 / 测试集（训练之前）"></a>一、数据划分：训练 / 验证 / 测试集（训练之前）</h2><p><br></p><p><br></p><h2 id="二、模型估计：偏差-方差（训练之后，对训练结果的评估）"><a href="#二、模型估计：偏差-方差（训练之后，对训练结果的评估）" class="headerlink" title="二、模型估计：偏差 / 方差（训练之后，对训练结果的评估）"></a>二、模型估计：偏差 / 方差（训练之后，对训练结果的评估）</h2><p><br></p><h3 id="1-高偏差的解决："><a href="#1-高偏差的解决：" class="headerlink" title="1.高偏差的解决："></a>1.<strong>高偏差</strong>的解决：</h3><p>（1）扩大网络规模，如添加隐藏层或隐藏单元数目<br>（2）寻找合适的网络架构，使用更大的 NN 结构<br>（3）花费更长时间训练，增加迭代次数</p><p><br></p><h3 id="2-高方差的解决："><a href="#2-高方差的解决：" class="headerlink" title="2.高方差的解决："></a>2.<strong>高方差</strong>的解决：</h3><p>（1）获取更多的数据<br>（2）正则化（regularization），防止过拟合<br>（3）寻找更合适的网络结构</p><p><br></p><p><br></p><h2 id="三、解决高方差的方法：正则化（regularization），防止过拟合（对于训练结果不理想的解决方法）"><a href="#三、解决高方差的方法：正则化（regularization），防止过拟合（对于训练结果不理想的解决方法）" class="headerlink" title="三、解决高方差的方法：正则化（regularization），防止过拟合（对于训练结果不理想的解决方法）"></a>三、解决高方差的方法：正则化（regularization），防止过拟合（对于训练结果不理想的解决方法）</h2><p><br></p><h3 id="1-L2"><a href="#1-L2" class="headerlink" title="1. L2"></a>1. <strong>L2</strong></h3><p>加入的正则化项，衰减了W的权重</p><p><br></p><h3 id="2-dropout"><a href="#2-dropout" class="headerlink" title="2. dropout"></a>2. <strong>dropout</strong></h3><p>神经元不会特别依赖于任何一个输入特征。反向随机失活（Inverted dropout）是实现 dropout 的方法。</p><p><br></p><h3 id="3-数据扩增（Data-Augmentation）"><a href="#3-数据扩增（Data-Augmentation）" class="headerlink" title="3.数据扩增（Data Augmentation）"></a>3.数据扩增（Data Augmentation）</h3><p><br></p><h3 id="4-早停止法（Early-Stopping）"><a href="#4-早停止法（Early-Stopping）" class="headerlink" title="4.早停止法（Early Stopping）"></a>4.早停止法（Early Stopping）</h3><p><br></p><p><br></p><h1 id="优化（二）—-加快学习的算法"><a href="#优化（二）—-加快学习的算法" class="headerlink" title="优化（二）— 加快学习的算法"></a>优化（二）— 加快学习的算法</h1><p><br></p><h2 id="一、对于训练集的处理"><a href="#一、对于训练集的处理" class="headerlink" title="一、对于训练集的处理"></a>一、对于训练集的处理</h2><p><br></p><h3 id="Mini-Batch"><a href="#Mini-Batch" class="headerlink" title="Mini-Batch"></a>Mini-Batch</h3><p>每次处理训练集的一部分进行梯度下降，避免了遍历完整个训练集才完成一次参数更新</p><p><br></p><p><br></p><h2 id="二、初始化参数W、b"><a href="#二、初始化参数W、b" class="headerlink" title="二、初始化参数W、b"></a>二、初始化参数W、b</h2><p><br></p><p>在参数W，b初始化时就进行优化，应对反向传播时会出现的<strong>梯度消失和梯度爆炸</strong>（vanishing/exploding gradient）问题</p><h3 id="1-He-Initialization"><a href="#1-He-Initialization" class="headerlink" title="1.He Initialization"></a>1.He Initialization</h3><p>激活函数使用 ReLU 时，Var(wi)=2/n[l-1] He Initialization</p><p><br></p><h3 id="2-Xavier-initialization"><a href="#2-Xavier-initialization" class="headerlink" title="2.Xavier initialization"></a>2.Xavier initialization</h3><p>激活函数使用 tanh 时，Var(wi)=1/n[l-1] Xavier initialization</p><p><br></p><p><br></p><h2 id="三、反向传播加速梯度下降的过程"><a href="#三、反向传播加速梯度下降的过程" class="headerlink" title="三、反向传播加速梯度下降的过程"></a>三、反向传播加速梯度下降的过程</h2><h3 id="1-Momentum"><a href="#1-Momentum" class="headerlink" title="1.Momentum"></a>1.Momentum</h3><p>将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）</p><p><br></p><h3 id="2-RMS-Prop"><a href="#2-RMS-Prop" class="headerlink" title="2.RMS Prop"></a>2.RMS Prop</h3><p>使波动大的维度参数更新慢，波动小的维度参数更新快</p><p><br></p><h3 id="3-Adam"><a href="#3-Adam" class="headerlink" title="3.Adam"></a>3.Adam</h3><p>将Momentum 和 RMS Prop相结合</p><p><br></p><p><br></p><h2 id="四、改变学习率加速收敛"><a href="#四、改变学习率加速收敛" class="headerlink" title="四、改变学习率加速收敛"></a>四、改变学习率加速收敛</h2><p>学习率衰减：初期学习率大，梯度下降快，后期学习步长小，有助于收敛至最优解</p><p><br></p><p><br></p><h2 id="五、标准化、归一化-Normalization"><a href="#五、标准化、归一化-Normalization" class="headerlink" title="五、标准化、归一化 Normalization"></a>五、标准化、归一化 Normalization</h2><p><br></p><h3 id="1-标准化输入（Normalization）"><a href="#1-标准化输入（Normalization）" class="headerlink" title="1.标准化输入（Normalization）"></a>1.标准化输入（Normalization）</h3><p>对于输入特征标准化，加快学习速度</p><p><br></p><h3 id="2-Batch-Normalization"><a href="#2-Batch-Normalization" class="headerlink" title="2.Batch Normalization"></a>2.Batch Normalization</h3><ol><li><p>通过对隐藏层各神经元的输入标准化，<strong>提高神经网络训练速度</strong></p></li><li><p>减小不同的样本分布（<strong>Covariate Shift</strong> ）对后面几层网络的影响。样本分布改变，使得前面层的权重变化后，输出Z发生很大变化，但是在Batch Normalization作用下，归一化后的Z分布相比于前一次迭代的归一化的Z，并没有发生很大变化，所以作为这一层的输出，并不会对后面一层的输入的分布产生很大影响，从而减少各层 W、b 之间的耦合性，<strong>增强鲁棒性（robust）</strong></p><p>（这里表扬一下自己，感觉自己太厉害了，竟然在厕所里想通了Covariate Shift 的意思，哈哈哈~）</p></li><li><p>Batch Normalization 也起到<strong>微弱的正则化</strong>（regularization）效果</p></li></ol><p><br></p><p>PS：我老是把Normalization和Regularization弄混淆 ~</p><p><br></p><p><br></p><h1 id="超参数调试"><a href="#超参数调试" class="headerlink" title="超参数调试"></a>超参数调试</h1><p><br></p><h3 id="1-超参数重要程度排序"><a href="#1-超参数重要程度排序" class="headerlink" title="1.超参数重要程度排序"></a>1.超参数重要程度排序</h3><ul><li><p>最重要：</p><p>学习率 α</p></li><li><p>其次重要：</p><p>β：动量衰减参数，常设置为 0.9</p><p>hidden units：各隐藏层神经元个数</p><p>mini-batch 的大小</p></li><li><p>再次重要：</p><p>β1，β2，ϵ：Adam 优化算法的超参数，常设为 0.9、0.999、10−810−8</p><p>layers：神经网络层数</p><p>decay_rate：学习衰减率</p></li></ul><p><br></p><h3 id="2-调参技巧"><a href="#2-调参技巧" class="headerlink" title="2.调参技巧"></a>2.调参技巧</h3><p>随机选择点进行调试</p><p><br></p><h3 id="3-为超参数选择合适调试范围"><a href="#3-为超参数选择合适调试范围" class="headerlink" title="3.为超参数选择合适调试范围"></a>3.为超参数选择合适调试范围</h3><p><br></p><p><br></p><h1 id="优化会遇到的问题"><a href="#优化会遇到的问题" class="headerlink" title="优化会遇到的问题"></a>优化会遇到的问题</h1><h2 id="一、判断梯度计算是否正确"><a href="#一、判断梯度计算是否正确" class="headerlink" title="一、判断梯度计算是否正确"></a>一、判断梯度计算是否正确</h2><p>由于微分分析计算梯度容易出错 ，实际操作时常常<strong>将分析梯度法的结果和数值梯度法的结果作比较</strong>，以此来<strong>检查其实现的正确性</strong></p><p>注意：只适用于debug，判断梯度计算是否正确，且不能和dropout同时使用</p><p><br></p><h2 id="二、高维度优化遇到的问题"><a href="#二、高维度优化遇到的问题" class="headerlink" title="二、高维度优化遇到的问题"></a>二、高维度优化遇到的问题</h2><p>1.低维度：局部最优会阻碍找到全局最优的解</p><p>2.高维度：不存在局部最优的阻碍，但是<strong>鞍点</strong>会降低学习速度，通过Adama算法加速提高效率</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization和Neural Networks and Deep Learning 两门课的课程笔记大纲梳理，我用我觉得的理解进行归类，方面以后查找 ~&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业（六）</title>
    <link href="http://yoursite.com/2018/06/25/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E5%85%AD%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/25/Deep-Learning-吴恩达-作业（六）/</id>
    <published>2018-06-25T12:49:20.000Z</published>
    <updated>2018-06-26T01:59:02.890Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h1><p>PS：用双边求导的结果与梯度的逼近程度来检查梯度是否计算错误，需要对每个神经元的W、b进行计算</p><a id="more"></a><h2 id="1-双边求导"><a href="#1-双边求导" class="headerlink" title="1.双边求导"></a>1.双边求导</h2><p><img src="http://p8ge6t5tt.bkt.clouddn.com/gradient%20checking1.JPG" alt=""></p><h2 id="2-逼近程度的判断"><a href="#2-逼近程度的判断" class="headerlink" title="2.逼近程度的判断"></a>2.逼近程度的判断</h2><p><img src="http://p8ge6t5tt.bkt.clouddn.com/gradient%20checking2.JPG" alt=""></p><h2 id="最后贴上我的作业"><a href="#最后贴上我的作业" class="headerlink" title="最后贴上我的作业"></a>最后贴上我的作业</h2><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E4%BD%9C%E4%B8%9A%E5%85%AD.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Gradient-Checking&quot;&gt;&lt;a href=&quot;#Gradient-Checking&quot; class=&quot;headerlink&quot; title=&quot;Gradient Checking&quot;&gt;&lt;/a&gt;Gradient Checking&lt;/h1&gt;&lt;p&gt;PS：用双边求导的结果与梯度的逼近程度来检查梯度是否计算错误，需要对每个神经元的W、b进行计算&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业（五）</title>
    <link href="http://yoursite.com/2018/06/25/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/25/Deep-Learning-吴恩达-作业（五）/</id>
    <published>2018-06-25T09:37:43.000Z</published>
    <updated>2018-07-26T12:31:26.494Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><p>PS：要明确的是正则化是对过拟合现象的修正，不是必须要做的</p><a id="more"></a><h2 id="1-L2-Regularization"><a href="#1-L2-Regularization" class="headerlink" title="1. L2 Regularization"></a>1. L2 Regularization</h2><p>1.每前向传播完成后计算 cost function 需要加上正则化项</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/regulation5.JPG" alt=""></p><p>2.反向传播计算dW时也需要加上正则化项</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/regulation6.JPG" alt=""></p><p>3.权重W变为较小的值，从而防止过拟合</p><h2 id="2-Dropout"><a href="#2-Dropout" class="headerlink" title="2. Dropout"></a>2. Dropout</h2><p>1.将前一层的输出同时是这一层的输入A[l]，以一定概率keep_prob丢弃，再将剩余的 <strong>A[l]除以keep_prob</strong>  (而不影响下一层的计算)</p><p>2.反向传播时，对于 dA[l] 进行同样的丢弃和缩放</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E4%BD%9C%E4%B8%9A%E4%BA%94.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Regularization&quot;&gt;&lt;a href=&quot;#Regularization&quot; class=&quot;headerlink&quot; title=&quot;Regularization&quot;&gt;&lt;/a&gt;Regularization&lt;/h1&gt;&lt;p&gt;PS：要明确的是正则化是对过拟合现象的修正，不是必须要做的&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
</feed>
