<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>绿小蕤</title>
  <icon>https://www.gravatar.com/avatar/e4d7a8bd1cb84fb3b4123916b4ea2f6b</icon>
  <subtitle>好逸恶劳,贪生怕死</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-01-15T09:37:59.267Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>绿小蕤</name>
    <email>528036346@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>密码学</title>
    <link href="http://yoursite.com/2019/01/15/%E5%AF%86%E7%A0%81%E5%AD%A6/"/>
    <id>http://yoursite.com/2019/01/15/密码学/</id>
    <published>2019-01-15T09:20:33.000Z</published>
    <updated>2019-01-15T09:37:59.267Z</updated>
    
    <content type="html"><![CDATA[<p>PS：最近老是看到MD5和SHA-1，不知道这是什么，所以就看了一下crash course解密的那一集，突然发现，自己对计算机网络的认识真的是太肤浅了<br>以前觉得hash不就是某种数据结构，没想过有其它用途<br>以前觉得网络就是从某个地方取数据，没考虑过安全问题</p><a id="more"></a><p><br></p><hr><p>crash course 学习心得</p><p>替换加密：凯撒密码</p><p>将字母替换为其它字母，缺点是字母出现频率一样，字母E加密为字母X，字母E出现频率很高，则密文中字母X的出现频率也会很高</p><p>移位加密</p><p>将明文放入网格，从某个方向读</p><p>Enigma机</p><p>这个转子的机械结构真的是神奇</p><p>密钥交换：Diffie-Hellman </p><p>Diffie-Hellman 通过模幂运算这种单向函数，可以双方共享密钥<br><code>基数^指数 mod 模</code> 其中基数和模是公开的   </p><p>甲：选择了指数x 甲=B^x mod M<br>乙：选择了指数y 乙=B^y mod M<br>甲乙互相交换模幂运算结果<br>甲乙建立的共享密钥=(B^x mod M)^y=(B^y mod M)^x</p><p>数学真神奇</p><hr><p><br></p><h2 id="一、对称加密、非对称加密"><a href="#一、对称加密、非对称加密" class="headerlink" title="一、对称加密、非对称加密"></a>一、对称加密、非对称加密</h2><h3 id="对称加密"><a href="#对称加密" class="headerlink" title="对称加密"></a>对称加密</h3><p>加、解密使用的同是一串密钥， 常用的对称加密算法：DES，AES等</p><h3 id="非对称加密"><a href="#非对称加密" class="headerlink" title="非对称加密"></a>非对称加密</h3><p>加、解密使用不同的密钥，公钥加密的信息，只有私钥才能解密。反之，私钥加密的信息，只有公钥才能解密。 常用的非对称加密算法：RSA</p><p><br></p><h2 id="二、数字摘要、数字签名"><a href="#二、数字摘要、数字签名" class="headerlink" title="二、数字摘要、数字签名"></a>二、数字摘要、数字签名</h2><p>下面面这个例子很不错啊</p><p><a href="http://www.cnblogs.com/JeffreySun/archive/2010/06/24/1627247.html" target="_blank" rel="noopener">http://www.cnblogs.com/JeffreySun/archive/2010/06/24/1627247.html</a></p><h3 id="１-数字摘要（Digital-Digest）"><a href="#１-数字摘要（Digital-Digest）" class="headerlink" title="１.数字摘要（Digital Digest）"></a>１.数字摘要（Digital Digest）</h3><p>数字摘要也称作为安全HASH编码法（SHA：Secure Hash Algorithm），常用的摘要算法有MD5、SHA1、SHA256</p><p>一段信息，经过摘要算法得到一串哈希值，就是摘要(digest)，信息是任意长度，而摘要是定长<br>摘要不同于加密算法，不存在解密，从摘要反推原信息很难（可以认为能加密但无法解密还原，但可以通过比对摘要是否相同，判断原信息是否相同）</p><p>算法的基本要求</p><ul><li>正向快速：给定明文和 hash 算法，在有限时间和有限资源内能计算出 hash 值</li><li>逆向困难：给定（若干） hash 值，在有限时间内很难（基本不可能）逆推出明文</li><li>输入敏感：原始输入信息修改一点信息，产生的 hash 值看起来应该都有很大不同</li><li>冲突避免：很难找到两段内容不同的明文，使得它们的 hash 值一致（发生冲突）</li></ul><p>摘要相同，信息一定相同，利用这个特点，摘要还可以用于应用在网站后台数据库中，用于比对用户的输入密码和预设密码是否相同。这里都无需关心密码本身是什么，关注的是密码是否相同，而密码是否相同取决于摘要是否相同，所以问题转化成了摘要是否相同。将用户密码的摘要而不是密码本身保存在数据库中，因为反推很难，所以真实密码是保密的。</p><p><br></p><h3 id="2-数字签名（Digital-Signature）"><a href="#2-数字签名（Digital-Signature）" class="headerlink" title="2.数字签名（Digital Signature）"></a>2.数字签名（Digital Signature）</h3><p>数字签名一般是结合了数字摘要技术和公开密钥算法共同使用</p><p>(1)生成签名信息 </p><ul><li>对信息M进行HASH函数处理，生成摘要H</li><li>发送者私钥加密H来获取数字签名S    </li><li><p>发送 {M, S}  </p><p>(2)验证签名信息 </p></li><li><p>接受{M, S} 并区分开它们 </p></li><li>对接收到的信息M进行HASH函数处理，生成摘要H*    </li><li>取得发送者的公钥    </li><li>用公钥解密S，来获取H </li><li>比较H和H<em>，如果H和H</em>是一样的，即说明信息在发送过程中没有被篡改</li></ul><p>由于明文信息也通过网络进行传递，因此在做完数字签名后，还要对整个信息（包括明文信息M和数字签名的密文信息S）进行加密</p><p><br></p><h3 id="3-数字证书（CA）"><a href="#3-数字证书（CA）" class="headerlink" title="3.数字证书（CA）"></a>3.数字证书（CA）</h3><p>数字证书是由权威机构给某网站颁发的一种认可凭证<br>数字证书可以保证数字证书里的公钥确实是这个证书的所有者</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：最近老是看到MD5和SHA-1，不知道这是什么，所以就看了一下crash course解密的那一集，突然发现，自己对计算机网络的认识真的是太肤浅了&lt;br&gt;以前觉得hash不就是某种数据结构，没想过有其它用途&lt;br&gt;以前觉得网络就是从某个地方取数据，没考虑过安全问题&lt;/p&gt;
    
    </summary>
    
    
      <category term="计算机网络、密码学" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E3%80%81%E5%AF%86%E7%A0%81%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>caffe（九）——配置文件</title>
    <link href="http://yoursite.com/2019/01/10/caffe%EF%BC%88%E4%B9%9D%EF%BC%89%E2%80%94%E2%80%94%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"/>
    <id>http://yoursite.com/2019/01/10/caffe（九）——配置文件/</id>
    <published>2019-01-10T08:58:34.000Z</published>
    <updated>2019-01-10T09:08:28.965Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th>文件</th><th>实现</th></tr></thead><tbody><tr><td>solver.prototxt</td><td>优化</td></tr><tr><td>train_val.prototxt</td><td>训练、测试</td></tr><tr><td>deploy.prototxt</td><td>预测</td></tr></tbody></table><a id="more"></a><p><br></p><h2 id="一、solver-prototxt"><a href="#一、solver-prototxt" class="headerlink" title="一、solver.prototxt"></a>一、solver.prototxt</h2><p>优化(参数更新)</p><p>优化与网络模型分离，所以需要两个文件 solver.prototxt 与 train_val.prototxt，在solver.prototxt 中调用网络模型 train_val.prototxt</p><p>PS：我突然有些理解optimization的意思了，就是减少error，即通过调整权重来减小loss，即预测值与实际值的差距</p><h3 id="1-学习率调整策略"><a href="#1-学习率调整策略" class="headerlink" title="1.学习率调整策略"></a>1.学习率调整策略</h3><p>lr_policy</p><table><thead><tr><th>type</th><th>参数设置</th></tr></thead><tbody><tr><td>fixed</td><td>保持base_lr不变</td></tr><tr><td>step</td><td>如果设置为step,则还需要设置一个stepsize,  返回 base_lr * gamma ^ (floor(iter / stepsize)),其中iter表示当前的迭代次数</td></tr><tr><td>exp</td><td>返回base_lr *gamma^iter，iter为当前迭代次数</td></tr><tr><td>inv</td><td>如果设置为inv,还需要设置一个power, 返回base_lr <em> (1 + gamma </em> iter) ^ (- power)</td></tr><tr><td>multistep</td><td>如果设置为multistep,则还需要设置一个stepvalue。这个参数和step很相似，step是均匀等间隔变化，而multistep则是根据stepvalue值变化</td></tr><tr><td>poly</td><td>学习率进行多项式误差, 返回 base_lr (1 - iter/max_iter) ^ (power)</td></tr><tr><td>sigmoid</td><td>学习率进行sigmod衰减，返回 base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))</td></tr></tbody></table><p><br></p><h2 id="二、train-val-prototxt"><a href="#二、train-val-prototxt" class="headerlink" title="二、train_val.prototxt"></a>二、train_val.prototxt</h2><p>训练和验证阶段</p><p>定义训练集和验证集的来源<br>训练阶段，定义损失函数<br>验证阶段，测试集的准确度</p><p><br></p><h2 id="三、deploy-prototxt"><a href="#三、deploy-prototxt" class="headerlink" title="三、deploy.prototxt"></a>三、deploy.prototxt</h2><p>预测阶段</p><p>通过分类器，输出各类的概率</p>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;文件&lt;/th&gt;
&lt;th&gt;实现&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;solver.prototxt&lt;/td&gt;
&lt;td&gt;优化&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;train_val.prototxt&lt;/td&gt;
&lt;td&gt;训练、测试&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;deploy.prototxt&lt;/td&gt;
&lt;td&gt;预测&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>caffe（八）——layer</title>
    <link href="http://yoursite.com/2019/01/10/caffe%EF%BC%88%E5%85%AB%EF%BC%89%E2%80%94%E2%80%94layer/"/>
    <id>http://yoursite.com/2019/01/10/caffe（八）——layer/</id>
    <published>2019-01-10T08:55:09.000Z</published>
    <updated>2019-01-10T09:03:55.279Z</updated>
    
    <content type="html"><![CDATA[<p>官网 tutorial：<a href="http://caffe.berkeleyvision.org/tutorial/" target="_blank" rel="noopener">http://caffe.berkeleyvision.org/tutorial/</a></p><p>官网 layer 解析：<a href="http://caffe.berkeleyvision.org/tutorial/layers.html" target="_blank" rel="noopener">http://caffe.berkeleyvision.org/tutorial/layers.html</a></p><p>参考该博主的caffe学习系列，写得很详细，超赞的：<a href="https://www.cnblogs.com/denny402/tag/caffe/" target="_blank" rel="noopener">https://www.cnblogs.com/denny402/tag/caffe/</a></p><a id="more"></a><p><br></p><h2 id="一、数据层"><a href="#一、数据层" class="headerlink" title="一、数据层"></a>一、数据层</h2><p>提供其它格式转换为Blobs输入，也提供数据从Blobs转换成别的格式保存输出</p><ul><li>数据来源</li><li>高效的数据库的 LevelDB 和 LMDB</li><li>直接来自于内存</li><li>来自磁盘的 hdf5 文件和图片格式文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;cifar&quot;</span><br><span class="line">  type: &quot;Data&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    # scale为0.00390625，实际上就是1/255, 即将输入数据由0-255归一化到0-1之间</span><br><span class="line">    scale: 0.00390625</span><br><span class="line"></span><br><span class="line">    # 用一个配置文件来进行均值操作</span><br><span class="line">    mean_file_size: &quot;examples/cifar10/mean.binaryproto&quot;</span><br><span class="line"></span><br><span class="line">    # 1表示开启镜像，0表示关闭，也可用ture和false来表示</span><br><span class="line">    mirror: 1  </span><br><span class="line"></span><br><span class="line">    # 剪裁一个 227*227的图块，在训练阶段随机剪裁，在测试阶段从中间裁剪</span><br><span class="line">    crop_size: 227</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: &quot;examples/cifar10/cifar10_train_lmdb&quot;</span><br><span class="line">    batch_size: 100</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(1)top、bottom: 用bottom来输入数据，用top来输出数据，只有top没有bottom，则只有输出，没有输入</p><p>(2)data、label: 在数据层，(data,label)是分类模型所必需的</p><p>(3)transform_param：数据预处理</p><p>(4)data_param：不同数据来源，这里只考虑数据来源于数据库 backend: LMDB</p><p><br></p><h2 id="二、视觉层"><a href="#二、视觉层" class="headerlink" title="二、视觉层"></a>二、视觉层</h2><p>1.卷积层  type: “Convolution”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;conv1&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;data&quot;</span><br><span class="line">  top: &quot;conv1&quot;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 20</span><br><span class="line">    kernel_size: 5</span><br><span class="line">    stride: 1</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: &quot;xavier&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: &quot;constant&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(1)param</p><ul><li>lr_mult: 学习率的系数，最终的学习率是 <strong>lr_mult*base_lr</strong><br>如果有两个lr_mult, 则第一个表示权值的学习率，第二个表示偏置项的学习率，一般偏置项的学习率是权值学习率的 <strong>两倍</strong></li></ul><p>(2)convolution_param:</p><ul><li>num_output: 卷积核的个数，fearure map的个数</li><li>kernel_size: 卷积核的大小，也可以用kernel_h和kernel_w设定</li><li>stride: 卷积核的步长，也可以用stride_h和stride_w来设定</li><li>pad: 扩充边缘，默认为0，不扩充，扩充的时候是左右、上下对称的，也可以通过pad_h和pad_w来设定（注意，扩充是对图像边缘，而不是卷积核）</li><li>weight_filler: 权值初始化，默认为“constant”，全为0，一般用”xavier”或”gaussian”</li><li>bias_filler: 偏置项初始化，一般设置为”constant”，全为0</li><li>bias_term: 是否开启偏置项，默认为true, 开启</li><li>group: 分组卷积，默认为1组（减少运算量，但groups之间不share feature map）</li></ul><p>输入输出尺寸计算</p><p>输入：n<em>c0</em>w0<em>h0<br>输出：n</em>c1<em>w1</em>h1(c1就是num_output)<br> w1=(w0+2<em>pad-kernel_size)/stride+1;<br> h1=(h0+2</em>pad-kernel_size)/stride+1;<br>(如果设置stride为1，pad=(kernel_size-1)/2，则宽度和高度不变）</p><p><br></p><h3 id="2-池化层-type-“Pooling”"><a href="#2-池化层-type-“Pooling”" class="headerlink" title="2.池化层 type: “Pooling”"></a>2.池化层 type: “Pooling”</h3><p>与卷积层相同</p><ul><li>pool: 池化方法，有MAX、AVE、TOCHASTIC</li></ul><h3 id="3-LRN-type-LRN"><a href="#3-LRN-type-LRN" class="headerlink" title="3.LRN type: LRN"></a>3.LRN type: LRN</h3><p>Local Response Normalization局部响应归一化，侧抑制，在通道方向上</p><h3 id="4-im2col"><a href="#4-im2col" class="headerlink" title="4.im2col"></a>4.im2col</h3><p>将矩阵重叠划分为多个子矩阵，将其序列化为向量</p><p>在caffe中，卷积计算就是先进行im2col，再进行内积计算</p><p><br></p><h2 id="三、激活层"><a href="#三、激活层" class="headerlink" title="三、激活层"></a>三、激活层</h2><p>1.Sigmoid<br>2.ReLU<br>3.TanH<br>4.AbsVal: 绝对值<br>5.Power： 幂运算</p><p><br></p><h2 id="四、其它层"><a href="#四、其它层" class="headerlink" title="四、其它层"></a>四、其它层</h2><h3 id="1-softmax-loss层-type-“SoftmaxWithLoss”"><a href="#1-softmax-loss层-type-“SoftmaxWithLoss”" class="headerlink" title="1.softmax-loss层 type: “SoftmaxWithLoss”"></a>1.softmax-loss层 type: “SoftmaxWithLoss”</h3><h3 id="2-softmax-type-“Softmax”"><a href="#2-softmax-type-“Softmax”" class="headerlink" title="2.softmax type: “Softmax”"></a>2.softmax type: “Softmax”</h3><h3 id="3-全连接层-type-“InnerProduct”"><a href="#3-全连接层-type-“InnerProduct”" class="headerlink" title="3.全连接层 type: “InnerProduct”"></a>3.全连接层 type: “InnerProduct”</h3><h3 id="4-精确度-type-“Accuracy”"><a href="#4-精确度-type-“Accuracy”" class="headerlink" title="4.精确度   type: “Accuracy”"></a>4.精确度   type: “Accuracy”</h3><p>预测精确度，只有test阶段才有，因此需要加入include参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;accuracy&quot;</span><br><span class="line">  type: &quot;Accuracy&quot;</span><br><span class="line">  bottom: &quot;ip2&quot;</span><br><span class="line">  bottom: &quot;label&quot;</span><br><span class="line">  top: &quot;accuracy&quot;</span><br><span class="line"></span><br><span class="line">  # Test阶段的预测精确度</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-reshape-type-“Reshape”"><a href="#5-reshape-type-“Reshape”" class="headerlink" title="5.reshape type: “Reshape”"></a>5.reshape type: “Reshape”</h3><p>改变维度</p><h3 id="6-Dropout"><a href="#6-Dropout" class="headerlink" title="6.Dropout"></a>6.Dropout</h3><p>设置dropout_ratio</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;官网 tutorial：&lt;a href=&quot;http://caffe.berkeleyvision.org/tutorial/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://caffe.berkeleyvision.org/tutorial/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官网 layer 解析：&lt;a href=&quot;http://caffe.berkeleyvision.org/tutorial/layers.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://caffe.berkeleyvision.org/tutorial/layers.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;参考该博主的caffe学习系列，写得很详细，超赞的：&lt;a href=&quot;https://www.cnblogs.com/denny402/tag/caffe/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/denny402/tag/caffe/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>caffe（七）——blob</title>
    <link href="http://yoursite.com/2019/01/10/caffe%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94blob/"/>
    <id>http://yoursite.com/2019/01/10/caffe（七）——blob/</id>
    <published>2019-01-10T08:54:33.000Z</published>
    <updated>2019-01-10T09:02:00.040Z</updated>
    
    <content type="html"><![CDATA[<p>官网 tutorial：<a href="http://caffe.berkeleyvision.org/tutorial/" target="_blank" rel="noopener">http://caffe.berkeleyvision.org/tutorial/</a></p><h2 id="caffe模型"><a href="#caffe模型" class="headerlink" title="caffe模型"></a>caffe模型</h2><p>Nets网络由layers层构成，在层中的计算和层与层之间的数据流动依赖于Blobs</p><h2 id="caffe模型训练"><a href="#caffe模型训练" class="headerlink" title="caffe模型训练"></a>caffe模型训练</h2><p>Forward：Data-&gt;layers-&gt;loss<br>Backward：loss-&gt;diff-&gt;update weight(solver：optimization method)</p><a id="more"></a><p><br></p><h2 id="Blob"><a href="#Blob" class="headerlink" title="Blob"></a>Blob</h2><p>PS：关于这个</p><h3 id="1-Blob与数据"><a href="#1-Blob与数据" class="headerlink" title="1.Blob与数据"></a>1.Blob与数据</h3><p>A Blob is a wrapper over the actual data being processed and passed along by Caffe. Blobs provide a unified memory interface holding data; e.g., batches of images, model parameters, and derivatives for optimization.</p><p>forward、backward过程中输入数据、模型权重、梯度值，都是以blob的形式在层与层之间的流动以及计算</p><p> A blob stores two chunks of memories, data and diff. The former is the normal data that we pass along, and the latter is the gradient computed by the network.</p><p> 一个blob对应于计算后需要流入下一层的数据和梯度</p><h3 id="2-Blob的维度"><a href="#2-Blob的维度" class="headerlink" title="2.Blob的维度"></a>2.Blob的维度</h3><p>A blob is an N-dimensional array stored in a C-contiguous fashion.</p><p><strong>C contiguous fashion</strong>  means that the n-dim data is stored as a long and contiguous array in memory. The order of the elements in memory is according to C fashion: trailing dimensions are stored first. That is if you have c by h by w 3d blob, <strong>in memory rows will be saved one after the other</strong>, and after completing all the rows of the first channel, only then the rows of the next channel are written.</p><p>conventional blob dimensions for batches of image data：<br>batch size N x channel K x height H x width W</p><h3 id="3-CPU与GPU的同步"><a href="#3-CPU与GPU的同步" class="headerlink" title="3.CPU与GPU的同步"></a>3.CPU与GPU的同步</h3><p>The reason for such design is that, a Blob uses a SyncedMem class to synchronize values between the CPU and GPU in order to hide the synchronization details and to minimize data transfer.</p><p>SyncedMem 类实现CPU与GPU的同步，具体实现是依赖标志位</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;官网 tutorial：&lt;a href=&quot;http://caffe.berkeleyvision.org/tutorial/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://caffe.berkeleyvision.org/tutorial/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;caffe模型&quot;&gt;&lt;a href=&quot;#caffe模型&quot; class=&quot;headerlink&quot; title=&quot;caffe模型&quot;&gt;&lt;/a&gt;caffe模型&lt;/h2&gt;&lt;p&gt;Nets网络由layers层构成，在层中的计算和层与层之间的数据流动依赖于Blobs&lt;/p&gt;
&lt;h2 id=&quot;caffe模型训练&quot;&gt;&lt;a href=&quot;#caffe模型训练&quot; class=&quot;headerlink&quot; title=&quot;caffe模型训练&quot;&gt;&lt;/a&gt;caffe模型训练&lt;/h2&gt;&lt;p&gt;Forward：Data-&amp;gt;layers-&amp;gt;loss&lt;br&gt;Backward：loss-&amp;gt;diff-&amp;gt;update weight(solver：optimization method)&lt;/p&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>TCP/IP HTTP Soket</title>
    <link href="http://yoursite.com/2019/01/08/TCP-IP-HTTP-Soket/"/>
    <id>http://yoursite.com/2019/01/08/TCP-IP-HTTP-Soket/</id>
    <published>2019-01-08T03:49:34.000Z</published>
    <updated>2019-01-08T11:18:16.284Z</updated>
    
    <content type="html"><![CDATA[<p>OSI网络由下往上分为物理层、数据链路层、网络层、传输层、会话层、表示层和应用层</p><a id="more"></a><h2 id="一、TCP-IP-HTTP-Socket"><a href="#一、TCP-IP-HTTP-Socket" class="headerlink" title="一、TCP/IP HTTP Socket"></a>一、TCP/IP HTTP Socket</h2><p><a href="https://blog.csdn.net/Pk_zsq/article/details/6087367" target="_blank" rel="noopener">https://blog.csdn.net/Pk_zsq/article/details/6087367</a></p><p><strong>IP协议对应于网络层，TCP协议对应于传输层，HTTP协议对应于应用层</strong></p><h3 id="1-TCP-IP"><a href="#1-TCP-IP" class="headerlink" title="1.TCP/IP"></a>1.TCP/IP</h3><p>TCP/IP是一个 <strong>协议簇</strong>，包括应用层,传输层，网络层，网络访问层，每一层包括很多协议，因为TCP、IP协议是两个很重要的协议，就用它两命名，主要解决 <strong>数据如何在网络中传输</strong></p><h3 id="2-HTTP"><a href="#2-HTTP" class="headerlink" title="2.HTTP"></a>2.HTTP</h3><p>HTTP协议，主要解决如何 <strong>包装数据</strong></p><h3 id="3-Socket"><a href="#3-Socket" class="headerlink" title="3.Socket"></a>3.Socket</h3><p>Socket本身并不是协议，是对TCP/IP协议的 <strong>封装</strong>，是一个调用接口（API）<br>通过Socket，才能使用TCP/IP协议，<br>就像操作系统会提供标准的编程接口，TCP/IP也要提供可供程序员做网络开发所用的接口，这就是Socket编程接口</p><h2 id="二、TCP三次握手、四次挥手"><a href="#二、TCP三次握手、四次挥手" class="headerlink" title="二、TCP三次握手、四次挥手"></a>二、TCP三次握手、四次挥手</h2><p><a href="https://github.com/jawil/blog/issues/14" target="_blank" rel="noopener">https://github.com/jawil/blog/issues/14</a></p><p>TCP是 <strong>面向连接</strong>的协议，在收发数据前，必须和对方建立可靠的连接</p><p>TCP建立连接为什么是三次握手?是为了满足在不可靠信道上可靠地传输信息，三次通信是理论上的最小值</p><p>三次握手可以防止以下情况的发生：</p><p>client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段，但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求，于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了，由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据，但server却以为新的运输连接已经建立，并一直等待client发来数据，server的等待浪费了资源</p><p>当客户端和服务器通过三次握手建立了TCP连接以后，当数据传送完毕，断开TCP的连接，需要四次挥手</p><h2 id="三、TCP与UDP的区别"><a href="#三、TCP与UDP的区别" class="headerlink" title="三、TCP与UDP的区别"></a>三、TCP与UDP的区别</h2><p>TCP是 <strong>面向连接</strong>的、可靠的数据流传输，发送数据前需要先建立连接，<strong>求安全</strong></p><p>TCP可以处理乱序，丢失的数据包（丢失后重发），拥挤情况自动调整传输速率</p><p>UDP不是面向连接的、不可靠的数据流传输，发送数据前不需要建立连接，<strong>求速度</strong></p><p>TCP适合用于网页，邮件等，UDP适合用于视频，语音广播等</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;OSI网络由下往上分为物理层、数据链路层、网络层、传输层、会话层、表示层和应用层&lt;/p&gt;
    
    </summary>
    
    
      <category term="计算机网络" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>网页生成</title>
    <link href="http://yoursite.com/2019/01/07/%E7%BD%91%E9%A1%B5%E7%94%9F%E6%88%90/"/>
    <id>http://yoursite.com/2019/01/07/网页生成/</id>
    <published>2019-01-07T13:34:17.000Z</published>
    <updated>2019-01-07T13:38:20.907Z</updated>
    
    <content type="html"><![CDATA[<p>PS：我不是做前端开发的，因为搭博客的时候，实在是这个也不会那个也不懂，所以连蒙带猜，下面写的只是我个人的理解 ~</p><a id="more"></a><p><br></p><h2 id="一、html、css、js"><a href="#一、html、css、js" class="headerlink" title="一、html、css、js"></a>一、html、css、js</h2><p>段叔叔说的比喻，可以意会一下，html是一篇文章的名词即网页的内容，那css是形容词即网页的样式，js是动词实现一些交互</p><p><a href="https://blog.csdn.net/celtics00/article/details/60466964" target="_blank" rel="noopener">https://blog.csdn.net/celtics00/article/details/60466964</a></p><h3 id="1-html"><a href="#1-html" class="headerlink" title="1.html"></a>1.html</h3><p>html超文本标记语言，只负责描述网页内容</p><p>超文本是指通过超链接的方式将文字组织在一起，标记相当于对网页各个部分内容进行了区分</p><h3 id="2-css"><a href="#2-css" class="headerlink" title="2.css"></a>2.css</h3><p>描述网页样式的语句合并成一个文件，这个文件叫层叠样式表，简称css</p><h3 id="3-js"><a href="#3-js" class="headerlink" title="3.js"></a>3.js</h3><p>JavaScript是一种轻量级的编程语言，通过将js插入到html中执行，可以对事件作出反应 ，改变html内容、图像、样式</p><h2 id="4-三者的关系"><a href="#4-三者的关系" class="headerlink" title="4.三者的关系"></a>4.三者的关系</h2><p><strong>js文件和css文件最终是要应用到html中</strong></p><p>通过 <code>&lt;script&gt;</code> 元素来插入js代码，<code>&lt;script&gt;</code> 元素可以放在 <code>&lt;head&gt;</code> 或 <code>&lt;body&gt;</code> 中，或者在外部编写js文件，在html中引用</p><p>通过 <code>&lt;style&gt;</code> 元素插入css代码，<code>&lt;style&gt;</code> 元素必须放在 <code>&lt;head&gt;</code> 元素中，或者在外部编写css文件，在html中引用，也可以直接在某元素中规定css样式</p><p><br></p><h2 id="二、Node-js、Express、EJS"><a href="#二、Node-js、Express、EJS" class="headerlink" title="二、Node.js、Express、EJS"></a>二、Node.js、Express、EJS</h2><p>###1.Node.js</p><p>Node.js是一个能够在服务器端运行 JavaScript 代码的运行环境</p><p>Node.js在服务器端可以使用 JavaScript，既然是驱动 JavaScript 的工具，所以也是由 JavaScript 引擎驱动(后面第三部分写到引擎部分)</p><p>PS：其实这里不是很明白，是我理解的问题吗，部署博客之前，一般会看看效果，使用 <code>hexo s</code> 启用服务预览，相当于将本机作为服务器，浏览器从本地端口 localhost：4000 读取数据，浏览器渲染网页，不明白Node.js到底做了什么??</p><h3 id="2-Express"><a href="#2-Express" class="headerlink" title="2.Express"></a>2.Express</h3><p>Express是一个基于Node.js平台的web应用开发框架</p><p>Express的模板有很多，目前较流行的应该是Jade和EJS</p><h3 id="3-EJS"><a href="#3-EJS" class="headerlink" title="3.EJS"></a>3.EJS</h3><p>EJS是Express的模板引擎</p><p><a href="https://blog.csdn.net/zdy0_2004/article/details/49480305" target="_blank" rel="noopener">https://blog.csdn.net/zdy0_2004/article/details/49480305</a></p><p><strong>模板引擎</strong>做的两件事：</p><p>1.根据一定的规则，解析所定义的模板(.ejs文件)</p><p>2.根据数据(.json文件)以及模板(.ejs文件)生成html</p><p>注意：浏览器只认识html/css/js，除非将其它语言，进一步转换成html/css/js</p><p><br></p><h2 id="三、浏览器内核和js引擎"><a href="#三、浏览器内核和js引擎" class="headerlink" title="三、浏览器内核和js引擎"></a>三、浏览器内核和js引擎</h2><h3 id="1-渲染引擎"><a href="#1-渲染引擎" class="headerlink" title="1.渲染引擎"></a>1.渲染引擎</h3><p>浏览器内核，即渲染引擎，endering Engine </p><p>渲染引擎，负责浏览器如何显示网页的内容以及页面的格式信息，不同的浏览器内核对网页编写语法的解释不同，因此同一网页在不同的内核的浏览器里的渲染（效果也可能不同。</p><h3 id="2-js引擎"><a href="#2-js引擎" class="headerlink" title="2.js引擎"></a>2.js引擎</h3><p>在早期浏览内核也是包含js引擎的，而现在js引擎单独提出来</p><p>JavaScript 引擎称作进程虚拟机，就是读取和编译 JavaScript 代码，目标就是在最短时间内编译出最优化的代码</p><p>经常看到的 Google v8 就是js引擎</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：我不是做前端开发的，因为搭博客的时候，实在是这个也不会那个也不懂，所以连蒙带猜，下面写的只是我个人的理解 ~&lt;/p&gt;
    
    </summary>
    
    
      <category term="计算机网络" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>yilia主题的相册搭建</title>
    <link href="http://yoursite.com/2019/01/07/yilia%E4%B8%BB%E9%A2%98%E7%9A%84%E7%9B%B8%E5%86%8C%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2019/01/07/yilia主题的相册搭建/</id>
    <published>2019-01-07T12:16:15.000Z</published>
    <updated>2019-01-07T12:18:13.973Z</updated>
    
    <content type="html"><![CDATA[<p>PS：之前觉得很麻烦，更重要的是不明白网页生成的原理，所以拖着，那天一直做到凌晨一点，总算做出了个大概，前端开发的知识不够啊 </p><p>参考：<a href="https://www.jianshu.com/p/a9f309aaa0e0" target="_blank" rel="noopener">https://www.jianshu.com/p/a9f309aaa0e0</a></p><a id="more"></a><p><br></p><h2 id="一、博客相册的搭建步骤"><a href="#一、博客相册的搭建步骤" class="headerlink" title="一、博客相册的搭建步骤"></a>一、博客相册的搭建步骤</h2><h3 id="1-GitHub上新建一个放图片的仓库"><a href="#1-GitHub上新建一个放图片的仓库" class="headerlink" title="1.GitHub上新建一个放图片的仓库"></a>1.GitHub上新建一个放图片的仓库</h3><p>注意图片的格式 “2019-01-01_xxx”</p><h3 id="2-博客目录下需要的文件"><a href="#2-博客目录下需要的文件" class="headerlink" title="2.博客目录下需要的文件"></a>2.博客目录下需要的文件</h3><p><a href="https://github.com/litten/BlogBackup" target="_blank" rel="noopener">https://github.com/litten/BlogBackup</a></p><p>(1) ejs、js、css、json(通过脚本生成) </p><p>关于这些文件到底有什么用，后面再写</p><p>(2) 更改 ins.js 中 render 函数里的minSrc、src路径对应托管到GitHub的相册地址，注意这里用的是 <strong>raw.githubusercontent.com</strong></p><p>raw.githubusercontent.com 返回存储在GitHub中的文件的原始内容，即没有任何 GitHub 界面</p><p>(3) 这里会漏掉文件 empty.png</p><p>之前觉得很奇怪明明路径什么都是对的，但是相册里的图片就是没有显示，后来发现 render 函数下面有这么一句，所以需要将这个 asserts 文件也 load 下来，放在source目录下 </p><p><img class="reward-img" data-type="' + type + '" data-src="' + minSrc + '" src="/assets/img/empty.png" itemprop="thumbnail" onload="lzld(this)"></p><h3 id="3-裁剪和生成json"><a href="#3-裁剪和生成json" class="headerlink" title="3.裁剪和生成json"></a>3.裁剪和生成json</h3><p><a href="https://github.com/lawlite19/Blog-Back-Up" target="_blank" rel="noopener">https://github.com/lawlite19/Blog-Back-Up</a></p><p>ImageProcess.py、tool.py 工具</p><ul><li><p>对需要上传的图片进行裁剪，生成缩略图</p></li><li><p>生成json文件，放在博客目录下</p></li></ul><p><br></p><h2 id="二、问题"><a href="#二、问题" class="headerlink" title="二、问题"></a>二、问题</h2><p>1.tools 是裁减了原图的，它将原图尺寸改为1：1之后，再做了resize 成为缩略图</p><p>2.如果图片名称中出现中文，报错 UnicodeDecodeError: ‘utf8’ codec can’t decode byte </p><p>我真的一遇到编码问题，头就开始晕，后面找个时间整理一下 ~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：之前觉得很麻烦，更重要的是不明白网页生成的原理，所以拖着，那天一直做到凌晨一点，总算做出了个大概，前端开发的知识不够啊 &lt;/p&gt;
&lt;p&gt;参考：&lt;a href=&quot;https://www.jianshu.com/p/a9f309aaa0e0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/a9f309aaa0e0&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>算法（二）——递归、迭代、回溯</title>
    <link href="http://yoursite.com/2019/01/06/%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E9%80%92%E5%BD%92%E3%80%81%E8%BF%AD%E4%BB%A3%E3%80%81%E5%9B%9E%E6%BA%AF/"/>
    <id>http://yoursite.com/2019/01/06/算法（二）——递归、迭代、回溯/</id>
    <published>2019-01-06T13:09:14.000Z</published>
    <updated>2019-01-06T13:17:41.533Z</updated>
    
    <content type="html"><![CDATA[<p>PS：感觉看样例代码有点明白了，有意思</p><p><a href="https://www.jianshu.com/p/48cf12b84526" target="_blank" rel="noopener">https://www.jianshu.com/p/48cf12b84526</a></p><a id="more"></a><h2 id="1-递归"><a href="#1-递归" class="headerlink" title="1.递归"></a>1.递归</h2><p>函数自己调用自己，将一个大型的复杂的问题转化为一个与原问题相似的规模较小的问题来解决</p><p>我觉得，递归=回溯(从要求的结果到递归终止条件)+迭代(从初值到要求的结果)</p><p><strong>从所需结果出发不断回溯前一运算，回溯过程中这些结果是未知的，直到回溯到初值令回溯终止，再层层递推回来得到当前要求的值</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">int funcA(int n)  </span><br><span class="line">&#123;  </span><br><span class="line">    if(n &gt; 1)  </span><br><span class="line">       return n+funcA(n-1);  </span><br><span class="line">    else   </span><br><span class="line">       return 1;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-迭代"><a href="#2-迭代" class="headerlink" title="2.迭代"></a>2.迭代</h2><p>根据前一次运算结果，进行下一次运算</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">int funcB(int n)  </span><br><span class="line">&#123;  </span><br><span class="line">    int i,s=0;  </span><br><span class="line">    for(i=1;i&lt;n;i++)  </span><br><span class="line">       s+=i;  </span><br><span class="line">    return s;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-回溯"><a href="#3-回溯" class="headerlink" title="3.回溯"></a>3.回溯</h2><p>属于递归的一个过程</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：感觉看样例代码有点明白了，有意思&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.jianshu.com/p/48cf12b84526&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/48cf12b84526&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>算法（一）——时间复杂度</title>
    <link href="http://yoursite.com/2019/01/06/%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/"/>
    <id>http://yoursite.com/2019/01/06/算法（一）——时间复杂度/</id>
    <published>2019-01-06T13:08:10.000Z</published>
    <updated>2019-01-06T13:15:02.722Z</updated>
    
    <content type="html"><![CDATA[<p>PS：和段叔叔跑步的时候推归并排序的 O(nlogn)，我感觉要飞升了</p><p><a href="https://blog.csdn.net/so_geili/article/details/53444816" target="_blank" rel="noopener">https://blog.csdn.net/so_geili/article/details/53444816</a></p><a id="more"></a><p>算法：问题的解决方案，而不是问题的答案</p><p>时间复杂度的计算，可以分为递归和非递归算法</p><h2 id="1-时间复杂度"><a href="#1-时间复杂度" class="headerlink" title="1.时间复杂度"></a>1.时间复杂度</h2><p>描述一个算法，在问题规模不断增大时，对应的时间增长曲线</p><h2 id="2-时间复杂度大O的理解"><a href="#2-时间复杂度大O的理解" class="headerlink" title="2.时间复杂度大O的理解"></a>2.时间复杂度大O的理解</h2><p>(1)T(n)：对于给定输入规模n，算法需要执行的运算次数为n的函数T(n)</p><p>(2)f(n)：存在常数 c 和函数 f(n)，使得当 n &gt;= c 时 T(n) &lt;= f(n)，即T(n)的 <strong>增长率</strong>小于等于f(n)，也说f(n)是T(n)的 <strong>上界</strong></p><p>(3)T(n) = O(f(n))：用 f(n)的增长速度来度量 T(n) 的增长速度，即这个算法的时间复杂度是 O(f(n))</p><h2 id="3-时间复杂度计算规则"><a href="#3-时间复杂度计算规则" class="headerlink" title="3.时间复杂度计算规则"></a>3.时间复杂度计算规则</h2><p>(1)忽略常数项 T(n) = n + 29，时间复杂度为 O(n)</p><p>(2)忽略低次项 T(n) = n^3 + n^2 + 29，时间复杂度为 O(n^3)</p><p>(3)忽略与最高阶相乘的常数 T(n) = 3n^3，此时时间复杂度为 O(n^3)</p><h2 id="4-递归算法时间复杂度的计算"><a href="#4-递归算法时间复杂度的计算" class="headerlink" title="4.递归算法时间复杂度的计算"></a>4.递归算法时间复杂度的计算</h2><p>PS：这个东西我真的很抗拒，各种放缩技巧</p><p>(1)主项定理</p><p>(2)递归树</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：和段叔叔跑步的时候推归并排序的 O(nlogn)，我感觉要飞升了&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/so_geili/article/details/53444816&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/so_geili/article/details/53444816&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>caffe（六）——可视化网络工具Netscope</title>
    <link href="http://yoursite.com/2019/01/04/caffe%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7Netscope/"/>
    <id>http://yoursite.com/2019/01/04/caffe（六）——可视化网络工具Netscope/</id>
    <published>2019-01-04T01:55:46.000Z</published>
    <updated>2019-01-04T01:59:17.944Z</updated>
    
    <content type="html"><![CDATA[<p>无疑之中发现的，caffe在线可视化网络结构工具，有一些常用的网络，也可以将网络结构文件 net.prototxt 放到editor里</p><p><a href="http://ethereon.github.io/netscope/quickstart.html" target="_blank" rel="noopener">http://ethereon.github.io/netscope/quickstart.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;无疑之中发现的，caffe在线可视化网络结构工具，有一些常用的网络，也可以将网络结构文件 net.prototxt 放到editor里&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://ethereon.github.io/netscope/quickstart.html&quot; t
      
    
    </summary>
    
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>caffe（五）——windows下输出log日志和画出accuracy和loss曲线</title>
    <link href="http://yoursite.com/2019/01/03/caffe%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94windows%E4%B8%8B%E8%BE%93%E5%87%BAlog%E6%97%A5%E5%BF%97%E5%92%8C%E7%94%BB%E5%87%BAaccuracy%E5%92%8Closs%E6%9B%B2%E7%BA%BF/"/>
    <id>http://yoursite.com/2019/01/03/caffe（五）——windows下输出log日志和画出accuracy和loss曲线/</id>
    <published>2019-01-03T03:12:19.000Z</published>
    <updated>2019-01-03T03:25:12.881Z</updated>
    
    <content type="html"><![CDATA[<p>这里注意运行的python版本，我下的caffe对应的是python2.7</p><p>参考：<a href="https://www.zhihu.com/question/49521165/answer/127675889" target="_blank" rel="noopener">https://www.zhihu.com/question/49521165/answer/127675889</a></p><a id="more"></a><p><br></p><h2 id="1-生成日志"><a href="#1-生成日志" class="headerlink" title="1.生成日志"></a>1.生成日志</h2><p><code>caffe.exe train --solver=solver.prototxt --weights=model.caffemodel &gt;train.log 2&gt;&amp;1</code></p><h3 id="2-画图"><a href="#2-画图" class="headerlink" title="2.画图"></a>2.画图</h3><p>(1)将 caffe-master\tools\extra\ 文件夹下的 parse_log.py、extract_seconds.py 、plot_training_log.py 还有 train.log 放到同一文件夹下</p><p>(2)修改 “plot_training_log.py” 文件</p><ul><li><p>get_log_parsing_script()</p></li><li><p>create_field_index()</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">field_index = &#123;train_key:&#123;&apos;Iters&apos;:0, &apos;Seconds&apos;:1, train_key + &apos; learning rate&apos;:2,</span><br><span class="line">                              train_key + &apos; loss&apos;:3&#125;,</span><br><span class="line">                   test_key:&#123;&apos;Iters&apos;:0, &apos;Seconds&apos;:1, test_key + &apos; learning rate&apos;:2,</span><br><span class="line">                   test_key + &apos; accuracy&apos;:3, test_key + &apos; loss&apos;:4&#125;&#125;</span><br></pre></td></tr></table></figure><ul><li>load_data()</li></ul><p><br></p><p>(3)<code>python parse_log.py train.log ./</code> 在当前目录下生成 .train、.test 文件</p><p>注意：之前很奇怪，不生成.train .test文件，后来发现是solver.prototxt中 display 参数的问题，应该是通过这个隔几次记录 accuracy、loss 值</p><p>(4)画图</p><p><code>python plot_training_log.py 0 accuracy.png train.log</code></p><p><code>python plot_training_log.py ６ lr.png train.log</code></p><p><code>python plot_training_log.py 8 loss.png train.log</code></p><p>数字参数对应的输出图的类型与plot_training_log.py中修改的create_field_index()有关</p><p><br></p><p>Supported chart types<br>0: Test accuracy  vs. Iters<br>1: Test accuracy  vs. Seconds<br>2: Test learning rate  vs. Iters<br>3: Test learning rate  vs. Seconds<br>4: Test loss  vs. Iters<br>5: Test loss  vs. Seconds<br>6: Train learning rate  vs. Iters<br>7: Train learning rate  vs. Seconds<br>8: Train loss  vs. Iters<br>9: Train loss  vs. Seconds</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里注意运行的python版本，我下的caffe对应的是python2.7&lt;/p&gt;
&lt;p&gt;参考：&lt;a href=&quot;https://www.zhihu.com/question/49521165/answer/127675889&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.zhihu.com/question/49521165/answer/127675889&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>caffe（四）——finetune</title>
    <link href="http://yoursite.com/2019/01/03/caffe%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94finetune/"/>
    <id>http://yoursite.com/2019/01/03/caffe（四）——finetune/</id>
    <published>2019-01-03T03:10:03.000Z</published>
    <updated>2019-01-03T03:15:15.266Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>training from scratch：从新训练一个网络，所有参数都被随机初始化</p></li><li><p>fine-tuning：是对某个模型，在训练好的参数的基础上，进行微调</p></li></ul><a id="more"></a><p><br></p><h2 id="一、步骤"><a href="#一、步骤" class="headerlink" title="一、步骤"></a>一、步骤</h2><p>参考：<a href="https://www.cnblogs.com/louyihang-loves-baiyan/p/5038758.html" target="_blank" rel="noopener">https://www.cnblogs.com/louyihang-loves-baiyan/p/5038758.html</a></p><h3 id="1-训练数据和测试数据"><a href="#1-训练数据和测试数据" class="headerlink" title="1.训练数据和测试数据"></a>1.训练数据和测试数据</h3><p>(1)准备训练集、测试集的txt，图像路径之后一个空格跟着类别的ID，<strong>ID必须从0开始，要连续</strong></p><p><strong>数据集要打乱，不然很可能不收敛</strong></p><p>(2)使用convert_imageset工具</p><p>-gray:是否以灰度图的方式打开图片，默认为false</p><p>-shuffle:是否随机打乱图片顺序，默认为false</p><p>-backend:需要转换成的db文件格式，可选为leveldb或lmdb,默认为lmdb</p><p>-resize_width/resize_height:改变图片的大小，默认为0，不改变</p><p>-check_size:检查所有的数据是否有相同的尺寸，默认为false,不检查</p><p>-encoded:是否将原图片编码放入最终的数据中，默认为false</p><p>-encode_type:与前一个参数对应，将图片编码为哪一个格式：‘png’,’jpg’…</p><p><br></p><h3 id="2-数据集的均值文件"><a href="#2-数据集的均值文件" class="headerlink" title="2.数据集的均值文件"></a>2.数据集的均值文件</h3><p>使用compute_image_mean工具</p><h3 id="3-修改网络最后一层"><a href="#3-修改网络最后一层" class="headerlink" title="3.修改网络最后一层"></a>3.修改网络最后一层</h3><h3 id="4-调整Solver的配置参数"><a href="#4-调整Solver的配置参数" class="headerlink" title="4.调整Solver的配置参数"></a>4.调整Solver的配置参数</h3><p>通常学习速率、步长、迭代次数都要适当减少，最后一层的学习率加快</p><p>lr_mult是学习率系数，最终学习率为 base_lr*lr_mult，如果有两个 lr_mult, 则第一个表示权值的学习率，第二个表示偏置项的学习率，一般偏置项的学习率是权值学习率的两倍</p><h3 id="5-启动训练，加载pretrained模型的参数"><a href="#5-启动训练，加载pretrained模型的参数" class="headerlink" title="5.启动训练，加载pretrained模型的参数"></a>5.启动训练，加载pretrained模型的参数</h3><p>使用caffe train工具</p><p><br></p><h2 id="二、调参技巧"><a href="#二、调参技巧" class="headerlink" title="二、调参技巧"></a>二、调参技巧</h2><p>PS：这部分持续更新，现在真的不是很会</p><p><a href="https://blog.csdn.net/u010402786/article/details/70141261" target="_blank" rel="noopener">https://blog.csdn.net/u010402786/article/details/70141261</a></p><p>增大最后一层的lr_mult，将weight和bias的学习速率加快10倍，让非微调层学习更快</p><p>将其它层的lr_mult设置为0，来完全阻止最后一层以外的所有层的微调</p><p>出现不收敛的问题，将lr设的小一点，一般从0.01开始，如果出现loss=nan就不断往小调整</p><p>可以把accuracy和loss的曲线画出来，方便设定stepsize，一般在accuracy和loss都趋于平缓的时候就可以减小lr</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;p&gt;training from scratch：从新训练一个网络，所有参数都被随机初始化&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;fine-tuning：是对某个模型，在训练好的参数的基础上，进行微调&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>caffe（三）——模型和数据集</title>
    <link href="http://yoursite.com/2019/01/02/caffe%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>http://yoursite.com/2019/01/02/caffe（三）——模型和数据集/</id>
    <published>2019-01-02T08:56:59.000Z</published>
    <updated>2019-01-02T08:58:31.784Z</updated>
    
    <content type="html"><![CDATA[<p>数据集和模型的下载</p><p><a href="http://dl.caffe.berkeleyvision.org" target="_blank" rel="noopener">http://dl.caffe.berkeleyvision.org</a></p><a id="more"></a><p><br></p><h2 id="Model-Zoo"><a href="#Model-Zoo" class="headerlink" title="Model Zoo"></a>Model Zoo</h2><p>训练好的模型pretrained</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>不是完整的数据集，而是mean.binaryproto、synset之类的文件</p><h3 id="1-ILSVRC和ImageNet数据集"><a href="#1-ILSVRC和ImageNet数据集" class="headerlink" title="1.ILSVRC和ImageNet数据集"></a>1.ILSVRC和ImageNet数据集</h3><p>ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 即 ImageNet大规模视觉识别挑战赛，基于ImageNet图像数据库</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据集和模型的下载&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://dl.caffe.berkeleyvision.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://dl.caffe.berkeleyvision.org&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>图神经网络</title>
    <link href="http://yoursite.com/2019/01/02/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/01/02/图神经网络/</id>
    <published>2019-01-02T07:29:22.000Z</published>
    <updated>2019-01-02T07:31:59.523Z</updated>
    
    <content type="html"><![CDATA[<p>今天看新闻，有关阿里达摩院发布2019年十大科技趋势，有一条，感觉很有意思。<br>原文是这样说的，单纯的深度学习已经成熟，而结合了深度学习的图神经网络将端到端学习与归纳推理相结合，有望解决深度学习无法处理的关系推理、可解释性等一系列问题。<br>开始不是很明白图神经网络是什么意思，所以稍微看了一下，做了个归纳。</p><p>新闻链接：<a href="https://mp.weixin.qq.com/s/Zt1mI-TMdJYxMKwJ4nSRUQ?client=tim&amp;ADUIN=528036346&amp;ADSESSION=1546410913&amp;ADTAG=CLIENT.QQ.5597_.0&amp;ADPUBNO=26878" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Zt1mI-TMdJYxMKwJ4nSRUQ?client=tim&amp;ADUIN=528036346&amp;ADSESSION=1546410913&amp;ADTAG=CLIENT.QQ.5597_.0&amp;ADPUBNO=26878</a></p><a id="more"></a><p><br></p><p>下面是关于图卷积神经网络的理解，参考的是下面这篇知乎:<br><a href="https://zhuanlan.zhihu.com/p/37091549" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37091549</a></p><h2 id="1-图"><a href="#1-图" class="headerlink" title="1.图"></a>1.图</h2><p>卷积神经网络研究的对象还是限制在Euclidean domains的数据，比如图片是二维矩阵，比如语音是规则的一维序列。<br>但现实生活中有很多数据并不具备规则的空间结构，称为Non Euclidean data。</p><p>图为一种数据结构，图(graph)由顶点(vertex)和边(edge)组成，边具有权重(weights)</p><p>图有两个基本的特性：</p><ul><li><p>一是每个节点都有自己的特征信息</p></li><li><p>二是图谱中的每个节点还具有结构信息，即与其它节点的连接</p></li></ul><p><br></p><h2 id="2-图卷计算法"><a href="#2-图卷计算法" class="headerlink" title="2.图卷计算法"></a>2.图卷计算法</h2><p>PS：这部分真心没有明白怎么实现卷积，我只是对图结构有了个概念</p><ul><li>发射（send）每一个节点将自身的特征信息经过变换后发送给邻居节点</li><li>接收（receive）每个节点将邻居节点的特征信息聚集起来</li><li>变换（transform）把前面的信息聚集之后做非线性变换</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天看新闻，有关阿里达摩院发布2019年十大科技趋势，有一条，感觉很有意思。&lt;br&gt;原文是这样说的，单纯的深度学习已经成熟，而结合了深度学习的图神经网络将端到端学习与归纳推理相结合，有望解决深度学习无法处理的关系推理、可解释性等一系列问题。&lt;br&gt;开始不是很明白图神经网络是什么意思，所以稍微看了一下，做了个归纳。&lt;/p&gt;
&lt;p&gt;新闻链接：&lt;a href=&quot;https://mp.weixin.qq.com/s/Zt1mI-TMdJYxMKwJ4nSRUQ?client=tim&amp;amp;ADUIN=528036346&amp;amp;ADSESSION=1546410913&amp;amp;ADTAG=CLIENT.QQ.5597_.0&amp;amp;ADPUBNO=26878&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.weixin.qq.com/s/Zt1mI-TMdJYxMKwJ4nSRUQ?client=tim&amp;amp;ADUIN=528036346&amp;amp;ADSESSION=1546410913&amp;amp;ADTAG=CLIENT.QQ.5597_.0&amp;amp;ADPUBNO=26878&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Advanced Technology" scheme="http://yoursite.com/tags/Advanced-Technology/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>caffe（二）——训练和测试MNIST</title>
    <link href="http://yoursite.com/2018/12/27/caffe%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95MNIST/"/>
    <id>http://yoursite.com/2018/12/27/caffe（二）——训练和测试MNIST/</id>
    <published>2018-12-27T03:08:12.000Z</published>
    <updated>2018-12-28T13:30:54.440Z</updated>
    
    <content type="html"><![CDATA[<p>下面以MNIST为例，通过caffe来训练、测试、单张图片的分类</p><p><a href="https://blog.csdn.net/jieleiping/article/details/53134174" target="_blank" rel="noopener">https://blog.csdn.net/jieleiping/article/details/53134174</a></p><font color="#f58220">可执行文件的目录：caffe-master\Build\x64\Release</font><font color="#f58220">可执行文件对应的cpp所在目录：caffe-master\tools</font>   <a id="more"></a><p><br></p><h2 id="一、样本格式"><a href="#一、样本格式" class="headerlink" title="一、样本格式"></a>一、样本格式</h2><p><a href="https://blog.csdn.net/hjimce/article/details/49248231" target="_blank" rel="noopener">https://blog.csdn.net/hjimce/article/details/49248231</a></p><p>caffe支持的训练数据格式 lmdb、h5py、levelIDB </p><ul><li>lmdb数据格式常用于单标签数据，分类问题</li><li>h5py数据格式常用于多标签数据，回归问题</li></ul><p><br></p><h2 id="二、-sh和-bat"><a href="#二、-sh和-bat" class="headerlink" title="二、.sh和.bat"></a>二、.sh和.bat</h2><p>.sh和.bat都是批处理文件，一个是Linux下的一个是Windows下的</p><p>注意：路径问题</p><p><br></p><h2 id="三、训练"><a href="#三、训练" class="headerlink" title="三、训练"></a>三、训练</h2><h3 id="1-数据集格式的转换"><a href="#1-数据集格式的转换" class="headerlink" title="(1)数据集格式的转换"></a>(1)数据集格式的转换</h3><p>新建 convert_mnist.bat，调用 convert_mnist.exe 将MNIST数据集转换为lmdb格式</p><h3 id="2-计算图片数据的均值"><a href="#2-计算图片数据的均值" class="headerlink" title="(2)计算图片数据的均值"></a>(2)计算图片数据的均值</h3><p>新建 compute_img_mean.bat，调用 compute_image_mean.exe 生成均值文件 mean.binaryproto</p><p>图片减去均值再训练，会提高训练速度和精度，其实不是很明白这个均值的作用，师兄觉得是归一化</p><p>将其加再 mean_file 位置<br><a href="https://www.cnblogs.com/denny402/p/5083300.html" target="_blank" rel="noopener">https://www.cnblogs.com/denny402/p/5083300.html</a></p><h3 id="3-训练数据集"><a href="#3-训练数据集" class="headerlink" title="(3)训练数据集"></a>(3)训练数据集</h3><p>新建 train_mnist.bat，调用 caffe.exe train 训练，使用的是 net_solver.prototxt</p><p>关于 caffe.exe</p><p>可以输入的4个参数</p><pre><code>train           train or finetune a modeltest            score a modeldevice_query    show GPU diagnostic informationtime            benchmark model execution time</code></pre><p><br></p><h3 id="四、测试"><a href="#四、测试" class="headerlink" title="四、测试"></a>四、测试</h3><p>test_mnist.bat，调用 caffe.exe test 训练数据集，使用的是模型文件 net.prototxt 和权重文件 net_iter.caffemodel</p><p>PS：刚开始不明白这几个文件的作用，报了下面这个错，就是因为把 net.prototxt 和 net_solver.prototxt 的使用混淆了</p><p>message type caffe.netparameter has no field named net</p><p>解决方法：<a href="https://github.com/alexgkendall/caffe-segnet/issues/16" target="_blank" rel="noopener">https://github.com/alexgkendall/caffe-segnet/issues/16</a></p><p><font color="#f58220">注意：</font>  </p><ul><li>net.prototxt                      对应网络的结构  </li><li>net_solver.prototxt               如何训练网络</li></ul><p>注意prototxt文件中的路径与.bat文件放置的位置有关</p><p><br></p><h3 id="五、对单张图片做分类"><a href="#五、对单张图片做分类" class="headerlink" title="五、对单张图片做分类"></a>五、对单张图片做分类</h3><h4 id="1-标签文件"><a href="#1-标签文件" class="headerlink" title="(1)标签文件"></a>(1)标签文件</h4><p>新建标签文件 synset_words.txt，与分类结果相对应</p><h4 id="2-分类"><a href="#2-分类" class="headerlink" title="(2)分类"></a>(2)分类</h4><p>需要用到5个文件<br>net.prototxt<br>net_iter.caffemodel<br>mean.binaryproto<br>synset_words.txt<br>picture.bmp</p><h5 id="使用可执行文件"><a href="#使用可执行文件" class="headerlink" title="使用可执行文件"></a><1>使用可执行文件</1></h5><p>新建 classify_img.bat，调用 classification.exe 对自己的图片做分类</p><h5 id="调用C-接口"><a href="#调用C-接口" class="headerlink" title="调用C++接口"></a><2>调用C++接口</2></h5><p>调用 caffe-master\examples\classification 接口，可能会出现下面的错误</p><h5 id="1）"><a href="#1）" class="headerlink" title="1）"></a>1）</h5><p>error C2220: 警告被视为错误 - 没有生成“object”文件</p><p>将“警告视为错误”的选项改为“否”</p><p><a href="https://blog.csdn.net/bagboy_taobao_com/article/details/5613625" target="_blank" rel="noopener">https://blog.csdn.net/bagboy_taobao_com/article/details/5613625</a></p><h5 id="2）"><a href="#2）" class="headerlink" title="2）"></a>2）</h5><p>F0519 14:54:12.494139 14504 layer_factory.hpp:77] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: Input (known types: Input )</p><p>网上的解决方法是，将需要用到的layer添加到头文件中<br>但是我觉得很奇怪，官方的解决方案里其它项目是通过引用的方式，与libcaffe链接，而我现在通过静态链接的方式链接，就会报错？？？</p><p><a href="https://www.cnblogs.com/love6tao/p/5847480.html" target="_blank" rel="noopener">https://www.cnblogs.com/love6tao/p/5847480.html</a></p><p>如果静态库里的某些方法没有任何地方调用，最终这些没有被调用到的方法或变量将会被丢弃掉，不会被链接到目标程序中，这样做大大减小生成二进制文件的体积</p><p>在微软的编译器中有的对于那些没有用到的变量和函数是不生效的，caffe中的这些层模板，其实都已经注册了，只不过，没有引用没办法生效</p><p>强制链接静态库的所有符号</p><p><a href="http://www.cnblogs.com/coderzh/p/LinkAllSymbols.html" target="_blank" rel="noopener">http://www.cnblogs.com/coderzh/p/LinkAllSymbols.html</a><br><a href="https://blog.csdn.net/LG1259156776/article/details/52542386" target="_blank" rel="noopener">https://blog.csdn.net/LG1259156776/article/details/52542386</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;下面以MNIST为例，通过caffe来训练、测试、单张图片的分类&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/jieleiping/article/details/53134174&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/jieleiping/article/details/53134174&lt;/a&gt;&lt;/p&gt;
&lt;font color=&quot;#f58220&quot;&gt;可执行文件的目录：caffe-master\Build\x64\Release&lt;/font&gt;

&lt;font color=&quot;#f58220&quot;&gt;可执行文件对应的cpp所在目录：caffe-master\tools&lt;/font&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>caffe（一）——windows下环境配置</title>
    <link href="http://yoursite.com/2018/12/27/caffe%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94windows%E4%B8%8B%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2018/12/27/caffe（一）——windows下环境配置/</id>
    <published>2018-12-27T03:07:05.000Z</published>
    <updated>2019-01-10T09:01:00.718Z</updated>
    
    <content type="html"><![CDATA[<p>PS：心力憔悴，是不是windows下做开发都这么不友好，师兄说Linux下配caffe只要几行命令</p><p>PS：从12.26-12.28，总算林林总总的</p><p>贾扬清的访谈，里面提到了caffe名字的来源，哈哈哈，有意思 ：</p><p><a href="http://www.10tiao.com/html/579/201604/2685478534/1.html" target="_blank" rel="noopener">http://www.10tiao.com/html/579/201604/2685478534/1.html</a></p><a id="more"></a><h1 id="windows下caffe配置"><a href="#windows下caffe配置" class="headerlink" title="windows下caffe配置"></a>windows下caffe配置</h1><h2 id="一、libcaffe的编译"><a href="#一、libcaffe的编译" class="headerlink" title="一、libcaffe的编译"></a>一、libcaffe的编译</h2><p>libcaffe的编译，是为了生成caffe相关的库文件、可执行文件，可以在 caffe-master\Build\x64\Release 中查看</p><p>参考：<a href="https://blog.csdn.net/what_lei/article/details/77857117" target="_blank" rel="noopener">https://blog.csdn.net/what_lei/article/details/77857117</a></p><h3 id="1-修改属性文件"><a href="#1-修改属性文件" class="headerlink" title="1.修改属性文件"></a>1.修改属性文件</h3><p>将 caffe-master\windows 下的 CommonSettings.props.example 重命名为 CommonSettings.props，并且修改相关内容</p><p>本来想支持python接口，但是提供的支持VS2013版本的caffe只支持python2.7，我之前只装了python3.5，就懒得再装一遍python2.7，本来也只想用C++的API</p><p><br></p><h3 id="2-NuGet"><a href="#2-NuGet" class="headerlink" title="2.NuGet"></a>2.NuGet</h3><p>NuGet是.NET平台的包管理工具，类似于python的pip(.NET是个平台，像Java虚拟机一样的一个平台，为支持的语言提供运行时的环境和开发环境)</p><p>NuGet可以理解为将Package从工程中 <strong>分离</strong>，帮助管理Package，真的很好用！</p><p>NuGet会自动下载第三方库，在 caffe-master 的平级目录下生成 NugetPackages，后面再说这个插件，真的是强大啊</p><p>caffe依赖库：<a href="https://blog.csdn.net/hqh45/article/details/51675104" target="_blank" rel="noopener">https://blog.csdn.net/hqh45/article/details/51675104</a></p><p><br></p><h3 id="3-无法打开输入文件“libcaffe-lib”-解决方法"><a href="#3-无法打开输入文件“libcaffe-lib”-解决方法" class="headerlink" title="3.无法打开输入文件“libcaffe.lib” 解决方法"></a>3.无法打开输入文件“libcaffe.lib” 解决方法</h3><p><a href="https://blog.csdn.net/liyunlong111/article/details/70435143" target="_blank" rel="noopener">https://blog.csdn.net/liyunlong111/article/details/70435143</a></p><p><br></p><h2 id="二、caffe环境配置"><a href="#二、caffe环境配置" class="headerlink" title="二、caffe环境配置"></a>二、caffe环境配置</h2><p>自己新建一个解决方案，然后调用 libcaffe.lib，而不是在Caffe.sln解决方案里新建一个项目使用 开始以为新建项目的配置属性只要和 CommonSettings.props 一样就可以了，但是很奇怪的是，在这个属性文件里面没有关于 NugetPackages 第三方包相关路径的引用，这个地方卡了很久，后来看到 Caffe 的解决方案的每个项目下面都有 packages.config，这部分是和 NuGet </p><h3 id="方法一：CommonSettings-props-添加-NugetPackages-中的第三方库和与caffe相关的库"><a href="#方法一：CommonSettings-props-添加-NugetPackages-中的第三方库和与caffe相关的库" class="headerlink" title="方法一：CommonSettings.props 添加 NugetPackages 中的第三方库和与caffe相关的库"></a>方法一：CommonSettings.props 添加 NugetPackages 中的第三方库和与caffe相关的库</h3><p>算是最简单粗暴的方法，直接添加需要的头文件和库，我是参考了下面这个博主的，如果真的自己一个个去找，头文件和库文件，我怕是要疯了吧</p><p><a href="http://abumaster.com/2017/04/18/Caffe%E7%9A%84C-%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8/" target="_blank" rel="noopener">http://abumaster.com/2017/04/18/Caffe%E7%9A%84C-%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8/</a></p><h3 id="方法二：NuGet添加本地已下载的包，将其安装到现有的解决方案里"><a href="#方法二：NuGet添加本地已下载的包，将其安装到现有的解决方案里" class="headerlink" title="方法二：NuGet添加本地已下载的包，将其安装到现有的解决方案里"></a>方法二：NuGet添加本地已下载的包，将其安装到现有的解决方案里</h3><h4 id="1-引入第三方包"><a href="#1-引入第三方包" class="headerlink" title="1.引入第三方包"></a>1.引入第三方包</h4><p>工具——库程序包管理器——管理解决方案的NuGet程序包——设置(添加之前下载的 NugetPackages的路径)——安装所有的包</p><p>包引入后，会生成 packages.config 文件</p><h4 id="2-引入caffe相关的库"><a href="#2-引入caffe相关的库" class="headerlink" title="2.引入caffe相关的库"></a>2.引入caffe相关的库</h4><p>将之前windows下的 CommonSettings.props </p><p>(1)头文件目录<br>caffe-master\include<br>caffe-master\include\caffe\proto</p><p>(2)附加库目录<br>caffe-master\Build\x64\Release</p><p>(3)附加依赖项<br>libcaffe.lib</p><h4 id="3-解决了两天的报错"><a href="#3-解决了两天的报错" class="headerlink" title="3.解决了两天的报错"></a>3.解决了两天的报错</h4><p>这个地方卡了一天多，刚开始想不明白有关的报错是，没有将hdf5相关的库链接，但是我明明用NuGet将 NugetPackages\hdf5-v120-complete.1.8.15.2 安装进了解决方案    </p><p>解决方法一：又是简单粗暴的方法，将 hdf5-v120-complete.1.8.15.2 的头文件和库通过配置文件里引入，但是想不明白明明用NuGet安装了会报错，后面发现是NuGet的问题</p><p>解决方法二：这个真的是学到了，以前都不知道有这种操作，感叹一下NuGet功能强大</p><p>项目右键——生成依赖项——生成自定义——查找现有生成自定义项文件</p><p>然后发现没有hdf5这个选项，将自定义项文件 在NugetPackages\hdf5-v120-complete.1.8.15.2\build\native\hdf5-v120.targets 导入，激动人心的时刻到了，哈哈哈，再也不需要自己手动include，学到了学到了</p><p>而且还有一点就是，相应的dll在生成的时候，会自动导入可执行文件相应的目录下，后面发现，这也是targets里配置的</p><p>具体过程参考下面这篇博文：<a href="https://blog.csdn.net/junparadox/article/details/51086374" target="_blank" rel="noopener">https://blog.csdn.net/junparadox/article/details/51086374</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：心力憔悴，是不是windows下做开发都这么不友好，师兄说Linux下配caffe只要几行命令&lt;/p&gt;
&lt;p&gt;PS：从12.26-12.28，总算林林总总的&lt;/p&gt;
&lt;p&gt;贾扬清的访谈，里面提到了caffe名字的来源，哈哈哈，有意思 ：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.10tiao.com/html/579/201604/2685478534/1.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.10tiao.com/html/579/201604/2685478534/1.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>Christmas Gift——pygame、pyinstaller</title>
    <link href="http://yoursite.com/2018/12/25/Christmas-Gift%E2%80%94%E2%80%94pygame%E3%80%81pyinstaller/"/>
    <id>http://yoursite.com/2018/12/25/Christmas-Gift——pygame、pyinstaller/</id>
    <published>2018-12-25T11:46:52.000Z</published>
    <updated>2019-01-05T06:46:50.761Z</updated>
    
    <content type="html"><![CDATA[<p>今天是圣诞节，然后玩了一个下午的pyinstaller，超累的，感觉还是自己太菜了，很多东西都不知道啊，还得多学习啊 ~</p><a id="more"></a><h2 id="一、pygame"><a href="#一、pygame" class="headerlink" title="一、pygame"></a>一、pygame</h2><p><a href="http://www.icode9.com/content-1-22888.html" target="_blank" rel="noopener">http://www.icode9.com/content-1-22888.html</a></p><p>PS：之前在公众号上看到使用pygame这个模块，实现下雪的效果，让我想到了飘雪花的水晶球<br>不知道圣诞节送段叔叔什么礼物，就觉得这个挺好看的，所以折腾了一下午，哈哈，虽然是个很简单的脚本，但是转换成可执行文件，真的有点费事，我还以为做不出来，只不过总算可以运行了，希望段叔叔能喜欢吧 ~</p><p>pygame是一个利用SDL库的写就的游戏库</p><p><br></p><h2 id="二、pyinstaller"><a href="#二、pyinstaller" class="headerlink" title="二、pyinstaller"></a>二、pyinstaller</h2><p>PS：每次卡住的不是代码，而是各种环境、配置，一个下午就一直卡着，一直报错啊，很奔溃啊<br>本来就是个学渣，又不想问段叔叔怎么解决各种warning，总算生成可执行文件</p><p>pyinstaller将.py文件打包为.exe </p><p>下面开始我的各种问题:</p><p>1.Warning一堆API的dll not found</p><p>然后就用了最简单粗暴的方法</p><p>将not found的dll从C:\Windows\System32\downlevel 复制到C:\Windows\System32或python的安装目录下</p><p>2.ImportError: No module named ‘setuptools._vendor’</p><p>开始用了各种方法，都不行，然后看了下面这篇博文，更新了一下setuptools的版本</p><p><a href="https://blog.csdn.net/qq_39360343/article/details/82772916" target="_blank" rel="noopener">https://blog.csdn.net/qq_39360343/article/details/82772916</a></p><p>3.还未解决的就是设置可执行文件的图标</p><p>不知道为什么在 <code>-i</code>后面加图标的路径，dist文件下就没有可执行文件生成，应该不是图片文件的大小的问题吧</p><p>最后放一下，这个超简单的圣诞节动态壁纸吧  </p><p><img src="https://github.com/Sophia0130/Blog-Album/blob/master/blog-video/video.gif?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天是圣诞节，然后玩了一个下午的pyinstaller，超累的，感觉还是自己太菜了，很多东西都不知道啊，还得多学习啊 ~&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习（五）——KNN</title>
    <link href="http://yoursite.com/2018/12/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94KNN/"/>
    <id>http://yoursite.com/2018/12/25/机器学习（五）——KNN/</id>
    <published>2018-12-25T06:32:22.000Z</published>
    <updated>2018-12-25T06:34:16.853Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>1.以所有已知类别的实例作为参照，选取参数K</p><p>2.计算已知实例与未知实例的距离</p><p>3.选择最近的K个实例</p><p>4.根据投票原则，将未知实例归为K个最近邻中多数的样本</p><a id="more"></a><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>1.算法复杂度高，未知实例需要与所有已知实例比较</p><p>2.样本分布不均匀时，未知实例容易被归为实例数量过大的起主导作用的那一类</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;算法流程&quot;&gt;&lt;a href=&quot;#算法流程&quot; class=&quot;headerlink&quot; title=&quot;算法流程&quot;&gt;&lt;/a&gt;算法流程&lt;/h2&gt;&lt;p&gt;1.以所有已知类别的实例作为参照，选取参数K&lt;/p&gt;
&lt;p&gt;2.计算已知实例与未知实例的距离&lt;/p&gt;
&lt;p&gt;3.选择最近的K个实例&lt;/p&gt;
&lt;p&gt;4.根据投票原则，将未知实例归为K个最近邻中多数的样本&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习（四）——XGBoost</title>
    <link href="http://yoursite.com/2018/12/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94XGBoost/"/>
    <id>http://yoursite.com/2018/12/25/机器学习（四）——XGBoost/</id>
    <published>2018-12-25T01:09:43.000Z</published>
    <updated>2018-12-25T01:13:36.434Z</updated>
    
    <content type="html"><![CDATA[<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>1.多个决策树的 <strong>集成</strong>               </p><p>2.一个树一个树向里增加，同时每加一个树，需要保证效果是提升的<br>当要增加树的时候，会将前面的树当成一个整体，构造的树，使得之前当成整体的目标函数减小</p><a id="more"></a><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>两者具体的区别：<a href="https://blog.csdn.net/u013709270/article/details/72553282" target="_blank" rel="noopener">https://blog.csdn.net/u013709270/article/details/72553282</a></p><p>Bagging <strong>并行</strong>生成<br>Boosting 将弱分类器 <strong>提升</strong>为强分类器，<strong>串行</strong>生成</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;XGBoost&quot;&gt;&lt;a href=&quot;#XGBoost&quot; class=&quot;headerlink&quot; title=&quot;XGBoost&quot;&gt;&lt;/a&gt;XGBoost&lt;/h2&gt;&lt;p&gt;1.多个决策树的 &lt;strong&gt;集成&lt;/strong&gt;               &lt;/p&gt;
&lt;p&gt;2.一个树一个树向里增加，同时每加一个树，需要保证效果是提升的&lt;br&gt;当要增加树的时候，会将前面的树当成一个整体，构造的树，使得之前当成整体的目标函数减小&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习（三）——PCA</title>
    <link href="http://yoursite.com/2018/12/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94PCA/"/>
    <id>http://yoursite.com/2018/12/24/机器学习（三）——PCA/</id>
    <published>2018-12-24T09:28:20.000Z</published>
    <updated>2019-01-05T05:42:36.488Z</updated>
    
    <content type="html"><![CDATA[<p>PS：关于PCA数学推导，协方差矩阵、特征向量计算的那部分我就不放</p><a id="more"></a><h2 id="一、降维"><a href="#一、降维" class="headerlink" title="一、降维"></a>一、降维</h2><p>projection 高维到低维的 <strong>投影</strong><br>highly redundant features 可以减少多余的特征</p><p><br></p><h2 id="二、为什么需要降维"><a href="#二、为什么需要降维" class="headerlink" title="二、为什么需要降维"></a>二、为什么需要降维</h2><p>1.使用较少的计算机内存或磁盘空间（选择k，使得原数据的方差尽可能保留）<br>2.加快学习算法（选择k，使得原数据的方差尽可能保留）<br>3.可视化数据（选择k=2、3，将数据放在二维平面或三维空间展示）</p><p><br></p><h2 id="三、PCA"><a href="#三、PCA" class="headerlink" title="三、PCA"></a>三、PCA</h2><h3 id="1-PCA的理解"><a href="#1-PCA的理解" class="headerlink" title="1.PCA的理解"></a>1.PCA的理解</h3><p><strong>PCA就是找到一个低维的平面，把所有的数据都投射到该平面上时，使得投射误差 projection error 尽可能的小</strong></p><p>n维降到k维，就是在n维空间中，找一个低维空间，<strong>该低维空间可以用k个n维的向量来表示</strong>，把所有的数据都投射到该空间中，使得投射的误差 projection error 尽可能的小</p><h3 id="2-PCA需要计算"><a href="#2-PCA需要计算" class="headerlink" title="2.PCA需要计算"></a>2.PCA需要计算</h3><p>(1)用于降维的，k个n维的方向向量 U_redeuce<br>(2)数据在低维空间投影后的特征 Z</p><h3 id="3-PCA实现步骤"><a href="#3-PCA实现步骤" class="headerlink" title="3.PCA实现步骤"></a>3.PCA实现步骤</h3><p><img src="https://github.com/Sophia0130/Blog-Album/blob/master/blog-img-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PCA%E6%AD%A5%E9%AA%A4%E5%AE%9E%E7%8E%B0.jpg?raw=true" alt=""></p><h3 id="4-PCA数学推导"><a href="#4-PCA数学推导" class="headerlink" title="4.PCA数学推导"></a>4.PCA数学推导</h3><h3 id="5-PCA与线性回归的区别"><a href="#5-PCA与线性回归的区别" class="headerlink" title="5.PCA与线性回归的区别"></a>5.PCA与线性回归的区别</h3><p>[右图] PCA最小化的是 <strong>投射误差</strong>（projection Error），不做预测<br>[左图] 线性回归最小化的是 <strong>预测误差</strong>，即预测结果与实际标签的距离</p><p><img src="https://images2015.cnblogs.com/blog/788978/201605/788978-20160524003006303-547302950.png" alt=""></p><p><br></p><h2 id="四、主成分个数k的选择"><a href="#四、主成分个数k的选择" class="headerlink" title="四、主成分个数k的选择"></a>四、主成分个数k的选择</h2><p>PCA的目的：是减少投射的平均均方误差</p><p>PCA的ｋ选取是：投射的平均均方误差与训练集方差的比例尽可能小的情况下，选择尽可能小的k<br>（这个比例小于1%，意味着原本数据的 <strong>方差</strong>有99%都保留下来了）</p><p>选取步骤：<br>1.令k=1，然后进行主要成分分析，获得前k个特征向量 U_reduce 和计算原数据 X 投影后的数据 Z<br>2.计算投射的平均均方误差与训练集方差的比例，是否小于1%<br>（如果不满足，令k=2，如此类推，直到找到可以使得比例小于1%的最小k值）</p><p><img src="https://images2015.cnblogs.com/blog/788978/201605/788978-20160524003014225-1738231865.png" alt=""></p><p><br></p><h2 id="五、PCA不适合设防止过拟合"><a href="#五、PCA不适合设防止过拟合" class="headerlink" title="五、PCA不适合设防止过拟合"></a>五、PCA不适合设防止过拟合</h2><p>PCA并不是一个好的方法用来防止过拟合，防止过拟合，还是应该用正则化</p><p>原因：<br>1.PCA是无监督的，会 <strong>丢失与Y相关的信息</strong><br>PCA是 <strong>无监督</strong>的，只关注输入数据X之间的相关性，降低数据Ｘ的维度，而不考虑标签Y，会让与Y有关的信息被丢失<br>对于监督学习，则寻找的是X与Y之间的联系</p><p>2.PCA会 <strong>丢失方差小的特征</strong><br><strong>PCA的假设是方差越大信息量越多</strong>，但是方差小的特征并不代表表对于标签没有意义</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：关于PCA数学推导，协方差矩阵、特征向量计算的那部分我就不放&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
