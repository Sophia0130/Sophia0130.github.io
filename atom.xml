<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>绿小蕤</title>
  <icon>https://www.gravatar.com/avatar/e4d7a8bd1cb84fb3b4123916b4ea2f6b</icon>
  <subtitle>好逸恶劳,贪生怕死</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-07-26T12:39:10.125Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>绿小蕤</name>
    <email>528036346@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TensorFlow（二）——单层神经网络实现MNIST的分类</title>
    <link href="http://yoursite.com/2018/07/26/TensorFlow%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0MNIST%E7%9A%84%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/07/26/TensorFlow（二）——单层神经网络实现MNIST的分类/</id>
    <published>2018-07-26T12:27:03.000Z</published>
    <updated>2018-07-26T12:39:10.125Z</updated>
    
    <content type="html"><![CDATA[<p>TensorFlow——实现单层神经网络实现手写数字的分类</p><p>发现了一篇博文，思路很清晰，把MNIST从多方面展开来写，不仅仅是实现分类网络的训练，很有收获。特别是代码的风格是用类写的，就很清楚，代码可以找博主的GitHub的最新版</p><p><a href="https://geektutu.com/post/tensorflow-mnist-simplest.html" target="_blank" rel="noopener">https://geektutu.com/post/tensorflow-mnist-simplest.html</a></p><a id="more"></a><p><br></p><h2 id="一、MNIST-手写数字数据库"><a href="#一、MNIST-手写数字数据库" class="headerlink" title="一、MNIST 手写数字数据库"></a>一、MNIST 手写数字数据库</h2><h3 id="1-数据集"><a href="#1-数据集" class="headerlink" title="1.数据集"></a>1.数据集</h3><p>分为60000行的训练数据集（<code>mnist.train</code>）和10000行的测试数据集（<code>mnist.test</code>）</p><p>每张图片的像素 28x28 被展开为长度 784 的行向量</p><p>训练数据：</p><p>图片：<code>mnist.train.images</code> 是一个形状为 <code>[60000, 784]</code> </p><p>图片的标签：<code>mnist.train.labels</code> 是一个 <code>[60000, 10]</code>    <strong>使用的是one-hot 独热码</strong></p><p>one-hot：使用N位状态寄存器来对N个状态进行编码，并且在任意时候，只有一位有效。最大优势在于状态比较时仅仅需要比较一个位，运算简单，但是占用资源多</p><p><br></p><h3 id="2-MNIST手写数字数据集的下载"><a href="#2-MNIST手写数字数据集的下载" class="headerlink" title="2.MNIST手写数字数据集的下载"></a>2.MNIST手写数字数据集的下载</h3><p>使用下面的代码加载失败</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.examples.tutorials.mnist.input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</span><br></pre></td></tr></table></figure><p><em>由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败</em></p><p>采取下面的办法解决  <a href="https://blog.csdn.net/Li_haiyu/article/details/78465806" target="_blank" rel="noopener">https://blog.csdn.net/Li_haiyu/article/details/78465806</a></p><p><br></p><h3 id="3-PCA-可视化-MNIST-手写数字识别数据集"><a href="#3-PCA-可视化-MNIST-手写数字识别数据集" class="headerlink" title="3.PCA 可视化 MNIST 手写数字识别数据集"></a>3.PCA 可视化 MNIST 手写数字识别数据集</h3><p><a href="http://projector.tensorflow.org/" target="_blank" rel="noopener">http://projector.tensorflow.org/</a></p><p>PCA 是一种常用的数据降维方法，可以将高维数据在二维或者三维可视化呈现</p><p><br></p><h3 id="4-显示MNIST的数据"><a href="#4-显示MNIST的数据" class="headerlink" title="4.显示MNIST的数据"></a>4.显示MNIST的数据</h3><p><a href="http://www.cnblogs.com/youmuchen/p/6713241.html" target="_blank" rel="noopener">http://www.cnblogs.com/youmuchen/p/6713241.html</a></p><p><br></p><p><br></p><h2 id="二、代码实现"><a href="#二、代码实现" class="headerlink" title="二、代码实现"></a>二、代码实现</h2><h3 id="1-代码流程"><a href="#1-代码流程" class="headerlink" title="1.代码流程"></a>1.代码流程</h3><ul><li>输入训练集和测试集</li><li>构造神经网络<ul><li>输入训练样本的ｘ[55000,784]、ｙ[55000,10] 一个样本为一行</li><li>设置神经网络参数 W[784,10]、b[10]</li><li>计算 y_prediction = x*W + b</li></ul></li><li>softmax计算10种类别各自的概率</li><li>交叉熵计算loss</li><li>训练网络</li><li>用训练集计算准确率</li></ul><p><br></p><h3 id="2-附加功能"><a href="#2-附加功能" class="headerlink" title="2.附加功能"></a>2.附加功能</h3><ul><li>可视化  tensorboard<ul><li>参数</li><li>loss、accuracy</li><li>图片</li></ul></li><li>参数的保存 Saver</li><li>Timeline</li></ul><p><br></p><h3 id="3-贴代码"><a href="#3-贴代码" class="headerlink" title="3.贴代码"></a>3.贴代码</h3><ul><li><p>train_or_test == 0 时训练网络得到参数</p><p>可以看到每隔50次训练的loss值</p></li></ul><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_train.jpg" alt=""></p><ul><li><p>train_or_test == 1 用得到的模型计算测试集精确度</p><p>测试集的精确度有0.88</p></li></ul><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_test.jpg" alt=""></p><p><br></p><p><br></p><h2 id="三、tensorboard"><a href="#三、tensorboard" class="headerlink" title="三、tensorboard"></a>三、tensorboard</h2><ul><li>网络结构</li></ul><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_graph.png" alt=""></p><p><br></p><ul><li>每隔50次训练的loss</li></ul><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_loss.png" alt=""></p><p><br></p><ul><li>每隔50次测试集的精确度</li></ul><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_accuracy.png" alt=""></p><p><br></p><p><br></p><h2 id="四、Timeline"><a href="#四、Timeline" class="headerlink" title="四、Timeline"></a>四、Timeline</h2><p>不要问我这个是啥意思，我也不知道，看不懂 ~</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/mnist_fc_timeline.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TensorFlow——实现单层神经网络实现手写数字的分类&lt;/p&gt;
&lt;p&gt;发现了一篇博文，思路很清晰，把MNIST从多方面展开来写，不仅仅是实现分类网络的训练，很有收获。特别是代码的风格是用类写的，就很清楚，代码可以找博主的GitHub的最新版&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://geektutu.com/post/tensorflow-mnist-simplest.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://geektutu.com/post/tensorflow-mnist-simplest.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow（一）——两层神经网络预测线性回归</title>
    <link href="http://yoursite.com/2018/07/26/TensorFlow%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E4%B8%A4%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%A2%84%E6%B5%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2018/07/26/TensorFlow（一）——两层神经网络预测线性回归/</id>
    <published>2018-07-26T12:25:12.000Z</published>
    <updated>2018-07-26T12:25:57.979Z</updated>
    
    <content type="html"><![CDATA[<p>PS：两层的神经网络训练线性回归，代码参考了莫烦的，GitHub上有，然后又往里面加了关于Timeline的部分</p><a id="more"></a><p><br></p><h2 id="一、代码"><a href="#一、代码" class="headerlink" title="一、代码"></a>一、代码</h2><p><img src="http://p8ge6t5tt.bkt.clouddn.com/nonlinerregression.jpg" alt=""></p><p><br></p><p><br></p><h2 id="二、tensorboard"><a href="#二、tensorboard" class="headerlink" title="二、tensorboard"></a>二、tensorboard</h2><p>PS ：真心不是很会看这个分析图，可以看看下面这篇博客</p><p><a href="https://blog.csdn.net/u010099080/article/details/77426577" target="_blank" rel="noopener">https://blog.csdn.net/u010099080/article/details/77426577</a></p><p><br></p><h3 id="SCALAR"><a href="#SCALAR" class="headerlink" title="SCALAR"></a>SCALAR</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/tf_scalar.jpg" alt=""></p><ol><li>图的左下角的3 个小图标，第一个：查看大图，第二个：是否对 y 轴对数化，第三个：是放大和复原</li><li>术语解释：</li></ol><p>Smoothing ： 指的是作图时曲线的平滑程度，使用类似指数平滑的处理方法，默认是 0.6。</p><p>STEP：横轴显示的是训练迭代次数</p><p>RELATIVE：训练的相对时间，相对于训练开始的时间</p><p>WALL：训练的绝对时间</p><p><br></p><p><br></p><h3 id="DISTRIBUTIONS"><a href="#DISTRIBUTIONS" class="headerlink" title="DISTRIBUTIONS"></a>DISTRIBUTIONS</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/tf_distribution.jpg" alt=""></p><p>从上到下的折线，随着横轴训练次数，纵轴参数的多分位数图</p><p>从上到下表示不同的分位数  [maximum, 93%, 84%, 69%, 50%, 31%, 16%, 7%, minimum]</p><p><br></p><p><br></p><h3 id="HISTOGRAMS"><a href="#HISTOGRAMS" class="headerlink" title="HISTOGRAMS"></a>HISTOGRAMS</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/tf_histogram.jpg" alt=""></p><ol><li>HISTOGRAMS 和 DISTRIBUTIONS 是对同一数据不同方式的展现。HISTOGRAMS是 频数分布直方图的堆叠，横轴表示权重值，纵轴表示训练步数。颜色越深表示时间越早，越浅表示越接近训练结束</li><li>两种模式：都是横轴为权重值，纵轴为频数</li></ol><p>OVERLAY  二维表示</p><p>OFFSET 三维表示</p><p><br></p><p><br></p><h3 id="GRAPH"><a href="#GRAPH" class="headerlink" title="GRAPH"></a>GRAPH</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/tf_graph.png" alt=""></p><ol><li><p>图像颜色两种模式：</p><p>基于结构的模式（Structure），相同的节点会有同样的颜色</p><p>基于硬件（Device），同一个硬件上的会有相同颜色</p></li><li><p>术语解释：</p><p>Run 创建了几个不同的Session（改变了超参数），对应的计算图</p><p>Session runs：一个session里，不同训练次数，对应的计算图</p></li></ol><p><br></p><p><br></p><h2 id="三、Timeline"><a href="#三、Timeline" class="headerlink" title="三、Timeline"></a>三、Timeline</h2><p>真心看不懂</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/tf_timeline.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：两层的神经网络训练线性回归，代码参考了莫烦的，GitHub上有，然后又往里面加了关于Timeline的部分&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（二）——最优化</title>
    <link href="http://yoursite.com/2018/07/26/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2018/07/26/cs231n笔记（二）——最优化/</id>
    <published>2018-07-26T12:23:52.000Z</published>
    <updated>2018-07-26T12:24:22.052Z</updated>
    
    <content type="html"><![CDATA[<p><strong>目录</strong></p><p>1.解决分类的三步骤</p><p>2.最优化</p><p>3.梯度计算</p><a id="more"></a><p><br></p><h2 id="一、分类问题三步骤"><a href="#一、分类问题三步骤" class="headerlink" title="一、分类问题三步骤"></a>一、分类问题三步骤</h2><p><strong>1. 评分函数（score function）</strong> ：将原始图像数据<strong>映射为各个类别的得分</strong>，得分高低代表图像属于该类别的可能性高低。</p><p>线性分类评分函数的线性映射：<img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%2Cb%29%3DWx_i%2Bb" alt="\displaystyle f(x_i,W,b)=Wx_i+b"></p><p><strong>2. 损失函数（loss function）</strong> ：量化分类标签的得分与真实标签之间一致性。也就是<strong>量化某个具体权重集W的质量</strong> 。</p><p><strong>3.最优化（Optimization）</strong> ：找到能够最小化损失函数值的W。</p><p><br></p><p><br></p><h2 id="二、最优化-Optimization"><a href="#二、最优化-Optimization" class="headerlink" title="二、最优化 Optimization"></a>二、最优化 Optimization</h2><h3 id="1-随机搜索"><a href="#1-随机搜索" class="headerlink" title="1.随机搜索"></a>1.随机搜索</h3><p>尝试了若干随机生成的权重矩阵W，其中某些的损失值较小，而另一些的损失值大些。我们可以把这次随机搜索中找到的最好的权重W取出，然后去跑测试集</p><p><br></p><h3 id="2-随机本地搜索"><a href="#2-随机本地搜索" class="headerlink" title="2.随机本地搜索"></a>2.随机本地搜索</h3><p>从一个随机<img src="http://www.zhihu.com/equation?tex=W" alt="W">开始，然后生成一个随机的扰动<img src="http://www.zhihu.com/equation?tex=%5Cdelta+W" alt="\delta W"> ，只有当<img src="http://www.zhihu.com/equation?tex=W%2B%5Cdelta+W" alt="W+\delta W">的损失值变低，才更新。</p><p><br></p><h3 id="3-梯度下降"><a href="#3-梯度下降" class="headerlink" title="3.梯度下降"></a>3.梯度下降</h3><p>前两个方法中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实<strong>不需要随机寻找方向</strong>，可以直接计算出最陡峭的方向，这个方向就是损失函数的梯度（gradient）。</p><p>方向导数：某个方向上的导数</p><p>梯度：是一个矢量，是方向导数中取到最大值的方向，梯度的值是方向导数的最大值。</p><p><br></p><p><br></p><h2 id="三、计算梯度"><a href="#三、计算梯度" class="headerlink" title="三、计算梯度"></a>三、计算梯度</h2><p>计算梯度有两种方法，一个是缓慢的近似方法（数值梯度法），但实现相对简单。另一个方法（分析梯度法）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。</p><p><br></p><h3 id="1-数值计算梯度法"><a href="#1-数值计算梯度法" class="headerlink" title="1.数值计算梯度法"></a>1.数值计算梯度法</h3><p>使用有限差值近似计算梯度   <img src="http://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7Bdf%28x%29%7D%7Bdx%7D%3D%5Clim_%7Bh%5Cto+0%7D%5Cfrac%7Bf%28x%2Bh%29-f%28x%29%7D%7Bh%7D" alt="\displaystyle\frac{df(x)}{dx}=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}"></p><p>对所有维度进行迭代，在每个维度上产生一个很小的变化h，计算函数在该维度上的偏导数。</p><p>注意在数学公式中，<strong>h</strong>的取值是趋近于0的，然而在实际中，用一个很小的数值（比如1e-5）。还有，实际中用中心差值公式（<strong>centered difference formula）</strong><img src="http://www.zhihu.com/equation?tex=%5Bf%28x%2Bh%29-f%28x-h%29%5D%2F2h" alt="[f(x+h)-f(x-h)]/2h">效果较好。</p><p><strong>缺点是效率低</strong>：计算数值梯度的复杂性和参数的量线性相关。如果有30730个参数，损失函数每走一步就需要计算30731次损失函数的梯度。现代神经网络很容易就有上千万的参数，显然这个策略不适合大规模数据。</p><p><br></p><h3 id="2-微分分析计算梯度"><a href="#2-微分分析计算梯度" class="headerlink" title="2.微分分析计算梯度"></a>2.微分分析计算梯度</h3><p>利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。</p><p><br></p><h3 id="3-梯度检查"><a href="#3-梯度检查" class="headerlink" title="3.梯度检查"></a>3.梯度检查</h3><p>由于微分分析计算梯度容易出错 ，实际操作时常常<strong>将分析梯度法的结果和数值梯度法的结果作比较</strong>，以此来<strong>检查其实现的正确性</strong>，这个步骤叫做<strong>梯度检查</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;目录&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1.解决分类的三步骤&lt;/p&gt;
&lt;p&gt;2.最优化&lt;/p&gt;
&lt;p&gt;3.梯度计算&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>进程和线程</title>
    <link href="http://yoursite.com/2018/07/26/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/07/26/进程和线程/</id>
    <published>2018-07-26T09:38:09.000Z</published>
    <updated>2018-07-26T09:41:15.879Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-进程和线程"><a href="#1-进程和线程" class="headerlink" title="1.进程和线程"></a>1.进程和线程</h3><ul><li><p>进程：运行中的应用程序称为进程，拥有系统资源（cpu、内存）</p></li><li><p>线程：进程中的一段代码，一个进程中可以有多段代码。本身不拥有资源（共享所在进程的资源）</p><p><br></p></li></ul><ul><li>多进程: 在操作系统中能同时运行多个任务(程序)</li><li>多线程: 在同一应用程序中有多个功能流同时执行</li></ul><p><br></p><h3 id="2-多线程并行和并发"><a href="#2-多线程并行和并发" class="headerlink" title="2.多线程并行和并发"></a>2.多线程并行和并发</h3><p>多线程并行 Parallelism：在不同CPU上（多核），同时执行多个线程</p><p>多线程并发 Concurrency ：在一个CPU上，做多线程之间的切换，如果CPU调度线程的时间足够快，就造成了多线程并发执行的假象</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-进程和线程&quot;&gt;&lt;a href=&quot;#1-进程和线程&quot; class=&quot;headerlink&quot; title=&quot;1.进程和线程&quot;&gt;&lt;/a&gt;1.进程和线程&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;进程：运行中的应用程序称为进程，拥有系统资源（cpu、内存）&lt;/p&gt;
&lt;/li&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs231n笔记（五）——卷积神经网络</title>
    <link href="http://yoursite.com/2018/07/25/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2018/07/25/cs231n笔记（五）——卷积神经网络/</id>
    <published>2018-07-25T03:35:57.000Z</published>
    <updated>2018-07-26T12:32:23.799Z</updated>
    
    <content type="html"><![CDATA[<p>PS：理解卷积和卷积神经网络</p><p><br></p><a id="more"></a><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><h3 id="一、自相关、互相关"><a href="#一、自相关、互相关" class="headerlink" title="一、自相关、互相关"></a>一、自相关、互相关</h3><p>两个相关函数都是对相似性的度量</p><ul><li><p>自相关：R(u)=f(t)*f(-t)</p><p>函数自身的周期性强弱</p></li></ul><ul><li><p>互相关：R(u)=f(t)*g(-t)</p><p>两个函数在不同的相对位置上互相匹配的程度</p></li></ul><p><br></p><h3 id="二、卷积层与全连接层的共同点和不同点"><a href="#二、卷积层与全连接层的共同点和不同点" class="headerlink" title="二、卷积层与全连接层的共同点和不同点"></a>二、卷积层与全连接层的共同点和不同点</h3><p>1.全连接层</p><p>权重看作是展开的一维图形模板，输入的图像也是展开的一维向量，两个向量做点积，若两个向量越相近，则点积的结果越大，即输入图像与图像模板的匹配程度越高。</p><p>2.卷积层</p><p>卷积神经网络中，卷积核是多维特征模板，但是不是整幅图像与模板匹配，而是图像的一部分与模板匹配，点积越大，也就是说匹配程度越大。</p><p><br></p><!--more--><h3 id="三、点积与叉积"><a href="#三、点积与叉积" class="headerlink" title="三、点积与叉积"></a>三、点积与叉积</h3><h4 id="1-点积、内积、数量积"><a href="#1-点积、内积、数量积" class="headerlink" title="1.点积、内积、数量积"></a>1.点积、内积、数量积</h4><p>$$<br>\mathbf{a}\cdot \mathbf{b} = \sum_{i=1}^n a_ib_i = a_1b_1 + a_2b_2 + \cdots + a_nb_n<br>$$</p><p>几何意义：b在a方向上的投影</p><p><br></p><h4 id="2-叉积、外积、向量积"><a href="#2-叉积、外积、向量积" class="headerlink" title="2.叉积、外积、向量积"></a>2.叉积、外积、向量积</h4><p>$$<br>\mathbf{a} \cdot \mathbf{b} = \mathbf{a} \mathbf{b}^T<br>$$</p><p>几何意义：a和b构成的平行四边形的面积</p><p><br></p><p><br></p><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p><br></p><h3 id="一、卷积层的处理"><a href="#一、卷积层的处理" class="headerlink" title="一、卷积层的处理"></a>一、卷积层的处理</h3><h4 id="1-padding：对原始图片在边界上进行填充填充"><a href="#1-padding：对原始图片在边界上进行填充填充" class="headerlink" title="1.padding：对原始图片在边界上进行填充填充"></a>1.padding：对原始图片在边界上进行填充填充</h4><p>每个方向扩展像素点数量为 p，填充后原始图片的大小为 (n+2p) <em> (n+2p)，滤波器保持 f </em> f不变，则输出图片大小为 (n+2p-f+1) * (n+2p-f+1)</p><p><br></p><ul><li><strong>Valid 卷积</strong>：不填充，直接卷积，结果大小为 (n-f+1) * (n-f+1)</li><li><strong>Same 卷积</strong>：进行填充，使得卷积后结果大小与输入一致 n+2p-f+1 = n ，得出 p =（f-1）/2</li></ul><p><br></p><h4 id="2-stride：卷积走的步长"><a href="#2-stride：卷积走的步长" class="headerlink" title="2.stride：卷积走的步长"></a>2.stride：卷积走的步长</h4><p>设步长为 s，填充长度为 p，输入图片大小为 n <em> n，滤波器大小为 f </em> f，则卷积后图片的尺寸为：</p><p>⌊（n+2p-f）/s+1⌋×⌊（n+2p-f）/s+1⌋⌊n+2p−fs+1⌋×⌊n+2p−fs+1⌋</p><p>⌊ ⌋ ：<strong>向下取整</strong>的符号</p><p><br></p><h3 id="二、卷积神经网络（CNN）中的三种层"><a href="#二、卷积神经网络（CNN）中的三种层" class="headerlink" title="二、卷积神经网络（CNN）中的三种层"></a>二、卷积神经网络（CNN）中的三种层</h3><h4 id="1-卷积层（Convolution-layer）"><a href="#1-卷积层（Convolution-layer）" class="headerlink" title="1.卷积层（Convolution layer）"></a>1.卷积层（Convolution layer）</h4><p>卷积核在上一级输入层上通过逐一滑动窗口计算而得，卷积核中的每一个参数都相当于传统神经网络中的权值参数，与对应的局部像素相连接，将卷积核的各个参数与对应的局部像素值相乘之和，通常还要再<strong>加上一个偏置参数</strong>。</p><p><br></p><h4 id="2-池化-采样层（Pooling-layer）"><a href="#2-池化-采样层（Pooling-layer）" class="headerlink" title="2.池化/采样层（Pooling layer）"></a>2.池化/采样层（Pooling layer）</h4><p>往往在卷积层后面，<strong>通过池化来降低卷积层输出的特征向量，不易出现过拟合</strong><br><br><br><strong>Max Pooling</strong>: 选择Pooling窗口中的最大值作为采样值<br><strong>Mean Pooling</strong>: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值</p><p><br></p><h4 id="3-全连接层（Fully-Connected-layer）"><a href="#3-全连接层（Fully-Connected-layer）" class="headerlink" title="3.全连接层（Fully Connected layer）"></a>3.全连接层（Fully Connected layer）</h4><p>每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来<br><br></p><h4 id="4-三种层在参数上的区别"><a href="#4-三种层在参数上的区别" class="headerlink" title="4.三种层在参数上的区别"></a>4.三种层在参数上的区别</h4><p>卷积层——仅有少量的参数<br>池化层——没有参数<br>全连接层——存在大量的参数</p><p><br></p><h4 id="5-注意"><a href="#5-注意" class="headerlink" title="5.注意"></a>5.注意</h4><ol><li>在计算神经网络的层数时，通常<strong>只统计具有权重和参数的层</strong>，因此<strong>池化层通常和之前的卷积层共同计为一层</strong>。</li><li>随着网络的深入，<strong>提取的特征图片大小将会逐渐减小，但同时通道数量应随之增加</strong></li></ol><p><br></p><p><br></p><h3 id="三、卷积运算参数少的机制"><a href="#三、卷积运算参数少的机制" class="headerlink" title="三、卷积运算参数少的机制"></a>三、卷积运算参数少的机制</h3><p>PS：卷积神经网络的参数少，从而解决了，一般神经网络处理图像数据时，需要用大大量参数的问题</p><p><br></p><p><strong>１．参数共享</strong>（Parameter sharing）：<strong>在卷积过程中，不管输入有多大，一个滤波器能对整个输入的某一特征进行探测</strong><br>２．<strong>稀疏连接</strong>（Sparsity of connections）：滤波器的尺寸限制使得，<strong>每个输出值只取决于输入在局部的一小部分的值</strong>，稀疏连接不同于全连接，每个输出综合了所有的输入特征</p><p><br></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：理解卷积和卷积神经网络&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（四）——神经网络</title>
    <link href="http://yoursite.com/2018/07/18/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2018/07/18/cs231n笔记（四）——神经网络/</id>
    <published>2018-07-18T01:46:56.000Z</published>
    <updated>2018-07-25T03:32:57.189Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络知识点大纲总结，还没有看完，所以持续更新 ~ </p><a id="more"></a><p><br></p><h2 id="一、非线性激活函数"><a href="#一、非线性激活函数" class="headerlink" title="一、非线性激活函数"></a>一、非线性激活函数</h2><h3 id="1-Sigmoid"><a href="#1-Sigmoid" class="headerlink" title="1.Sigmoid"></a>1.Sigmoid</h3><h3 id="2-tanh"><a href="#2-tanh" class="headerlink" title="2.tanh"></a>2.tanh</h3><h3 id="3-ReLU"><a href="#3-ReLU" class="headerlink" title="3.ReLU"></a>3.ReLU</h3><h3 id="4-Leaky-ReLU"><a href="#4-Leaky-ReLU" class="headerlink" title="4.Leaky ReLU"></a>4.Leaky ReLU</h3><h3 id="5-PReLU"><a href="#5-PReLU" class="headerlink" title="5.PReLU"></a>5.PReLU</h3><h3 id="6-EReLU"><a href="#6-EReLU" class="headerlink" title="6.EReLU"></a>6.EReLU</h3><h3 id="7-maxout"><a href="#7-maxout" class="headerlink" title="7.maxout"></a>7.maxout</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B01.jpg" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B02.png" alt=""></p><p><strong>使用非线性激活函数的原因</strong></p><p>激活函数是线性，比如，f(x) = x，那么每一层输出都是上层输入的线性函数，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果一样</p><p><br></p><p><strong>使用非线性函数的注意点</strong></p><p>ReLU非线性函数。注意设置好<strong>学习率</strong>，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰，就试试Leaky ReLU或者Maxout，不要再用sigmoid。</p><p><br></p><p><br></p><h2 id="二、数据预处理"><a href="#二、数据预处理" class="headerlink" title="二、数据预处理"></a>二、数据预处理</h2><h3 id="1-均值减法（Mean-subtraction）"><a href="#1-均值减法（Mean-subtraction）" class="headerlink" title="1.均值减法（Mean subtraction）"></a>1.均值减法（Mean subtraction）</h3><p>每个独立特征减去平均值，从几何上可以理解为在每个维度上都<strong>将数据云的中心都迁移到原点</strong>。</p><p><br></p><h3 id="2-归一化（Normalization）"><a href="#2-归一化（Normalization）" class="headerlink" title="2.归一化（Normalization）"></a>2.归一化（Normalization）</h3><p>做零中心化（zero-centered）处理，然后每个维度都除以其标准差。</p><p>第一种是对整幅图像做零中心化（zero-centered）处理，然后每个维度都除以其标准差</p><p>第二种是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。</p><p><br></p><h3 id="2-PCA和白化（Whitening）"><a href="#2-PCA和白化（Whitening）" class="headerlink" title="2.PCA和白化（Whitening）"></a>2.PCA和白化（Whitening）</h3><p><strong>注意：</strong></p><p>1.进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。</p><p>例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。<strong>应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。</strong></p><p>2.实际上在卷积神经网络中并不会采用PCA和白化变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。</p><p><br></p><p><br></p><h2 id="三、权重初始化"><a href="#三、权重初始化" class="headerlink" title="三、权重初始化"></a>三、权重初始化</h2><h3 id="1-错误：全零初始化"><a href="#1-错误：全零初始化" class="headerlink" title="1.错误：全零初始化"></a>1.错误：全零初始化</h3><p>具有对称性</p><p><br></p><h3 id="2-随机数初始化"><a href="#2-随机数初始化" class="headerlink" title="2.随机数初始化"></a>2.随机数初始化</h3><p><strong>初始化为小数</strong>：权重太小，随着网络的层次加深，激励函数的结果接近于0，也就是下一层的输入接近于0，那么在反向传播的时候就会计算出非常小的梯度。</p><p><strong>初始化为太大的数</strong>：会发生饱和使梯度消失</p><p><br></p><h3 id="3-Xavier-initialization"><a href="#3-Xavier-initialization" class="headerlink" title="3.Xavier initialization"></a>3.Xavier initialization</h3><p>Var(wi)=1/n[l-1]     n输入的神经元个数</p><p>为了得到较小的 wi，设置Var(wi)=1/n。就是将wi的方差设为1/n，标准差设为sqrt(1/n) </p><p><br></p><h3 id="4-He-Initialization"><a href="#4-He-Initialization" class="headerlink" title="4.He Initialization"></a>4.He Initialization</h3><p>Var(wi)=2/n[l-1]     n输入的神经元个数</p><p><br></p><p><br></p><h2 id="四、Batch-Normalization（BN）"><a href="#四、Batch-Normalization（BN）" class="headerlink" title="四、Batch Normalization（BN）"></a>四、Batch Normalization（BN）</h2><p>1.对各神经元的输入做类似的标准化处理，提高神经网络训练速度</p><p>2.减小不同的样本分布（Covariate Shift ）对后面几层网络的影响，使整体网络更加健壮。</p><p><br></p><p><br></p><h2 id="五、过拟合Overfitting的解决方式正则化-Regularization"><a href="#五、过拟合Overfitting的解决方式正则化-Regularization" class="headerlink" title="五、过拟合Overfitting的解决方式正则化 Regularization"></a>五、过拟合Overfitting的解决方式正则化 Regularization</h2><h3 id="方法一：L2-Regularization-正则化"><a href="#方法一：L2-Regularization-正则化" class="headerlink" title="方法一：L2  Regularization 正则化"></a>方法一：L2  Regularization 正则化</h3><p>PS：<strong>过拟合时的权重很大，正则化就是让权重不那么大</strong></p><p>1.<strong>过拟合的时候权重会很大</strong></p><p>过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，从而使得很小的区间里，函数值的变化很剧烈，即导数值的绝对值非常大。由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。</p><p>2.正则化防止过拟合的原因</p><p>正则化项对于原权重是个衰减项，从而使得权重不会变得太大</p><p><br></p><h3 id="方法二：droupout"><a href="#方法二：droupout" class="headerlink" title="方法二：droupout"></a>方法二：droupout</h3><p>随机失活是<strong>为每个神经元结点设置一个随机消除的概率</strong>，多地被使用在计算机视觉领域。</p><p><strong>注意，在测试阶段不要使用 dropout</strong></p><p><br></p><h3 id="方法三：数据扩增（Data-Augmentation）"><a href="#方法三：数据扩增（Data-Augmentation）" class="headerlink" title="方法三：数据扩增（Data Augmentation）"></a>方法三：数据扩增（Data Augmentation）</h3><p><br></p><h3 id="方法四：早停止法（Early-Stopping）"><a href="#方法四：早停止法（Early-Stopping）" class="headerlink" title="方法四：早停止法（Early Stopping）"></a>方法四：早停止法（Early Stopping）</h3><p><br></p><h3 id="方法五：Drop-Connect"><a href="#方法五：Drop-Connect" class="headerlink" title="方法五：Drop Connect"></a>方法五：Drop Connect</h3><p><br></p><h3 id="方法六：Fractional-Max-Pooling"><a href="#方法六：Fractional-Max-Pooling" class="headerlink" title="方法六：Fractional Max Pooling"></a>方法六：Fractional Max Pooling</h3><p><br></p><h3 id="方法七：Stochastic-Depth"><a href="#方法七：Stochastic-Depth" class="headerlink" title="方法七：Stochastic Depth"></a>方法七：Stochastic Depth</h3><p><br></p><p><br></p><h2 id="六、超参数的选择"><a href="#六、超参数的选择" class="headerlink" title="六、超参数的选择"></a>六、超参数的选择</h2><h3 id="1-交叉验证"><a href="#1-交叉验证" class="headerlink" title="1.交叉验证"></a>1.交叉验证</h3><p><br></p><h3 id="2-随机搜索优于网格搜索"><a href="#2-随机搜索优于网格搜索" class="headerlink" title="2.随机搜索优于网格搜索"></a>2.随机搜索优于网格搜索</h3><p><br></p><h3 id="3-为参数选择合适的范围"><a href="#3-为参数选择合适的范围" class="headerlink" title="3.为参数选择合适的范围"></a>3.为参数选择合适的范围</h3><p>有一些超参数做均匀随机取值是不合适的，需要按照一定的比例在不同的小范围内进行均匀随机取值。</p><p><br></p><h3 id="4-计算资源来决定训练模型的方式"><a href="#4-计算资源来决定训练模型的方式" class="headerlink" title="4.计算资源来决定训练模型的方式"></a>4.计算资源来决定训练模型的方式</h3><ul><li>Panda（熊猫方式）：<strong>受计算能力所限</strong>，同时试验大量模型比较困难。可以专注于<strong>试验一个或一小批模型</strong>，初始化，试着让其工作运转，观察它的表现，不断调整参数。</li><li>Caviar（鱼子酱方式）：拥有足够的计算能力去<strong>平行试验很多模型</strong>，尝试很多不同的超参数，选取效果最好的模型。</li></ul><p><br></p><p><br></p><h2 id="七、梯度下降的更新方法"><a href="#七、梯度下降的更新方法" class="headerlink" title="七、梯度下降的更新方法"></a>七、梯度下降的更新方法</h2><p><strong>加速神经网络的训练</strong></p><h3 id="1-SGD（随机梯度下降），Momentum动量，Nesterov动量"><a href="#1-SGD（随机梯度下降），Momentum动量，Nesterov动量" class="headerlink" title="1.SGD（随机梯度下降），Momentum动量，Nesterov动量"></a>1.SGD（随机梯度下降），Momentum动量，Nesterov动量</h3><p><br></p><h3 id="2-学习率退火"><a href="#2-学习率退火" class="headerlink" title="2.学习率退火"></a>2.学习率退火</h3><p><br></p><h3 id="3-二阶方法"><a href="#3-二阶方法" class="headerlink" title="3.二阶方法"></a>3.二阶方法</h3><p><br></p><h3 id="4-逐参数适应学习率方法（Adagrad，RMSProp）"><a href="#4-逐参数适应学习率方法（Adagrad，RMSProp）" class="headerlink" title="4.逐参数适应学习率方法（Adagrad，RMSProp）"></a>4.逐参数适应学习率方法（Adagrad，RMSProp）</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;神经网络知识点大纲总结，还没有看完，所以持续更新 ~ &lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（三）——分段式反向传播求梯度</title>
    <link href="http://yoursite.com/2018/07/16/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E5%88%86%E6%AE%B5%E5%BC%8F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B1%82%E6%A2%AF%E5%BA%A6/"/>
    <id>http://yoursite.com/2018/07/16/cs231n笔记（三）——分段式反向传播求梯度/</id>
    <published>2018-07-16T08:50:13.000Z</published>
    <updated>2018-07-22T02:58:32.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>1.反向传播</p><p>反向传播是利用<strong>链式法则</strong> ，递归计算梯度的方法</p><p>2.<strong>分段反向传播求梯度</strong>  (重点)</p><p><strong>不需要关于输入变量的明确的函数来计算导数</strong>，因为需要求的是梯度值。只需要将表达式分成不同的可以求导的模块，然后在反向传播中一步一步地计算<strong>梯度</strong>。</p><p>3.加法、乘法和取最大值在反向传播中关于梯度的理解</p><p>4.用向量化计算梯度</p><a id="more"></a><p><br></p><h2 id="一、反向传播"><a href="#一、反向传播" class="headerlink" title="一、反向传播"></a>一、反向传播</h2><p>反向传播是利用<strong>链式法则</strong>递归计算表达式的梯度的方法，链式法则是指用相乘将梯度表达式链接起来。</p><p><br></p><p><br></p><h2 id="二、分段反向传播求梯度"><a href="#二、分段反向传播求梯度" class="headerlink" title="二、分段反向传播求梯度"></a>二、分段反向传播求梯度</h2><p>不需要关于输入变量的明确的函数来计算导数，需要求的是梯度值。</p><p>分段计算是指将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。实际上<strong>并不需要关于输入变量的明确的函数来计算导数</strong>，只需要将表达式分成不同的可以求导的模块，然后在反向传播中一步一步地计算<strong>梯度</strong>。</p><p>假设有如下函数：</p><p><img src="http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x%2Cy%29%3D%5Cfrac%7Bx%2B%5Csigma%28y%29%7D%7B%5Csigma%28x%29%2B%28x%2By%29%5E2%7D" alt="\displaystyle f(x,y)=\frac{x+\sigma(y)}{\sigma(x)+(x+y)^2}"></p><p>（1）前向传播计算每个门的输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = 3 # 例子数值</span><br><span class="line">y = -4</span><br><span class="line"></span><br><span class="line"># 前向传播</span><br><span class="line">sigy = 1.0 / (1 + math.exp(-y)) # 分子中的sigmoi          #(1)</span><br><span class="line">num = x + sigy # 分子                                    #(2)</span><br><span class="line">sigx = 1.0 / (1 + math.exp(-x)) # 分母中的sigmoid         #(3)</span><br><span class="line">xpy = x + y                                              #(4)</span><br><span class="line">xpysqr = xpy**2                                          #(5)</span><br><span class="line">den = sigx + xpysqr # 分母                                #(6)</span><br><span class="line">invden = 1.0 / den                                       #(7)</span><br><span class="line">f = num * invden #                                  #(8)</span><br></pre></td></tr></table></figure><p>前向传播时创建了多个中间变量，每个都是比较简单的表达式，它们计算局部梯度的方法是已知的。当对前向传播时产生每个变量(<strong>sigy, num, sigx, xpy, xpysqr, den, invden</strong>)进行回传。我们会有同样数量的变量，但是都以<strong>d</strong>开头，需要存储对应变量的梯度。然后根据使用链式法则乘以上游梯度。</p><p>（2）反向传播计算每个门的梯度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 回传 f = num * invden</span><br><span class="line">dnum = invden # 分子的梯度                                         #(8)</span><br><span class="line">dinvden = num                                                     #(8)</span><br><span class="line"># 回传 invden = 1.0 / den </span><br><span class="line">dden = (-1.0 / (den**2)) * dinvden                                #(7)</span><br><span class="line"># 回传 den = sigx + xpysqr</span><br><span class="line">dsigx = (1) * dden                                                #(6)</span><br><span class="line">dxpysqr = (1) * dden                                              #(6)</span><br><span class="line"># 回传 xpysqr = xpy**2</span><br><span class="line">dxpy = (2 * xpy) * dxpysqr                                        #(5)</span><br><span class="line"># 回传 xpy = x + y</span><br><span class="line">dx = (1) * dxpy                                                   #(4)</span><br><span class="line">dy = (1) * dxpy                                                   #(4)</span><br><span class="line"># 回传 sigx = 1.0 / (1 + math.exp(-x))</span><br><span class="line">dx += ((1 - sigx) * sigx) * dsigx # Notice += !!                  #(3)</span><br><span class="line"># 回传 num = x + sigy</span><br><span class="line">dx += (1) * dnum                                                  #(2)</span><br><span class="line">dsigy = (1) * dnum                                                #(2)</span><br><span class="line"># 回传 sigy = 1.0 / (1 + math.exp(-y))</span><br><span class="line">dy += ((1 - sigy) * sigy) * dsigy                                 #(1)</span><br></pre></td></tr></table></figure><p><strong>（3）注意</strong></p><p><strong>对前向传播变量进行缓存</strong>：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。</p><p><strong>变量在不同分支的梯度要累加</strong>：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。<strong>如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加</strong>。看（3）理解。</p><p><br></p><p><br></p><h2 id="三、加法、乘法和取最大值在反向传播中关于梯度的理解"><a href="#三、加法、乘法和取最大值在反向传播中关于梯度的理解" class="headerlink" title="三、加法、乘法和取最大值在反向传播中关于梯度的理解"></a>三、加法、乘法和取最大值在反向传播中关于梯度的理解</h2><p>神经网络中最常用的加法、乘法和取最大值这三个门单元，它们在反向传播过程中的行为都有非常简单的解释。</p><p>2<em>[x\</em>y+max(w,z)]</p><p><img src="https://pic2.zhimg.com/80/39162d0c528144362cc09f1965d710d1_hd.jpg" alt="img"></p><p><strong>加法门单元</strong>：把输出的梯度相等地分发给它所有的输入。上例中，加法门把梯度2.00相等地路由给了两个输入。</p><p><strong>取最大值门单元</strong>对：将输出梯度转给前向传播中值最大的那个输入，其余的输入的梯度为0。上例中，取最大值门将梯度2.00转给了z变量，因为z的值比w高，于是w的梯度保持为0。</p><p><strong>乘法门单元</strong>：局部梯度就是交换之后的输入值，然后根据链式法则乘以输出值的梯度。上例中，x的梯度是-4.00x2.00=-8.00。</p><p>注意一种比较特殊的情况，如果乘法门单元的其中一个输入非常小，而另一个输入非常大，它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本<img src="http://www.zhihu.com/equation?tex=x_i" alt="x_i">乘以1000，那么权重的梯度将会增大1000倍，这样就必须降低学习率来弥补，所以数据预处理很重要。</p><p><br></p><p><br></p><h2 id="四、用向量化计算梯度"><a href="#四、用向量化计算梯度" class="headerlink" title="四、用向量化计算梯度"></a>四、用向量化计算梯度</h2>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;p&gt;1.反向传播&lt;/p&gt;
&lt;p&gt;反向传播是利用&lt;strong&gt;链式法则&lt;/strong&gt; ，递归计算梯度的方法&lt;/p&gt;
&lt;p&gt;2.&lt;strong&gt;分段反向传播求梯度&lt;/strong&gt;  (重点)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;不需要关于输入变量的明确的函数来计算导数&lt;/strong&gt;，因为需要求的是梯度值。只需要将表达式分成不同的可以求导的模块，然后在反向传播中一步一步地计算&lt;strong&gt;梯度&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;3.加法、乘法和取最大值在反向传播中关于梯度的理解&lt;/p&gt;
&lt;p&gt;4.用向量化计算梯度&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>cs231n笔记（一）——线性回归、逻辑回归、多分类</title>
    <link href="http://yoursite.com/2018/07/15/cs231n%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E3%80%81%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E3%80%81%E5%A4%9A%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/07/15/cs231n笔记（一）——线性回归、逻辑回归、多分类/</id>
    <published>2018-07-15T13:23:33.000Z</published>
    <updated>2018-07-22T02:57:45.048Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>一、线性回归</p><p>二、逻辑回归</p><p>三、多分类</p><p>1.根据像素差异分类</p><p>2.线性分类</p><p>   （1）多类SVM分类器-折叶损失</p><p>   （2）softmax分类器-交叉熵损失</p><a id="more"></a><p><br></p><h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p><br></p><p>即对于多维空间中存在的样本点，我们用特征的线性组合去拟合空间中点的分布和轨迹，用于对连续值结果进行预测。</p><p>线性回归的代价函数</p><p><img src="https://camo.githubusercontent.com/90f8b2963cbd56650a13d0568166905d9805f480/687474703a2f2f696d616765732e636e6974626c6f672e636f6d2f626c6f672f3537353537322f3230313331312f30393038313430352d39623439326363393533376434653662623461393739616166363430653836322e706e67" alt="img"></p><p><br></p><p><br></p><h1 id="逻辑回归（二分类）"><a href="#逻辑回归（二分类）" class="headerlink" title="逻辑回归（二分类）"></a>逻辑回归（二分类）</h1><p><br></p><p>1.过程</p><ul><li>输入特征x</li><li>用激活函数计算逻辑回归的输出 hθ(x)</li><li>计算hθ(x)的代价函数Jθ(x)</li><li>用梯度下降法结合Jθ(x)调整参数θ使得Jθ(x)最小</li><li>得出最终结果θ</li></ul><p>2.激活函数</p><p>3.逻辑回归的代价函数</p><p><img src="https://camo.githubusercontent.com/974a671827e2e3d36cd26c76c05ad600ec5502bc/687474703a2f2f696d616765732e636e6974626c6f672e636f6d2f626c6f672f3537353537322f3230313331312f30393038313433372d33306165393937633465633834303161396461663237366365646537346263372e706e67" alt="img"></p><p><img src="https://camo.githubusercontent.com/7766af8da33ce8b9bf8ca7b2c6308e78e5656693/687474703a2f2f696d616765732e636e6974626c6f672e636f6d2f626c6f672f3537353537322f3230313331312f30393038313731332d30613263623061333134613234333431396135623331623366333732393133342e706e67" alt="img"></p><p>不用线性回归的代价函数，是因为会有“非凸”函数的问题，就是这个函数有很多个局部最低点。</p><p><br></p><p><br></p><h1 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h1><p><br></p><h2 id="（一）通过像素差异分类"><a href="#（一）通过像素差异分类" class="headerlink" title="（一）通过像素差异分类"></a>（一）通过像素差异分类</h2><p>1.NN 最近邻分类器</p><p><strong>（1）Nearest Neighbor ：</strong>拿着测试图片和训练集中每一张图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。</p><p><strong>（2）k-Nearest Neighbor分类器</strong>：找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。</p><p>2.像素差异分类的缺点：图像更多的是按照背景和颜色被分类，而不是语义主体本身。</p><p><br></p><h2 id="（二）线性分类"><a href="#（二）线性分类" class="headerlink" title="（二）线性分类"></a>（二）线性分类</h2><p><br></p><h3 id="一、分类三个步骤"><a href="#一、分类三个步骤" class="headerlink" title="一、分类三个步骤"></a>一、分类三个步骤</h3><p>由三部分组成：</p><p><strong>1. 评分函数（score function）</strong> ：将原始图像数据映射为各个类别的得分，得分高低代表图像属于该类别的可能性高低。</p><p>线性分类评分函数为线性映射：<img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%2Cb%29%3DWx_i%2Bb" alt="\displaystyle f(x_i,W,b)=Wx_i+b"></p><p><strong>2. 损失函数（loss function）</strong> ：量化分类标签的得分与真实标签之间一致性。也就是<strong>量化某个具体权重集W的质量</strong> 。</p><p><strong>3.最优化（Optimization）</strong> ：找到能够最小化损失函数值的W。</p><p><br></p><h3 id="二、损失函数正则化"><a href="#二、损失函数正则化" class="headerlink" title="二、损失函数正则化"></a>二、损失函数正则化</h3><p>1.为什么正则化</p><p>向损失函数增加一个正则化惩罚（regularization penalty）<img src="https://www.zhihu.com/equation?tex=R%28W%29" alt="R(W)">部分。可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响，并避免过拟合。</p><p>注意：通常只对权重<img src="https://www.zhihu.com/equation?tex=W" alt="W">正则化，而不正则化偏差<img src="https://www.zhihu.com/equation?tex=b" alt="b">。</p><p>2.完整的损失函数</p><p>数据损失（data loss），即所有样例的的平均损失<img src="https://www.zhihu.com/equation?tex=L_i" alt="L_i">，和正则化损失（regularization loss）组成</p><p><img src="https://www.zhihu.com/equation?tex=L%3D%5Cdisplaystyle+%5Cunderbrace%7B+%5Cfrac%7B1%7D%7BN%7D%5Csum_i+L_i%7D_%7Bdata+%5C++loss%7D%2B%5Cunderbrace%7B%5Clambda+R%28W%29%7D_%7Bregularization+%5C+loss%7D" alt="L=\displaystyle \underbrace{ \frac{1}{N}\sum_i L_i}_{data \  loss}+\underbrace{\lambda R(W)}_{regularization \ loss}"></p><p><br></p><h3 id="三、损失函数分类"><a href="#三、损失函数分类" class="headerlink" title="三、损失函数分类"></a>三、损失函数分类</h3><p>1.多类SVM</p><p>（1）多类SVM</p><p><strong>希望正确分类类别<img src="https://www.zhihu.com/equation?tex=y_i" alt="y_i">的分数比不正确类别分数高，而且至少要高<img src="https://www.zhihu.com/equation?tex=%5CDelta" alt="\Delta">，如果不满足这点，就开始计算损失值。</strong></p><p>（2）损失函数</p><p><strong>折叶损失（hinge loss）</strong>：<img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cs_j-s_%7By_i%7D%2B%5CDelta%29" alt="\displaystyle L_i=\sum_{j\not=y_i}max(0,s_j-s_{y_i}+\Delta)">  </p><p><br></p><p>2.Softmax分类器</p><p>（1）Softmax分类器</p><p>是逻辑回归分类器即二分类，面对多个分类的一般化归纳。</p><p>（2）损失函数</p><p><strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong> ：<img src="http://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29" alt="\displaystyle Li=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})"></p><p>其中<img src="http://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D" alt="f_j(z)=\frac{e^{z_j}}{\sum_ke^{z_k}}">是softmax 函数</p><p>softmax 函数：<strong>将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;p&gt;一、线性回归&lt;/p&gt;
&lt;p&gt;二、逻辑回归&lt;/p&gt;
&lt;p&gt;三、多分类&lt;/p&gt;
&lt;p&gt;1.根据像素差异分类&lt;/p&gt;
&lt;p&gt;2.线性分类&lt;/p&gt;
&lt;p&gt;   （1）多类SVM分类器-折叶损失&lt;/p&gt;
&lt;p&gt;   （2）softmax分类器-交叉熵损失&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/tags/cs231n/"/>
    
  </entry>
  
  <entry>
    <title>显卡</title>
    <link href="http://yoursite.com/2018/07/15/%E6%98%BE%E5%8D%A1/"/>
    <id>http://yoursite.com/2018/07/15/显卡/</id>
    <published>2018-07-15T03:28:11.000Z</published>
    <updated>2018-07-22T03:20:49.147Z</updated>
    
    <content type="html"><![CDATA[<p>PS ：最近想入一个显示屏，结果发现自己连显卡是什么都搞不清楚，我真是菜的不行</p><p><br></p><h2 id="显卡"><a href="#显卡" class="headerlink" title="显卡"></a>显卡</h2><p>显卡是电脑进行数模信号转换的设备，承担输出显示图形的任务。</p><p>1.显存：又叫帧缓存，作用是用来<strong>存储</strong>GPU处理过或者即将提取的渲染数据</p><p>2.GPU（Graphic Processing Unit）：是显卡的核心引擎，专为图形渲染设计的，并行处理方面，浮点运算方面功能强大。</p><p>3.理解显存和GPU：GPU相当于电脑的CPU，显存相当于电脑的内存</p><a id="more"></a><p><br></p><h2 id="显卡分类"><a href="#显卡分类" class="headerlink" title="显卡分类"></a>显卡分类</h2><p>1.集显：集成显卡，显示核心集成在主板上</p><p>2.核显：就是集成在CPU里，也算是集显的一种</p><p>3.独显：相对集显而已，最明显的一个特性，就是<strong>自带显存</strong>，<strong>集显是共享内存的</strong>，性能上比集显要好很多</p><p>4.双显：一般是指笔记本同时配置有独显和集显（核显），可以在这两个显卡之间来回切换使用</p><p><br></p><h2 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h2><p>CUDA是NVIDIA推出的用于自家GPU的<strong>并行计算</strong>框架，并且CUDA只能在NVIDIA的GPU上运行。</p><p>CUDA 架构下，一个程序分为两个部份：Host 端和 device 端。Host 端是指在 CPU 上执行的部份，device 端则是在显示芯片上执行的部份。Device 端的程序又称为 “kernel”。通常 Host 端程序会将数据准备好后，复制到显卡的内存中，再由GPU执行 Device 端程序，完成后再由 host 端程序将结果从显卡的内存中取回。</p><p><br></p><p><strong>cuDNN</strong>（CUDA Deep Neural Network library）：是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。要用GPU训练模型，cuDNN不是必须的，但是一般会采用这个加速库。</p><p><br></p><h2 id="图形渲染"><a href="#图形渲染" class="headerlink" title="图形渲染"></a>图形渲染</h2><p>我的理解就是将图形尽量接近真实场景，逼真地呈现</p><p><a href="http://imgtec.eetrend.com/blog/9825" target="_blank" rel="noopener">http://imgtec.eetrend.com/blog/9825</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS ：最近想入一个显示屏，结果发现自己连显卡是什么都搞不清楚，我真是菜的不行&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;显卡&quot;&gt;&lt;a href=&quot;#显卡&quot; class=&quot;headerlink&quot; title=&quot;显卡&quot;&gt;&lt;/a&gt;显卡&lt;/h2&gt;&lt;p&gt;显卡是电脑进行数模信号转换的设备，承担输出显示图形的任务。&lt;/p&gt;
&lt;p&gt;1.显存：又叫帧缓存，作用是用来&lt;strong&gt;存储&lt;/strong&gt;GPU处理过或者即将提取的渲染数据&lt;/p&gt;
&lt;p&gt;2.GPU（Graphic Processing Unit）：是显卡的核心引擎，专为图形渲染设计的，并行处理方面，浮点运算方面功能强大。&lt;/p&gt;
&lt;p&gt;3.理解显存和GPU：GPU相当于电脑的CPU，显存相当于电脑的内存&lt;/p&gt;
    
    </summary>
    
    
      <category term="GPU" scheme="http://yoursite.com/tags/GPU/"/>
    
      <category term="硬件" scheme="http://yoursite.com/tags/%E7%A1%AC%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>程序是怎样跑起来的</title>
    <link href="http://yoursite.com/2018/07/14/%E7%A8%8B%E5%BA%8F%E6%98%AF%E6%80%8E%E6%A0%B7%E8%B7%91%E8%B5%B7%E6%9D%A5%E7%9A%84/"/>
    <id>http://yoursite.com/2018/07/14/程序是怎样跑起来的/</id>
    <published>2018-07-14T08:45:16.000Z</published>
    <updated>2018-07-22T03:20:16.711Z</updated>
    
    <content type="html"><![CDATA[<h1 id="程序是怎样跑起来的"><a href="#程序是怎样跑起来的" class="headerlink" title="程序是怎样跑起来的"></a>程序是怎样跑起来的</h1><p>PS：超级感谢师兄把这本书借给我，因为我的硬件基础实在是太差，这本书让我对硬件、操作系统有了个基本的概念。</p><p>这篇博就是记录读书笔记，有些章节我比较熟悉，就跳过了</p><a id="more"></a><p><br></p><h2 id="一、CPU"><a href="#一、CPU" class="headerlink" title="一、CPU"></a>一、CPU</h2><h3 id="1-CPU内部结构"><a href="#1-CPU内部结构" class="headerlink" title="1.CPU内部结构"></a>1.CPU内部结构</h3><ol><li>寄存器：暂存指令、数据</li><li>控制器：将内存中的指令、数据读入寄存器，根据运算结果控制计算机（如显示器的输入输出）</li><li>运算器：计算寄存器里的数据</li><li>时钟：发出CPU开始计时的时钟信号，CPU运行速度</li></ol><p>对于程序员而言，CPU的关注点在各种功能的<strong>寄存器</strong></p><h3 id="2-程序计数器"><a href="#2-程序计数器" class="headerlink" title="2.程序计数器"></a>2.程序计数器</h3><p>程序计数器是CPU寄存器的一种，CPU每执行一条指令，程序计数器的值加1，若执行指令占据多个内存地址，增加相应的数值，即程序计数器存储的是指令地址。</p><h3 id="3-条件分支和循环机制"><a href="#3-条件分支和循环机制" class="headerlink" title="3.条件分支和循环机制"></a>3.条件分支和循环机制</h3><p>程序流程：顺序执行、条件分支、循环</p><h3 id="4-函数调用机制"><a href="#4-函数调用机制" class="headerlink" title="4.函数调用机制"></a>4.函数调用机制</h3><p>（1）CALL指令：将调用函数后面的指令地址放到内存的栈里</p><p>（2）程序计数器被设定为要调用的函数的入口地址</p><p>（3）RETURN指令：调用函数执行完后，把栈里的地址取出，将程序计数器设定为该地址</p><h3 id="5-内存地址的查看"><a href="#5-内存地址的查看" class="headerlink" title="5.内存地址的查看"></a>5.内存地址的查看</h3><p>内存地址通过基址寄存器+变址寄存器来查看更方便，相当于<strong>二维数组的索引</strong>。</p><h3 id="6-CPU能直接识别和执行的只有机器语言"><a href="#6-CPU能直接识别和执行的只有机器语言" class="headerlink" title="6. CPU能直接识别和执行的只有机器语言"></a>6. CPU能直接识别和执行的只有机器语言</h3><p><br></p><p><br></p><h2 id="二、计算机信息用二进制数表示"><a href="#二、计算机信息用二进制数表示" class="headerlink" title="二、计算机信息用二进制数表示"></a>二、计算机信息用二进制数表示</h2><p>1.内存和磁盘都是用字节单位来存储和读写数据，所以字节是信息的基本单位，位是最小单位</p><p>2.计算机中的小数计算错误：是因为有些十进制的小数无法转换为二进制数</p><p>举个例子：十进制0.1转为二进制会变成0.00011001100……(1100循环) 。实际上，计算机不会这样处理小数。</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/error.JPG" alt=""></p><p>3.计算机中小数的表示：定点数和浮点数</p><ul><li>定点数：小数点需要固定到某个位置，可以自己在程序中指定，前面的是整数，后面的是小数。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-4d04c3d95b762e3b490a754131f35ea3_hd.jpg" alt="img"></p><ul><li>浮点数：由符号、尾数、基数、指数四部分组成</li></ul><p>双精度浮点数64位(double)，单精度浮点数32位(float)</p><p><img src="https://pic4.zhimg.com/80/v2-94bf60eecf4bcbb038ede2c8f18bfa93_hd.jpg" alt="img"></p><p><img src="https://pic1.zhimg.com/80/v2-b5d44cc4d1f293d07826fc052b20213c_hd.jpg" alt="img"></p><p>最高位是符号，接着是指数E，剩下的是尾数M，其中基数为2</p><p>4.正则表达式</p><p>尾数：用的是<strong>将小数点前面的值固定为1的正则表达式</strong></p><p>因为浮点数可以用不同的数值表示同一数值，所以需要制定统一的规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1011.0011</span><br><span class="line">0001.01100111 右移三位</span><br><span class="line">0110011000...00(确保长度为23，单精度浮点数的尾部部分是23)</span><br><span class="line">仅仅保存小数点后面部分，完成正则表达式（所以我总算明白了为什么这个叫尾数）</span><br></pre></td></tr></table></figure><p>5.EXCESS系统</p><p>指数：用的是EXCESS系统表，为的是<strong>表示负数时不使用符号位</strong>（通过将指数部分表示范围的中间值设为0）</p><p>举个例子：ABCDEFG　D表示0，那么E表示1，C表示-1</p><p><br></p><p><br></p><h2 id="三、内存"><a href="#三、内存" class="headerlink" title="三、内存"></a>三、内存</h2><h3 id="1-内存的物理机制"><a href="#1-内存的物理机制" class="headerlink" title="1.内存的物理机制"></a>1.内存的物理机制</h3><p>内存IC中有电源、地址信号、数据信号、控制信号 (读写操作)。</p><p> 内存，包括随机存储器（RAM），只读存储器（<strong>ROM</strong>），以及高速缓存（CACHE）。 只不过因为RAM是其中最重要的存储器。 通常所说的内即指电脑系统中的RAM。</p><p><br></p><h3 id="2-数据在内存中的存储"><a href="#2-数据在内存中的存储" class="headerlink" title="2.数据在内存中的存储"></a>2.数据在内存中的存储</h3><p>数据类型在内存中看就是，占据内存的大小(当然符号位先不管)</p><p>字节序</p><ul><li>Big endian ：按照从低地址到高地址的顺序存放数据的高位字节到低位字节</li><li>Little endian：按照从低地址到高地址的顺序存放据的低位字节到高位字节</li></ul><p><br></p><h3 id="3-指针"><a href="#3-指针" class="headerlink" title="3.指针"></a>3.指针</h3><p>存储数据的内存地址</p><p><code>char *d</code>  char 表示一次能从地址中读取一个字节的数据</p><p> <code>long *l</code> long 表示一次能从地址中读取四个字节的数据</p><p><br></p><h3 id="4-数组"><a href="#4-数组" class="headerlink" title="4.数组"></a>4.数组</h3><p>数组和内存的物理构造是一样的，数组使编程工作更高效</p><p><br></p><h3 id="5-栈、队列、环形缓冲区（不需要指定地址和索引）"><a href="#5-栈、队列、环形缓冲区（不需要指定地址和索引）" class="headerlink" title="5.栈、队列、环形缓冲区（不需要指定地址和索引）"></a>5.栈、队列、环形缓冲区（不需要指定地址和索引）</h3><p>栈：数据只能在栈顶插入(push)或删除(pop)  ，对内存中的数据读写 LIFO (last in first out)</p><p>队列：队尾插入数据，队头删除数据 ，对内存中的数据读写 FIFO (first in first out)</p><p>队列一般是以环状缓冲区的形式实现，使数据的写入和读出循环起来</p><p><br></p><h3 id="6-链表"><a href="#6-链表" class="headerlink" title="6.链表"></a>6.链表</h3><p>使元素插入、删除更容易，不像数组那样需要一部分数据整体都移动</p><p><br></p><h3 id="7-二叉查找树（数据搜索）"><a href="#7-二叉查找树（数据搜索）" class="headerlink" title="7.二叉查找树（数据搜索）"></a>7.二叉查找树（数据搜索）</h3><p>在链表的基础上插入、删除元素，考虑数据大小，将其分成左右两个方向</p><p><br></p><p><br></p><h2 id="四、磁盘"><a href="#四、磁盘" class="headerlink" title="四、磁盘"></a>四、磁盘</h2><h3 id="1-内存和磁盘"><a href="#1-内存和磁盘" class="headerlink" title="1.内存和磁盘"></a>1.内存和磁盘</h3><p>都可以用来存储程序命令和数据。</p><ul><li>内存：用电流来实现存储，高速小容量</li><li>磁盘：用磁效应来实现存储，低速高容量</li></ul><p>内存主要是指主内存 (存储CPU中运行的程序指令和数据)，磁盘主要是指硬盘</p><p><br></p><h3 id="2-存储程序方式-程序内置方式"><a href="#2-存储程序方式-程序内置方式" class="headerlink" title="2.存储程序方式 (程序内置方式)"></a>2.存储程序方式 (程序内置方式)</h3><p><strong>磁盘中存储的程序，必须要加载到内存才能运行</strong>，因为CPU运行和解析程序的时候要通过<strong>程序计数器</strong>指定内存地址来读取程序</p><p><br></p><h3 id="3-磁盘缓存-假想的磁盘"><a href="#3-磁盘缓存-假想的磁盘" class="headerlink" title="3.磁盘缓存 (假想的磁盘)"></a>3.磁盘缓存 (假想的磁盘)</h3><p>磁盘缓存：从磁盘中读取数据到内存中存储的方式，从而加快访问速度。</p><p>这种将低速设备中的数据保存到高速设备中的<strong>缓存</strong>方式，其它情况也会用到。Web浏览器通过远程Web服务器获取数据，在显示较大的图片或视频时，其实先将文件缓存在硬盘里，等到需要时从硬盘读取。(应该就是视频已经缓存到硬盘里，等你点开播放键，再从硬盘读取，这样读取速度会快)</p><p><br></p><h3 id="4-虚拟内存-假想的内存"><a href="#4-虚拟内存-假想的内存" class="headerlink" title="4.虚拟内存(假想的内存)"></a>4.虚拟内存(假想的内存)</h3><p>将磁盘的一部分作为内存使用，在<strong>内存不足时也能运行程序</strong>，其实是<strong>将实际内存与磁盘虚拟内存中的内容置换</strong>得以运行程序。</p><p>虚拟内存有分段式和分页式。<strong>分页式</strong>是指不考虑程序构造，将程序按照页(page)的大小进行分割，然后按照页大小在磁盘的虚拟内存和内存之间进行置换。</p><p>page file 1920MB</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/virtual%20memory.JPG" alt=""></p><p><br></p><h3 id="5-节约内存的编程方法"><a href="#5-节约内存的编程方法" class="headerlink" title="5.节约内存的编程方法"></a>5.节约内存的编程方法</h3><p>(1) DLL(Dynamic Link Library) 多个应用可以共用同一个DLL文件，从而节约了内存</p><p>(2) C语言编程可以通过调用_stdcall</p><p>C语言在调用函数后需要进行栈清理处理 </p><ul><li>在调用方处理：多次进行相同的栈清理处理</li><li>在被调用方处理：只进行一次栈清理</li></ul><p><br></p><h3 id="6-磁盘划分"><a href="#6-磁盘划分" class="headerlink" title="6. 磁盘划分"></a>6. 磁盘划分</h3><p>磁盘的划分有扇区和可变长</p><ul><li>扇区是硬盘上最小的读写单位</li><li>簇是<strong>文件系统</strong>的最小读写单位，是扇区的倍数，簇可以保证里面的扇区是连续的</li></ul><p><strong>文件系统</strong>：是操作系统在存储设备上组织文件的方法。Windows 98 以前所使用的是文件系统是 FAT，Windows 2000 以后的版本有所谓的 NTFS 。</p><p><br></p><h3 id="7-驱动"><a href="#7-驱动" class="headerlink" title="7.驱动"></a>7.驱动</h3><p>驱动程序就是运行在操作系统和硬件之间，用来协助<strong>操作系统控制硬件的程序</strong></p><p>操作系统只与输入输出设备的驱动打交道，比如显卡、声卡，与硬件直接打交道的是驱动程序</p><p><br></p><p><br></p><h2 id="五、程序的运行环境"><a href="#五、程序的运行环境" class="headerlink" title="五、程序的运行环境"></a>五、程序的运行环境</h2><h3 id="1-操作系统和硬件共同决定了程序的运行环境"><a href="#1-操作系统和硬件共同决定了程序的运行环境" class="headerlink" title="1. 操作系统和硬件共同决定了程序的运行环境"></a>1. 操作系统和硬件共同决定了程序的运行环境</h3><p><br></p><h3 id="2-机器语言"><a href="#2-机器语言" class="headerlink" title="2.机器语言"></a>2.机器语言</h3><p>机器语言是用<strong>二进制代码</strong>表示的计算机能直接识别和执行的一种<strong>机器指令的集合</strong>。这种指令集称为机器码或原生码。<strong>不同种类的计算机其机器语言是不相通的</strong>，按某种计算机的机器指令编制的程序不能在另一种计算机上执行。</p><p>对于<strong>源代码</strong>进行编译后得到<strong>本地代码</strong>，即机器语言的程序。</p><p><br></p><h3 id="3-不同操作系统API不同"><a href="#3-不同操作系统API不同" class="headerlink" title="3. 不同操作系统API不同"></a>3. 不同操作系统API不同</h3><p>应用软件需要根据不同的操作系统来开发，<strong>应用程序向操作系统传递指令的途径叫API</strong>，像鼠标输入、显示器输出等</p><p><br></p><h3 id="4-利用虚拟机软件运行其它操作系统下才能运行的应用"><a href="#4-利用虚拟机软件运行其它操作系统下才能运行的应用" class="headerlink" title="4. 利用虚拟机软件运行其它操作系统下才能运行的应用"></a>4. 利用虚拟机软件运行其它操作系统下才能运行的应用</h3><p><br></p><h3 id="5-Java虚拟机"><a href="#5-Java虚拟机" class="headerlink" title="5. Java虚拟机"></a>5. Java虚拟机</h3><p>Java编译后生成的不是特定CPU使用的本地代码，而是<strong>字节代码</strong>，Java虚拟机为字节代码的运行提供了运行环境，即<strong>一边编译为本地代码一边运行</strong>。</p><p>Java虚拟机看作是不依赖于特定硬件及操作系统的运行环境</p><p><br></p><h3 id="6-BIOS"><a href="#6-BIOS" class="headerlink" title="6. BIOS"></a>6. BIOS</h3><p><strong>BIOS放在内存中的ROM里</strong> ，掉电不丢失</p><p>BIOS (Basic Input Output System ) 记录了控制外围设备的程序和数据。开机后，BIOS首先确认硬件是否正常运行，没有问题后，会启动<strong>硬盘里的引导程序</strong>。</p><p><br></p><p><br></p><h2 id="六、源文件到可执行文件"><a href="#六、源文件到可执行文件" class="headerlink" title="六、源文件到可执行文件"></a>六、源文件到可执行文件</h2><h3 id="1-计算机只能运行本地代码"><a href="#1-计算机只能运行本地代码" class="headerlink" title="1.计算机只能运行本地代码"></a>1.计算机只能运行本地代码</h3><p><br></p><h3 id="2-编译器"><a href="#2-编译器" class="headerlink" title="2.编译器"></a>2.编译器</h3><p>编译器与编程语言还与CPU的种类有关，将源代码转换为本地代码 </p><p>所以对于源文件编译生成的目标文件 .obj ，其内容是本地代码</p><p><strong>交叉编译器</strong>：在一种计算机环境中运行的编译程序，能编译出<strong>另外一种环境下运行的代码</strong> </p><p><br></p><h3 id="3-链接器"><a href="#3-链接器" class="headerlink" title="3.链接器"></a>3.链接器</h3><p>编译后生成的目标文件无法直接运行，需要用链接器将多个目标文件结合，才能生成课执行文件.exe</p><p><br></p><h3 id="4-库文件"><a href="#4-库文件" class="headerlink" title="4.库文件"></a>4.库文件</h3><p>库文件：将多个目标文件集中保存到一个文件中的形式，链接时将指定的目标文件抽取出。静态数据库(.lib文件)和动态数据库(.dll文件)。</p><p><strong>静态链接</strong>：lib 即包括函数位置信息的索引，也包括实现，在编译时直接将代码加入程序当中</p><p><strong>动态链接</strong>：需要用到两个文件，一个是 lib文件，包含 dll 中的函数名称和位置。另一个是dll文件，包含实际的函数和数据，应用程序通过 lib 文件链接到 dll 文件</p><p><br></p><h3 id="5-可执行文件的运行"><a href="#5-可执行文件的运行" class="headerlink" title="5.可执行文件的运行"></a>5.可执行文件的运行</h3><p>1）可执行文件是放在硬盘中的，需要将程序加载进内存后运行</p><p>2）本地代码处理变量或函数时，跳转到的是变量或函数在内存中的地址，但是可执行文件中并没有指定变量或函数的实际内存地址。可执行文件中变量或函数的实际地址如何表示？</p><p>可执行文件中给变量或函数分配的是<strong>虚拟内存的地址</strong>，程序运行时，会将虚拟内存的地址转为实际内存的地址，这需要<strong>再配置信息</strong>（为这种地址转换提供的信息）</p><p><br></p><h3 id="6-程序加载生成堆和栈"><a href="#6-程序加载生成堆和栈" class="headerlink" title="6.程序加载生成堆和栈"></a>6.程序加载生成堆和栈</h3><p>可执行文件中包括再配置信息、函数、数据，<strong>不存在堆和栈</strong></p><p>堆和栈需要的内存空间是在可执行文件运行时分配的</p><p><br></p><h3 id="7-栈"><a href="#7-栈" class="headerlink" title="7.栈"></a>7.栈</h3><p>存储函数内部的局部变量和方法调用所用参数的内存区域</p><p><strong>自动申请和分配</strong></p><p><br></p><h3 id="8-堆"><a href="#8-堆" class="headerlink" title="8.堆"></a>8.堆</h3><p>通过malloc和new等<strong>动态申请</strong>内存的语句使用，也需要用户<strong>手动释放</strong></p><p><br></p><p><br></p><h2 id="七、操作系统和应用"><a href="#七、操作系统和应用" class="headerlink" title="七、操作系统和应用"></a>七、操作系统和应用</h2><h3 id="1-监控程序"><a href="#1-监控程序" class="headerlink" title="1.监控程序"></a>1.监控程序</h3><p>监控程序：操作系统的原型，具有加载和运行的功能，即将各种应用程序加载到内存中运行</p><p>初期的操作系统：任何程序的输入输出部分都是一样的，所以将这一部分加入到监控程序中形成初期操作系统</p><p><br></p><h3 id="2-系统调用"><a href="#2-系统调用" class="headerlink" title="2.系统调用"></a>2.系统调用</h3><p>系统调用：操作系统的硬件控制功能</p><p>高级语言的<strong>可移植性</strong>：高级语言程序编译后生成相应操作系统的系统调用，即生成利用系统调用的本地代码</p><p><br></p><h3 id="3-Windows操作系统的特征"><a href="#3-Windows操作系统的特征" class="headerlink" title="3.Windows操作系统的特征"></a>3.Windows操作系统的特征</h3><h4 id="（1）API"><a href="#（1）API" class="headerlink" title="（1）API"></a>（1）API</h4><p>API：应用程序接口，将应用程序和操作系统相连接，由多个DLL文件提供</p><h4 id="（2）GUI-图形用户界面"><a href="#（2）GUI-图形用户界面" class="headerlink" title="（2）GUI 图形用户界面"></a>（2）GUI 图形用户界面</h4><h4 id="（3）-WYSIWYG"><a href="#（3）-WYSIWYG" class="headerlink" title="（3） WYSIWYG"></a>（3） WYSIWYG</h4><p>what you see is what you get 所见即所得 ：显示器和打印机作为同等的输出设备处理</p><h4 id="（4）多任务"><a href="#（4）多任务" class="headerlink" title="（4）多任务"></a>（4）多任务</h4><p>通过时钟切割，使得多个程序同时运行，即在短时间内让多个程序切换运行，看起来就像多个程序同时运行</p><h4 id="（5）网络功能和数据库功能"><a href="#（5）网络功能和数据库功能" class="headerlink" title="（5）网络功能和数据库功能"></a>（5）网络功能和数据库功能</h4><p>叫作中间件，操作系统和中间件合起来叫系统软件，应用可以利用操作系统，也可以利用中间件的功能</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E4%B8%AD%E9%97%B4%E4%BB%B6.png" alt=""></p><h4 id="6-即插即用-PNP（plug-and-play）"><a href="#6-即插即用-PNP（plug-and-play）" class="headerlink" title="(6)即插即用 PNP（plug and play）"></a>(6)即插即用 PNP（plug and play）</h4><p>新的设备连接到计算机，系统会自动安装和设定设备的驱动程序</p><p>基本的驱动装系统的时候都是自动安装的，但是大部分的硬件驱动是要自己安装的，比如升级的显卡、声卡</p><p><br></p><p><br></p><h2 id="八、汇编语言"><a href="#八、汇编语言" class="headerlink" title="八、汇编语言"></a>八、汇编语言</h2><p>汇编语言使用助记符 </p><p>汇编语言编译为本地代码的过程叫汇编，反之，叫反汇编</p><p>汇编语言与本地代码一一对应，高级语言反编译是无法做到完全还原</p><p><br></p><p><br></p><h2 id="九、硬件"><a href="#九、硬件" class="headerlink" title="九、硬件"></a>九、硬件</h2><h3 id="1-系统调用"><a href="#1-系统调用" class="headerlink" title="1.系统调用"></a>1.系统调用</h3><p>系统调用实现对硬件的控制，而API则是调用的函数，函数实体存储在DLL文件中</p><p><br></p><h3 id="2-IN、OUT"><a href="#2-IN、OUT" class="headerlink" title="2.IN、OUT"></a>2.IN、OUT</h3><p>向外围设备进行输入输出操作的指令</p><p>IN：将指定端口号的端口数据输入到CPU寄存器</p><p>OUT：将CPU寄存器中的数据输出到指定端口号的端口</p><p><br></p><h3 id="3-I-O-控制器"><a href="#3-I-O-控制器" class="headerlink" title="3.I/O 控制器"></a>3.I/O 控制器</h3><p>主机附带了连接外围设备的连接器，连接器内部有I/O控制器，I/O控制器是用来交换主机与外围设备电流特性的IC，因为电压不同、数字信号与模拟信号的电流特性不同，主机与外围设备无法直接连接</p><p><strong>I/O控制器可以控制多个外围设备</strong></p><p><br></p><h3 id="4-端口-port"><a href="#4-端口-port" class="headerlink" title="4.端口 port"></a>4.端口 port</h3><p>端口是I/O控制器中用于临时保存数据的内存，也叫寄存器（CPU内部的寄存器主要用于数据运算，I/O寄存器主要用于临时存储数据）</p><p>端口号又叫I/O地址，用于区分端口</p><p><strong>IN OUT指令在指定了端口号的端口和CPU之间进行数据的输入输出，和通过内存地址的内存与CPU进行数据交换其实是一样的</strong> </p><p><br></p><h3 id="5-中断请求-IRQ（Interrupt-Request）"><a href="#5-中断请求-IRQ（Interrupt-Request）" class="headerlink" title="5.中断请求 IRQ（Interrupt Request）"></a>5.中断请求 IRQ（Interrupt Request）</h3><p>外围设备向CPU请求中断，使用<strong>中断编号</strong>对不同设备区分</p><p>操作系统和BIOS提供响应中断编号的<strong>中断处理程序</strong></p><p><strong>中断控制器</strong>将多个设备发出的中断请求有序的发给CPU</p><p>利用中断可以实现对数据的实时处理</p><p><br></p><h3 id="6-直接内存存取-DMA（Direct-Memory-Access）"><a href="#6-直接内存存取-DMA（Direct-Memory-Access）" class="headerlink" title="6.直接内存存取 DMA（Direct Memory Access）"></a>6.直接内存存取 DMA（Direct Memory Access）</h3><p>外围设备直接与主内存进行数据传输，<strong>不需要通过CPU</strong>，节约了时间</p><p>PS：I/O端口号、中断号、DMA通道用于识别外围设备</p><p><br></p><h3 id="7-显示机制"><a href="#7-显示机制" class="headerlink" title="7.显示机制"></a>7.显示机制</h3><p>与主内存相独立的VRAM中存储显示器显示的信息，通过中断实现数据的显示</p><p><br></p><p><br></p><h2 id="十、随机数"><a href="#十、随机数" class="headerlink" title="十、随机数"></a>十、随机数</h2><p>1.随机数：用程序来<strong>表示人类的直觉</strong>的一种方法</p><p>2.<strong>伪随机数</strong>：借助公式产生具有一定规律性的随机数</p><p>3.<strong>随机数的种子</strong></p><p>通过一个例子理解：有一种获取伪随机数的方法叫线性同余 R`=(a*R+b) mod c </p><p>R是当前随机数 R`是下一个出现的随机数</p><p>R、a、b、c的数值与当前系统时间有关，并且都有默认值，但是<strong>不调用获取当前系统时间的函数</strong>，R、a、b、c的数值使用的就是默认值，这里R、a、b、c就是随机数的种子</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;程序是怎样跑起来的&quot;&gt;&lt;a href=&quot;#程序是怎样跑起来的&quot; class=&quot;headerlink&quot; title=&quot;程序是怎样跑起来的&quot;&gt;&lt;/a&gt;程序是怎样跑起来的&lt;/h1&gt;&lt;p&gt;PS：超级感谢师兄把这本书借给我，因为我的硬件基础实在是太差，这本书让我对硬件、操作系统有了个基本的概念。&lt;/p&gt;
&lt;p&gt;这篇博就是记录读书笔记，有些章节我比较熟悉，就跳过了&lt;/p&gt;
    
    </summary>
    
    
      <category term="硬件" scheme="http://yoursite.com/tags/%E7%A1%AC%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>解决pip安装慢的问题</title>
    <link href="http://yoursite.com/2018/06/28/%E8%A7%A3%E5%86%B3pip%E5%AE%89%E8%A3%85%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/06/28/解决pip安装慢的问题/</id>
    <published>2018-06-28T11:33:08.000Z</published>
    <updated>2018-06-28T11:34:44.078Z</updated>
    
    <content type="html"><![CDATA[<p>完全转载该博主的原文： <a href="https://blog.csdn.net/furzoom/article/details/53897318" target="_blank" rel="noopener">https://blog.csdn.net/furzoom/article/details/53897318</a></p><p>由于国外官方pypi经常被墙，导致不可用，所以需要将pip源更换一下，这样就能解决被墙导致的装不上库的烦恼。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;完全转载该博主的原文： &lt;a href=&quot;https://blog.csdn.net/furzoom/article/details/53897318&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/furzoom
      
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业（七）</title>
    <link href="http://yoursite.com/2018/06/28/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%B8%83%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/28/Deep-Learning-吴恩达-作业（七）/</id>
    <published>2018-06-28T02:17:34.000Z</published>
    <updated>2018-06-28T02:22:23.093Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、对于训练集的处理"><a href="#一、对于训练集的处理" class="headerlink" title="一、对于训练集的处理"></a>一、对于训练集的处理</h2><h3 id="1-Mini-Batch"><a href="#1-Mini-Batch" class="headerlink" title="1.Mini-Batch"></a>1.Mini-Batch</h3><p>每次处理训练集的一部分进行梯度下降，避免了遍历完整个训练集才完成一次参数更新</p><h3 id="2-Batch-Gradient-Descent"><a href="#2-Batch-Gradient-Descent" class="headerlink" title="2.Batch Gradient Descent"></a>2.Batch Gradient Descent</h3><p>每次处理完所有训练集进行梯度下降</p><h3 id="3-Stochastic-Gradient-Descent"><a href="#3-Stochastic-Gradient-Descent" class="headerlink" title="3.Stochastic Gradient Descent"></a>3.Stochastic Gradient Descent</h3><p>每次处理完一个样本就进行梯度下降</p><h2 id="二、反向传播加速梯度下降的过程"><a href="#二、反向传播加速梯度下降的过程" class="headerlink" title="二、反向传播加速梯度下降的过程"></a>二、反向传播加速梯度下降的过程</h2><h3 id="1-Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）"><a href="#1-Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）" class="headerlink" title="1.Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）"></a>1.Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）</h3><h3 id="2-RMS-Prop：使波动大的维度参数更新慢，波动小的维度参数更新快"><a href="#2-RMS-Prop：使波动大的维度参数更新慢，波动小的维度参数更新快" class="headerlink" title="2.RMS Prop：使波动大的维度参数更新慢，波动小的维度参数更新快"></a>2.RMS Prop：使波动大的维度参数更新慢，波动小的维度参数更新快</h3><h3 id="3-Adam：将Momentum-和-RMS-Prop相结合"><a href="#3-Adam：将Momentum-和-RMS-Prop相结合" class="headerlink" title="3.Adam：将Momentum 和 RMS Prop相结合"></a>3.Adam：将Momentum 和 RMS Prop相结合</h3><a id="more"></a><p>下面是我的作业喽 ~</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E4%BD%9C%E4%B8%9A%E4%B8%83.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、对于训练集的处理&quot;&gt;&lt;a href=&quot;#一、对于训练集的处理&quot; class=&quot;headerlink&quot; title=&quot;一、对于训练集的处理&quot;&gt;&lt;/a&gt;一、对于训练集的处理&lt;/h2&gt;&lt;h3 id=&quot;1-Mini-Batch&quot;&gt;&lt;a href=&quot;#1-Mini-Batch&quot; class=&quot;headerlink&quot; title=&quot;1.Mini-Batch&quot;&gt;&lt;/a&gt;1.Mini-Batch&lt;/h3&gt;&lt;p&gt;每次处理训练集的一部分进行梯度下降，避免了遍历完整个训练集才完成一次参数更新&lt;/p&gt;
&lt;h3 id=&quot;2-Batch-Gradient-Descent&quot;&gt;&lt;a href=&quot;#2-Batch-Gradient-Descent&quot; class=&quot;headerlink&quot; title=&quot;2.Batch Gradient Descent&quot;&gt;&lt;/a&gt;2.Batch Gradient Descent&lt;/h3&gt;&lt;p&gt;每次处理完所有训练集进行梯度下降&lt;/p&gt;
&lt;h3 id=&quot;3-Stochastic-Gradient-Descent&quot;&gt;&lt;a href=&quot;#3-Stochastic-Gradient-Descent&quot; class=&quot;headerlink&quot; title=&quot;3.Stochastic Gradient Descent&quot;&gt;&lt;/a&gt;3.Stochastic Gradient Descent&lt;/h3&gt;&lt;p&gt;每次处理完一个样本就进行梯度下降&lt;/p&gt;
&lt;h2 id=&quot;二、反向传播加速梯度下降的过程&quot;&gt;&lt;a href=&quot;#二、反向传播加速梯度下降的过程&quot; class=&quot;headerlink&quot; title=&quot;二、反向传播加速梯度下降的过程&quot;&gt;&lt;/a&gt;二、反向传播加速梯度下降的过程&lt;/h2&gt;&lt;h3 id=&quot;1-Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）&quot;&gt;&lt;a href=&quot;#1-Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）&quot; class=&quot;headerlink&quot; title=&quot;1.Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）&quot;&gt;&lt;/a&gt;1.Momentum：将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）&lt;/h3&gt;&lt;h3 id=&quot;2-RMS-Prop：使波动大的维度参数更新慢，波动小的维度参数更新快&quot;&gt;&lt;a href=&quot;#2-RMS-Prop：使波动大的维度参数更新慢，波动小的维度参数更新快&quot; class=&quot;headerlink&quot; title=&quot;2.RMS Prop：使波动大的维度参数更新慢，波动小的维度参数更新快&quot;&gt;&lt;/a&gt;2.RMS Prop：使波动大的维度参数更新慢，波动小的维度参数更新快&lt;/h3&gt;&lt;h3 id=&quot;3-Adam：将Momentum-和-RMS-Prop相结合&quot;&gt;&lt;a href=&quot;#3-Adam：将Momentum-和-RMS-Prop相结合&quot; class=&quot;headerlink&quot; title=&quot;3.Adam：将Momentum 和 RMS Prop相结合&quot;&gt;&lt;/a&gt;3.Adam：将Momentum 和 RMS Prop相结合&lt;/h3&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning 吴恩达 课程笔记大纲</title>
    <link href="http://yoursite.com/2018/06/27/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%A4%A7%E7%BA%B2/"/>
    <id>http://yoursite.com/2018/06/27/Deep-Learning-吴恩达-课程笔记大纲/</id>
    <published>2018-06-27T02:59:57.000Z</published>
    <updated>2018-07-22T03:13:28.242Z</updated>
    
    <content type="html"><![CDATA[<p>PS：Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization和Neural Networks and Deep Learning 两门课的课程笔记大纲梳理，我用我觉得的理解进行归类，方面以后查找 ~</p><a id="more"></a><h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><p><br></p><h2 id="一、以神经网络的结构看逻辑回归"><a href="#一、以神经网络的结构看逻辑回归" class="headerlink" title="一、以神经网络的结构看逻辑回归"></a>一、以神经网络的结构看逻辑回归</h2><ol><li>sigmoid函数</li><li>loss function 损失函数和 cost funcrion 代价函数</li><li>Gradient Descent 梯度下降法</li></ol><p><br></p><h2 id="二、向量化（Vectorization）-对应到硬件就是并行计算"><a href="#二、向量化（Vectorization）-对应到硬件就是并行计算" class="headerlink" title="二、向量化（Vectorization）(对应到硬件就是并行计算)"></a>二、向量化（Vectorization）(对应到硬件就是并行计算)</h2><p><br></p><h2 id="三、广播（broadcasting）"><a href="#三、广播（broadcasting）" class="headerlink" title="三、广播（broadcasting）"></a>三、广播（broadcasting）</h2><p><br></p><p><br></p><h1 id="浅层神经网络"><a href="#浅层神经网络" class="headerlink" title="浅层神经网络"></a>浅层神经网络</h1><p><br></p><h2 id="一、计算神经网络的输出-数学角度"><a href="#一、计算神经网络的输出-数学角度" class="headerlink" title="一、计算神经网络的输出 (数学角度)"></a>一、计算神经网络的输出 (数学角度)</h2><p><br></p><h2 id="二、向量化实现-代码实现"><a href="#二、向量化实现-代码实现" class="headerlink" title="二、向量化实现 (代码实现)"></a>二、向量化实现 (代码实现)</h2><p><br></p><h2 id="三、激活函数"><a href="#三、激活函数" class="headerlink" title="三、激活函数"></a>三、激活函数</h2><p><br></p><h2 id="四、神经网络的反向梯度下降-数学和代码"><a href="#四、神经网络的反向梯度下降-数学和代码" class="headerlink" title="四、神经网络的反向梯度下降(数学和代码)"></a>四、神经网络的反向梯度下降(数学和代码)</h2><p><br></p><h2 id="五、参数随机初始化"><a href="#五、参数随机初始化" class="headerlink" title="五、参数随机初始化"></a>五、参数随机初始化</h2><p><br></p><p><br></p><h1 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h1><h2 id="一、使用深层表示的原因"><a href="#一、使用深层表示的原因" class="headerlink" title="一、使用深层表示的原因"></a>一、使用深层表示的原因</h2><p><br></p><h2 id="二、搭建深层神经网络块"><a href="#二、搭建深层神经网络块" class="headerlink" title="二、搭建深层神经网络块"></a>二、搭建深层神经网络块</h2><p><br></p><h2 id="三、矩阵的维度"><a href="#三、矩阵的维度" class="headerlink" title="三、矩阵的维度"></a>三、矩阵的维度</h2><p><br></p><p><br></p><hr><h1 id="优化（一）-—-过拟合"><a href="#优化（一）-—-过拟合" class="headerlink" title="优化（一） — 过拟合"></a>优化（一） — 过拟合</h1><p><br></p><h2 id="一、数据划分：训练-验证-测试集（训练之前）"><a href="#一、数据划分：训练-验证-测试集（训练之前）" class="headerlink" title="一、数据划分：训练 / 验证 / 测试集（训练之前）"></a>一、数据划分：训练 / 验证 / 测试集（训练之前）</h2><p><br></p><p><br></p><h2 id="二、模型估计：偏差-方差（训练之后，对训练结果的评估）"><a href="#二、模型估计：偏差-方差（训练之后，对训练结果的评估）" class="headerlink" title="二、模型估计：偏差 / 方差（训练之后，对训练结果的评估）"></a>二、模型估计：偏差 / 方差（训练之后，对训练结果的评估）</h2><p><br></p><h3 id="1-高偏差的解决："><a href="#1-高偏差的解决：" class="headerlink" title="1.高偏差的解决："></a>1.<strong>高偏差</strong>的解决：</h3><p>（1）扩大网络规模，如添加隐藏层或隐藏单元数目<br>（2）寻找合适的网络架构，使用更大的 NN 结构<br>（3）花费更长时间训练，增加迭代次数</p><p><br></p><h3 id="2-高方差的解决："><a href="#2-高方差的解决：" class="headerlink" title="2.高方差的解决："></a>2.<strong>高方差</strong>的解决：</h3><p>（1）获取更多的数据<br>（2）正则化（regularization），防止过拟合<br>（3）寻找更合适的网络结构</p><p><br></p><p><br></p><h2 id="三、解决高方差的方法：正则化（regularization），防止过拟合（对于训练结果不理想的解决方法）"><a href="#三、解决高方差的方法：正则化（regularization），防止过拟合（对于训练结果不理想的解决方法）" class="headerlink" title="三、解决高方差的方法：正则化（regularization），防止过拟合（对于训练结果不理想的解决方法）"></a>三、解决高方差的方法：正则化（regularization），防止过拟合（对于训练结果不理想的解决方法）</h2><p><br></p><h3 id="1-L2"><a href="#1-L2" class="headerlink" title="1. L2"></a>1. <strong>L2</strong></h3><p>加入的正则化项，衰减了W的权重</p><p><br></p><h3 id="2-dropout"><a href="#2-dropout" class="headerlink" title="2. dropout"></a>2. <strong>dropout</strong></h3><p>神经元不会特别依赖于任何一个输入特征。反向随机失活（Inverted dropout）是实现 dropout 的方法。</p><p><br></p><h3 id="3-数据扩增（Data-Augmentation）"><a href="#3-数据扩增（Data-Augmentation）" class="headerlink" title="3.数据扩增（Data Augmentation）"></a>3.数据扩增（Data Augmentation）</h3><p><br></p><h3 id="4-早停止法（Early-Stopping）"><a href="#4-早停止法（Early-Stopping）" class="headerlink" title="4.早停止法（Early Stopping）"></a>4.早停止法（Early Stopping）</h3><p><br></p><p><br></p><h1 id="优化（二）—-加快学习的算法"><a href="#优化（二）—-加快学习的算法" class="headerlink" title="优化（二）— 加快学习的算法"></a>优化（二）— 加快学习的算法</h1><p><br></p><h2 id="一、对于训练集的处理"><a href="#一、对于训练集的处理" class="headerlink" title="一、对于训练集的处理"></a>一、对于训练集的处理</h2><p><br></p><h3 id="Mini-Batch"><a href="#Mini-Batch" class="headerlink" title="Mini-Batch"></a>Mini-Batch</h3><p>每次处理训练集的一部分进行梯度下降，避免了遍历完整个训练集才完成一次参数更新</p><p><br></p><p><br></p><h2 id="二、初始化参数W、b"><a href="#二、初始化参数W、b" class="headerlink" title="二、初始化参数W、b"></a>二、初始化参数W、b</h2><p><br></p><p>在参数W，b初始化时就进行优化，应对反向传播时会出现的<strong>梯度消失和梯度爆炸</strong>（vanishing/exploding gradient）问题</p><h3 id="1-He-Initialization"><a href="#1-He-Initialization" class="headerlink" title="1.He Initialization"></a>1.He Initialization</h3><p>激活函数使用 ReLU 时，Var(wi)=2/n[l-1] He Initialization</p><p><br></p><h3 id="2-Xavier-initialization"><a href="#2-Xavier-initialization" class="headerlink" title="2.Xavier initialization"></a>2.Xavier initialization</h3><p>激活函数使用 tanh 时，Var(wi)=1/n[l-1] Xavier initialization</p><p><br></p><p><br></p><h2 id="三、反向传播加速梯度下降的过程"><a href="#三、反向传播加速梯度下降的过程" class="headerlink" title="三、反向传播加速梯度下降的过程"></a>三、反向传播加速梯度下降的过程</h2><h3 id="1-Momentum"><a href="#1-Momentum" class="headerlink" title="1.Momentum"></a>1.Momentum</h3><p>将前几次梯度下降的指数平均作为梯度值来更新参数（避免震荡）</p><p><br></p><h3 id="2-RMS-Prop"><a href="#2-RMS-Prop" class="headerlink" title="2.RMS Prop"></a>2.RMS Prop</h3><p>使波动大的维度参数更新慢，波动小的维度参数更新快</p><p><br></p><h3 id="3-Adam"><a href="#3-Adam" class="headerlink" title="3.Adam"></a>3.Adam</h3><p>将Momentum 和 RMS Prop相结合</p><p><br></p><p><br></p><h2 id="四、改变学习率加速收敛"><a href="#四、改变学习率加速收敛" class="headerlink" title="四、改变学习率加速收敛"></a>四、改变学习率加速收敛</h2><p>学习率衰减：初期学习率大，梯度下降快，后期学习步长小，有助于收敛至最优解</p><p><br></p><p><br></p><h2 id="五、标准化、归一化-Normalization"><a href="#五、标准化、归一化-Normalization" class="headerlink" title="五、标准化、归一化 Normalization"></a>五、标准化、归一化 Normalization</h2><p><br></p><h3 id="1-标准化输入（Normalization）"><a href="#1-标准化输入（Normalization）" class="headerlink" title="1.标准化输入（Normalization）"></a>1.标准化输入（Normalization）</h3><p>对于输入特征标准化，加快学习速度</p><p><br></p><h3 id="2-Batch-Normalization"><a href="#2-Batch-Normalization" class="headerlink" title="2.Batch Normalization"></a>2.Batch Normalization</h3><ol><li><p>通过对隐藏层各神经元的输入标准化，<strong>提高神经网络训练速度</strong></p></li><li><p>减小不同的样本分布（<strong>Covariate Shift</strong> ）对后面几层网络的影响。样本分布改变，使得前面层的权重变化后，输出Z发生很大变化，但是在Batch Normalization作用下，归一化后的Z分布相比于前一次迭代的归一化的Z，并没有发生很大变化，所以作为这一层的输出，并不会对后面一层的输入的分布产生很大影响，从而减少各层 W、b 之间的耦合性，<strong>增强鲁棒性（robust）</strong></p><p>（这里表扬一下自己，感觉自己太厉害了，竟然在厕所里想通了Covariate Shift 的意思，哈哈哈~）</p></li><li><p>Batch Normalization 也起到<strong>微弱的正则化</strong>（regularization）效果</p></li></ol><p><br></p><p>PS：我老是把Normalization和Regularization弄混淆 ~</p><p><br></p><p><br></p><h1 id="超参数调试"><a href="#超参数调试" class="headerlink" title="超参数调试"></a>超参数调试</h1><p><br></p><h3 id="1-超参数重要程度排序"><a href="#1-超参数重要程度排序" class="headerlink" title="1.超参数重要程度排序"></a>1.超参数重要程度排序</h3><ul><li><p>最重要：</p><p>学习率 α</p></li><li><p>其次重要：</p><p>β：动量衰减参数，常设置为 0.9</p><p>hidden units：各隐藏层神经元个数</p><p>mini-batch 的大小</p></li><li><p>再次重要：</p><p>β1，β2，ϵ：Adam 优化算法的超参数，常设为 0.9、0.999、10−810−8</p><p>layers：神经网络层数</p><p>decay_rate：学习衰减率</p></li></ul><p><br></p><h3 id="2-调参技巧"><a href="#2-调参技巧" class="headerlink" title="2.调参技巧"></a>2.调参技巧</h3><p>随机选择点进行调试</p><p><br></p><h3 id="3-为超参数选择合适调试范围"><a href="#3-为超参数选择合适调试范围" class="headerlink" title="3.为超参数选择合适调试范围"></a>3.为超参数选择合适调试范围</h3><p><br></p><p><br></p><h1 id="优化会遇到的问题"><a href="#优化会遇到的问题" class="headerlink" title="优化会遇到的问题"></a>优化会遇到的问题</h1><h2 id="一、判断梯度计算是否正确"><a href="#一、判断梯度计算是否正确" class="headerlink" title="一、判断梯度计算是否正确"></a>一、判断梯度计算是否正确</h2><p>由于微分分析计算梯度容易出错 ，实际操作时常常<strong>将分析梯度法的结果和数值梯度法的结果作比较</strong>，以此来<strong>检查其实现的正确性</strong></p><p>注意：只适用于debug，判断梯度计算是否正确，且不能和dropout同时使用</p><p><br></p><h2 id="二、高维度优化遇到的问题"><a href="#二、高维度优化遇到的问题" class="headerlink" title="二、高维度优化遇到的问题"></a>二、高维度优化遇到的问题</h2><p>1.低维度：局部最优会阻碍找到全局最优的解</p><p>2.高维度：不存在局部最优的阻碍，但是<strong>鞍点</strong>会降低学习速度，通过Adama算法加速提高效率</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization和Neural Networks and Deep Learning 两门课的课程笔记大纲梳理，我用我觉得的理解进行归类，方面以后查找 ~&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业（六）</title>
    <link href="http://yoursite.com/2018/06/25/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E5%85%AD%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/25/Deep-Learning-吴恩达-作业（六）/</id>
    <published>2018-06-25T12:49:20.000Z</published>
    <updated>2018-06-26T01:59:02.890Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h1><p>PS：用双边求导的结果与梯度的逼近程度来检查梯度是否计算错误，需要对每个神经元的W、b进行计算</p><a id="more"></a><h2 id="1-双边求导"><a href="#1-双边求导" class="headerlink" title="1.双边求导"></a>1.双边求导</h2><p><img src="http://p8ge6t5tt.bkt.clouddn.com/gradient%20checking1.JPG" alt=""></p><h2 id="2-逼近程度的判断"><a href="#2-逼近程度的判断" class="headerlink" title="2.逼近程度的判断"></a>2.逼近程度的判断</h2><p><img src="http://p8ge6t5tt.bkt.clouddn.com/gradient%20checking2.JPG" alt=""></p><h2 id="最后贴上我的作业"><a href="#最后贴上我的作业" class="headerlink" title="最后贴上我的作业"></a>最后贴上我的作业</h2><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E4%BD%9C%E4%B8%9A%E5%85%AD.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Gradient-Checking&quot;&gt;&lt;a href=&quot;#Gradient-Checking&quot; class=&quot;headerlink&quot; title=&quot;Gradient Checking&quot;&gt;&lt;/a&gt;Gradient Checking&lt;/h1&gt;&lt;p&gt;PS：用双边求导的结果与梯度的逼近程度来检查梯度是否计算错误，需要对每个神经元的W、b进行计算&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业（五）</title>
    <link href="http://yoursite.com/2018/06/25/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/25/Deep-Learning-吴恩达-作业（五）/</id>
    <published>2018-06-25T09:37:43.000Z</published>
    <updated>2018-07-26T12:31:26.494Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><p>PS：要明确的是正则化是对过拟合现象的修正，不是必须要做的</p><a id="more"></a><h2 id="1-L2-Regularization"><a href="#1-L2-Regularization" class="headerlink" title="1. L2 Regularization"></a>1. L2 Regularization</h2><p>1.每前向传播完成后计算 cost function 需要加上正则化项</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/regulation5.JPG" alt=""></p><p>2.反向传播计算dW时也需要加上正则化项</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/regulation6.JPG" alt=""></p><p>3.权重W变为较小的值，从而防止过拟合</p><h2 id="2-Dropout"><a href="#2-Dropout" class="headerlink" title="2. Dropout"></a>2. Dropout</h2><p>1.将前一层的输出同时是这一层的输入A[l]，以一定概率keep_prob丢弃，再将剩余的 <strong>A[l]除以keep_prob</strong>  (而不影响下一层的计算)</p><p>2.反向传播时，对于 dA[l] 进行同样的丢弃和缩放</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E4%BD%9C%E4%B8%9A%E4%BA%94.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Regularization&quot;&gt;&lt;a href=&quot;#Regularization&quot; class=&quot;headerlink&quot; title=&quot;Regularization&quot;&gt;&lt;/a&gt;Regularization&lt;/h1&gt;&lt;p&gt;PS：要明确的是正则化是对过拟合现象的修正，不是必须要做的&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业——关于plot_decision_boundary()函数的理解</title>
    <link href="http://yoursite.com/2018/06/25/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8Eplot-decision-boundary-%E5%87%BD%E6%95%B0%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>http://yoursite.com/2018/06/25/Deep-Learning-吴恩达-作业——关于plot-decision-boundary-函数的理解/</id>
    <published>2018-06-25T08:24:17.000Z</published>
    <updated>2018-06-25T12:32:11.643Z</updated>
    
    <content type="html"><![CDATA[<p>PS：看了一下午这个函数，真的是要把我高抓狂了，总算知道它在做什么了 ~ 哎，真累啊</p><a id="more"></a><p>1.得到最终的预测结果0或者1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#得到最终的预测结果0或者1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_dec</span><span class="params">(parameters, X)</span></span></span><br><span class="line"><span class="function">    """</span></span><br><span class="line"><span class="function">    <span class="title">Used</span> <span class="title">for</span> <span class="title">plotting</span> <span class="title">decision</span> <span class="title">boundary</span>.</span></span><br><span class="line"><span class="function">    </span></span><br><span class="line"><span class="function">    <span class="title">Arguments</span>:</span></span><br><span class="line">    parameters -- python dictionary containing your parameters </span><br><span class="line">    X -- input data of size (m, K)</span><br><span class="line">    </span><br><span class="line">    Returns</span><br><span class="line">    predictions -- vector of predictions of our model (red: <span class="number">0</span> / blue: <span class="number">1</span>)</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    # Predict using forward propagation and a classification threshold of 0.5</span></span><br><span class="line"><span class="string">    a3, cache = forward_propagation(X, parameters)</span></span><br><span class="line"><span class="string">    predictions = (a3&gt;0.5)</span></span><br><span class="line"><span class="string">    return predictions</span></span><br></pre></td></tr></table></figure><p>2.划分格子-得到每个格点的坐标-对每个格点的值进行预测(作为颜色区分)-画边界线-加上训练数据的散点图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#划分格子-得到每个格点的坐标-对每个格点的值进行预测(作为颜色区分)-画边界线-加上训练数据的散点图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># Set min and max values and give it some padding #生成网格x、y的范围，不是整幅图的范围</span></span><br><span class="line">    x_min, x_max = X[<span class="number">0</span>, :].min() - <span class="number">1</span>, X[<span class="number">0</span>, :].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[<span class="number">1</span>, :].min() - <span class="number">1</span>, X[<span class="number">1</span>, :].max() + <span class="number">1</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Generate a grid of points with distance h between them #生成网格矩阵</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))   </span><br><span class="line">    <span class="comment">#画格子图理解：1.xx返回每个格点的x坐标  2.yy返回的是每个格点的y坐标</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Predict the function value for the whole grid</span></span><br><span class="line">    Z = model(np.c_[xx.ravel(), yy.ravel()]) </span><br><span class="line">    <span class="comment">#model()括号里的参数是每个格点的坐标  #model的功能得到对应格点的预测结果 0或者1</span></span><br><span class="line">    Z = Z.reshape(xx.shape)   </span><br><span class="line">    <span class="comment"># Plot the contour and training examples</span></span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) <span class="comment">#找边界线</span></span><br><span class="line">    plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :],  c=np.squeeze(y), cmap=plt.cm.Spectral)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#调用函数</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><h2 id="1-lambda函数-匿名函数"><a href="#1-lambda函数-匿名函数" class="headerlink" title="1. lambda函数-匿名函数"></a>1. lambda函数-匿名函数</h2><p>匿名函数冒号前面的x表示函数参数。匿名函数有个限制，就是<strong>只能有一个表达式</strong>，不用写return，<strong>返回值就是该表达式的结果</strong></p><p><code>list(map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9]))</code></p><p><code>[1, 4, 9, 16, 25, 36, 49, 64, 81]</code></p><h2 id="2-np-c-和-np-r"><a href="#2-np-c-和-np-r" class="headerlink" title="2. np.c_ 和 np.r_"></a>2. np.c_ 和 np.r_</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([1, 2, 3])</span><br><span class="line">b = np.array([4, 5, 6])</span><br><span class="line"></span><br><span class="line">c=np.c_[a,b]</span><br><span class="line">d=np.r_[a,b]</span><br></pre></td></tr></table></figure><p>我还是没有理解，简直喵了个咪的抓狂 ~</p><h2 id="3-np-ravel和np-flatten"><a href="#3-np-ravel和np-flatten" class="headerlink" title="3. np.ravel和np.flatten"></a>3. np.ravel和np.flatten</h2><p>相同点：将多维数组降位一维      </p><p>不同点：          </p><ul><li>numpy.flatten()返回的是拷贝，不会影响原始矩阵        </li><li>numpy.ravel()返回的是view,通过ravel修改了矩阵，原来的矩阵会发生改变</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：看了一下午这个函数，真的是要把我高抓狂了，总算知道它在做什么了 ~ 哎，真累啊&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业（四）</title>
    <link href="http://yoursite.com/2018/06/24/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/24/Deep-Learning-吴恩达-作业（四）/</id>
    <published>2018-06-24T07:46:03.000Z</published>
    <updated>2018-06-25T12:17:46.018Z</updated>
    
    <content type="html"><![CDATA[<h2 id="initialization"><a href="#initialization" class="headerlink" title="initialization"></a>initialization</h2><p>PS：参数初始化是为了缓解梯度消失和爆炸问题，加快梯度下降从而加快学习的速度</p><p>下面是三种不同的参数W、b的初始化方式，对于多次迭代后 cost function下降效果的影响</p><a id="more"></a><p><img src="http://p8ge6t5tt.bkt.clouddn.com/initization%E4%BD%9C%E4%B8%9A%E5%9B%9B.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;initialization&quot;&gt;&lt;a href=&quot;#initialization&quot; class=&quot;headerlink&quot; title=&quot;initialization&quot;&gt;&lt;/a&gt;initialization&lt;/h2&gt;&lt;p&gt;PS：参数初始化是为了缓解梯度消失和爆炸问题，加快梯度下降从而加快学习的速度&lt;/p&gt;
&lt;p&gt;下面是三种不同的参数W、b的初始化方式，对于多次迭代后 cost function下降效果的影响&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习和强化学习</title>
    <link href="http://yoursite.com/2018/06/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/06/21/深度学习和强化学习/</id>
    <published>2018-06-21T07:06:39.000Z</published>
    <updated>2018-06-21T07:07:12.516Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络是个模型，深度学习是一种方法。深度学习可以理解成用深度神经网络来进行机器学习，可以有效解决层数多的网络不好学习的问题。</p><a id="more"></a><h2 id="深度学习（Deep-Learning）"><a href="#深度学习（Deep-Learning）" class="headerlink" title="深度学习（Deep Learning）"></a>深度学习（Deep Learning）</h2><p><strong>DNN （深度神经网络）</strong>将原始信号（例如RGB像素值）<strong>直接</strong>输入DNN，而不需要创建任何域特定的输入功能。通过多层神经元，DNN可以“自动”通过每一层产生适当的特征，最后提供一个非常好的预测。这极大地消除了寻找”特征工程”的麻烦。</p><p>DNN也演变成许多不同的网络拓扑结构，CNN（卷积神经网络），RNN（递归神经网络），LSTM（长期短期记忆），GAN（生成敌对网络），转移学习（Transfer Learning），注意模型（attention model）。</p><p>深度学习模型：<a href="https://www.zhihu.com/question/38679133" target="_blank" rel="noopener">https://www.zhihu.com/question/38679133</a></p><h2 id="强化学习（Reinforcement-Learning）"><a href="#强化学习（Reinforcement-Learning）" class="headerlink" title="强化学习（Reinforcement Learning）"></a>强化学习（Reinforcement Learning）</h2><p>强化学习有四个要素，环境模型、Agent（学习者）、回报函数、策略。其实就是一种<strong>奖惩机制</strong>，奖励那些适应性好的学习，惩罚那些适应性不好的学习。强化学习学习得非常快，因为每一个新的反馈都会影响随后的决定。</p><h2 id="深度学习与强化学习的区别"><a href="#深度学习与强化学习的区别" class="headerlink" title="深度学习与强化学习的区别"></a>深度学习与强化学习的区别</h2><p>深度学习的学习过程是静态的，强化学习的学习过程是动态的。静态与动态的区别在于是否会与环境进行交互，深度学习是给什么样本就学什么，而强化学习是要和环境进行交互，再通过环境给出的奖惩来学习。<strong>深度学习解决的更多是感知问题，强化学习解决的主要是决策问题</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;神经网络&quot;&gt;&lt;a href=&quot;#神经网络&quot; class=&quot;headerlink&quot; title=&quot;神经网络&quot;&gt;&lt;/a&gt;神经网络&lt;/h2&gt;&lt;p&gt;神经网络是个模型，深度学习是一种方法。深度学习可以理解成用深度神经网络来进行机器学习，可以有效解决层数多的网络不好学习的问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-吴恩达-作业（三）</title>
    <link href="http://yoursite.com/2018/06/20/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/20/Deep-Learning-吴恩达-作业（三）/</id>
    <published>2018-06-20T12:54:48.000Z</published>
    <updated>2018-06-21T13:55:21.231Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hdf5文件"><a href="#hdf5文件" class="headerlink" title="hdf5文件"></a>hdf5文件</h2><p>PS：第一次接触这种文件格式，可以参考下面两篇博文快速知道其存储形式 ~</p><p><a href="https://blog.csdn.net/yudf2010/article/details/50353292" target="_blank" rel="noopener">https://blog.csdn.net/yudf2010/article/details/50353292</a></p><p><a href="http://docs.h5py.org/en/latest/quick.html#quick" target="_blank" rel="noopener">http://docs.h5py.org/en/latest/quick.html#quick</a></p><p>h5py文件是存放两类对象的容器</p><ul><li>数据集(dataset)：像数组类的数据集合，和numpy的数组差不多。</li></ul><ul><li>组(group)：像文件夹一样的容器，类似python中的 dict，有键(key)和值(value)。group中可以存放dataset或者其他的group。</li></ul><a id="more"></a><h3 id="1-查看h5文件中的key"><a href="#1-查看h5文件中的key" class="headerlink" title="1. 查看h5文件中的key"></a>1. 查看h5文件中的key</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#h5文件的读取  </span></span><br><span class="line">f_train = h5py.File(<span class="string">'D:/机器学习/深度学习神经网络/作业三/datasets/train_catvnoncat.h5'</span>,<span class="string">'r'</span>)   <span class="comment">#打开h5文件  </span></span><br><span class="line"><span class="comment"># 可以查看所有的主键  </span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> f_train.keys():  </span><br><span class="line">    print(f_train[key].name)   <span class="comment">#名字</span></span><br><span class="line">    print(f_train[key].shape)  <span class="comment">#大小</span></span><br><span class="line">    print(f_train[key].value)  <span class="comment">#数据</span></span><br></pre></td></tr></table></figure><h3 id="2-key的打印结果"><a href="#2-key的打印结果" class="headerlink" title="2. key的打印结果"></a>2. key的打印结果</h3><p>train_catvnoncat.h5 和 test_catvnoncat.h5 中key的打印结果一样，train中样本209，test中样本50</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">/list_classes</span><br><span class="line">(2,)</span><br><span class="line">[b&apos;non-cat&apos; b&apos;cat&apos;]</span><br><span class="line">/train_set_x</span><br><span class="line">(209, 64, 64, 3)</span><br><span class="line">[[[[ 17  31  56]</span><br><span class="line">   [ 22  33  59]</span><br><span class="line">   [ 25  35  62]</span><br><span class="line">   ...</span><br><span class="line">   [  0   0   0]</span><br><span class="line">   [  0   0   0]</span><br><span class="line">   [  0   0   0]]]]</span><br><span class="line">/train_set_y</span><br><span class="line">(209,)</span><br><span class="line">[0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0</span><br><span class="line"> 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1</span><br><span class="line"> 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0</span><br><span class="line"> 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0</span><br><span class="line"> 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]</span><br></pre></td></tr></table></figure><h3 id="3-查看key的数据类型"><a href="#3-查看key的数据类型" class="headerlink" title="3. 查看key的数据类型"></a>3. 查看key的数据类型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f_train = h5py.File(<span class="string">'D:/机器学习/深度学习神经网络/作业三/datasets/train_catvnoncat.h5'</span>,<span class="string">'r'</span>)   </span><br><span class="line">train_x_orig = f_train[<span class="string">'train_set_x'</span>]</span><br><span class="line">train_y = f_train[<span class="string">'train_set_y'</span>]</span><br><span class="line">classes = f_train[<span class="string">'list_classes'</span>]</span><br><span class="line">print(train_x_orig.dtype)</span><br><span class="line">print(train_y.dtype)</span><br></pre></td></tr></table></figure><ul><li>list_classes ：uint8</li><li>train_set_x ：int64 应该是64* 64* 3大小的图片</li><li>train_set_y ：|S7 好像是Matlab里的v7.3格式，不知道啊 ~</li></ul><p>PS：下面是两个两个编程作业~</p><h2 id="两层的神经网络"><a href="#两层的神经网络" class="headerlink" title="两层的神经网络"></a>两层的神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###########两层的神经网络 LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID 识别是否有猫</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment">###########加载数据</span></span><br><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data() <span class="comment">#dnn_app_utils中的函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将三个通道合为一列，标准化至0~1</span></span><br><span class="line"><span class="comment"># Reshape the training and test examples</span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T  <span class="comment"># The "-1" makes reshape flatten the remaining dimensions </span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x's shape: "</span> + str(train_x.shape)) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x's shape: "</span> + str(test_x.shape))</span><br><span class="line"><span class="comment">###########加载数据</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###########两层的神经网络参数训练 LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID</span></span><br><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># num_px * num_px * 3</span></span><br><span class="line">n_h = <span class="number">7</span>         <span class="comment"># 隐藏层神经元个数</span></span><br><span class="line">n_y = <span class="number">1</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- If set to True, this will print the cost every 100 iterations </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary containing W1, W2, b1, and b2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                              <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                           <span class="comment"># number of examples</span></span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary, by calling one of the functions you'd previously implemented</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y) <span class="comment">#初始化参数</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get W1, b1, W2 and b2 from the dictionary parameters.</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: "X, W1, b1, W2, b2". Output: "A1, cache1, A2, cache2".</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, <span class="string">'relu'</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, <span class="string">'sigmoid'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initializing backward propagation</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Backward propagation. Inputs: "dA2, cache2, cache1". Outputs: "dA1, dW2, db2; also dA0 (not used), dW1, db1".</span></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, <span class="string">'sigmoid'</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, <span class="string">'relu'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2</span></span><br><span class="line">        grads[<span class="string">'dW1'</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve W1, b1, W2, b2 from parameters</span></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>: <span class="comment">#每迭代100次记录cost的值</span></span><br><span class="line">            costs.append(cost)</span><br><span class="line">       </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"><span class="comment">###########两层的神经网络</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算样本准确率</span></span><br><span class="line">predictions_train = predict(train_x, train_y, parameters) <span class="comment">#训练集准确率为1</span></span><br><span class="line">predictions_test = predict(test_x, test_y, parameters) <span class="comment">#测试集准确率为0.72</span></span><br></pre></td></tr></table></figure><p>训练过程中cost function的值的下降</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn3.JPG" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn4.JPG" alt=""></p><h2 id="四层的神经网络"><a href="#四层的神经网络" class="headerlink" title="四层的神经网络"></a>四层的神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###########四层的神经网络 LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID 识别是否有猫</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment">###########加载数据</span></span><br><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data() <span class="comment">#dnn_app_utils中的函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将三个通道合为一列，标准化至0~1</span></span><br><span class="line"><span class="comment"># Reshape the training and test examples</span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T  <span class="comment"># The "-1" makes reshape flatten the remaining dimensions </span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span></span><br><span class="line"><span class="comment">#print ("train_x's shape: " + str(train_x.shape)) </span></span><br><span class="line"><span class="comment">#print ("test_x's shape: " + str(test_x.shape))</span></span><br><span class="line"><span class="comment">###########加载数据</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###########四层神经网络参数训练，每层的神经元个数 LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID</span></span><br><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>] <span class="comment">#  4-layer model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span><span class="comment">#lr was 0.009</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    costs = []                         <span class="comment"># keep track of cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Parameters initialization. (≈ 1 line of code)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        AL, caches =  L_model_forward(X, parameters)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cost.</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line"></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">                </span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show() <span class="comment">#这里要注意的一个问题是：程序会一直show这张图而不往下执行</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"><span class="comment">###########四层神经网络</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算样本准确率</span></span><br><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="keyword">True</span>)</span><br><span class="line">predictions_train = predict(train_x, train_y, parameters) <span class="comment">#训练集准确率为0.98</span></span><br><span class="line">predictions_test = predict(test_x, test_y, parameters) <span class="comment">#测试集准确率为0.8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#打印分类错误的图片</span></span><br><span class="line">print_mislabeled_images(classes, test_x, test_y, pred_test)</span><br><span class="line"></span><br><span class="line"><span class="string">''' 测试自己的图片</span></span><br><span class="line"><span class="string">my_image = "cat1.jpg" </span></span><br><span class="line"><span class="string">my_label_y = [1] # the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">fname = "images/" + my_image</span></span><br><span class="line"><span class="string">image = np.array(ndimage.imread(fname, flatten=False))</span></span><br><span class="line"><span class="string">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))</span></span><br><span class="line"><span class="string">my_image = my_image/255.</span></span><br><span class="line"><span class="string">my_predicted_image = predict(my_image, my_label_y, parameters)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">plt.imshow(image)</span></span><br><span class="line"><span class="string">print ("y = " + str(np.squeeze(my_predicted_image)) + ", your L-layer model predicts a \""</span></span><br><span class="line"><span class="string">       + classes[int(np.squeeze(my_predicted_image)),].decode("utf-8") +  "\" picture.")</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p>训练过程中cost function的值的下降</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn5.JPG" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn6.JPG" alt=""></p><p>看了一下资源消耗，用的是CPU和内存，可能用不同的框架，用的资源不一样。可以将数据从硬盘读进内存，输入CPU计算。或者将数据从硬盘读入显存，用GPU计算。</p><h2 id="待解决的问题"><a href="#待解决的问题" class="headerlink" title="待解决的问题"></a>待解决的问题</h2><p>我现在最大的疑惑就是，怎么把训练好的网络参数储存起来，以什么格式储存？后面要检测图片中是否有猫的时候，直接将参数取来用，不需要从头再计算一遍参数。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;hdf5文件&quot;&gt;&lt;a href=&quot;#hdf5文件&quot; class=&quot;headerlink&quot; title=&quot;hdf5文件&quot;&gt;&lt;/a&gt;hdf5文件&lt;/h2&gt;&lt;p&gt;PS：第一次接触这种文件格式，可以参考下面两篇博文快速知道其存储形式 ~&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yudf2010/article/details/50353292&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/yudf2010/article/details/50353292&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://docs.h5py.org/en/latest/quick.html#quick&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://docs.h5py.org/en/latest/quick.html#quick&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;h5py文件是存放两类对象的容器&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据集(dataset)：像数组类的数据集合，和numpy的数组差不多。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;组(group)：像文件夹一样的容器，类似python中的 dict，有键(key)和值(value)。group中可以存放dataset或者其他的group。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning 吴恩达 作业（二）</title>
    <link href="http://yoursite.com/2018/06/20/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/20/Deep-Learning-吴恩达-作业（二）/</id>
    <published>2018-06-20T12:01:53.000Z</published>
    <updated>2018-06-25T07:02:05.101Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Building-your-Deep-Neural-Network-Step-by-Step"><a href="#Building-your-Deep-Neural-Network-Step-by-Step" class="headerlink" title="Building your Deep Neural Network: Step by Step"></a>Building your Deep Neural Network: Step by Step</h2><p>L层神经网络：前L-1层的激励函数是ReLU，输出层激励函数是sigmod</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Building your Deep Neural Network: Step by Step</span></span><br><span class="line"><span class="comment">#前L-1层的激励函数是ReLU,输出层激励函数是sigmod</span></span><br><span class="line"><span class="comment">#参数存储需要看清楚</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v4 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化每一层的参数 W、b</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network 每一层神经元的个数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parameters = &#123;&#125;                <span class="comment"># dict类型数据</span></span><br><span class="line">    L = len(layer_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):          <span class="comment"># 0就是输入层</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l<span class="number">-1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 两种激励函数,计算当前层的A,返回cache：A_prev, W, b, Z</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span><br><span class="line"><span class="string">             stored for computing the backward pass efficiently    </span></span><br><span class="line"><span class="string">             dict数据类型</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        <span class="comment">##linear_cache (A_prev, W, b)</span></span><br><span class="line">        <span class="comment">##activation_cache Z</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        Z, linear_cache = np.dot(W,A_prev)+b,(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        Z, linear_cache = np.dot(W,A_prev)+b,(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># L层的前向传播 输出AL(y-hat) caches(A_prev、W、b、Z)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- last post-activation value   最后输出的y-hat </span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []     <span class="comment">#存储A_prev、W、b、Z</span></span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="number">2</span>                  <span class="comment"># number of layers in the neural network //表示整数除</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU]*(L-1). Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L): <span class="comment">#for执行1~L-1</span></span><br><span class="line">        A_prev = A </span><br><span class="line">        A, cache = linear_activation_forward(A,  parameters[<span class="string">'W'</span> + str(l)],  parameters[<span class="string">'b'</span> + str(l)], <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement LINEAR -&gt; SIGMOID. Add "cache" to the "caches" list.</span></span><br><span class="line">    AL, cache = linear_activation_forward(A,  parameters[<span class="string">'W'</span> + str(L)],  parameters[<span class="string">'b'</span> + str(L)], <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># cost function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function defined by equation (7).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from aL and y.</span></span><br><span class="line">    cost = -np.sum(np.multiply(Y,np.log(AL))+ np.multiply(<span class="number">1</span>-Y,np.log(<span class="number">1</span>-AL)))/m</span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据dZ 计算 dA_prev, dW, db</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dW = np.dot(dZ,A_prev.T)/m</span><br><span class="line">    db = np.sum(dZ,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)/m <span class="comment">#同一行的相加 </span></span><br><span class="line">    dA_prev = np.dot(W.T,dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据dA计算dZ,dA_prev, dW, db</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></span><br><span class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    <span class="comment">#linear_cache：A_prev, W, b</span></span><br><span class="line">    <span class="comment">#activation_cache：Z</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#dZ=dA*g`(Z) 不同激励函数的导数不同</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache) <span class="comment">#dZ</span></span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache) <span class="comment">#根据dZ计算参数梯度 dA_prev, dW, db</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># L层反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ... </span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>] <span class="comment">#样本个数</span></span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment">#初始化起始dAL,第L层的dA</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "dAL, current_cache". Outputs: "grads["dAL-1"], grads["dWL"], grads["dbL"]</span></span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]  <span class="comment">#第l层的 A_prev，W，b，Z 放在caches[l-1]</span></span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class="string">"sigmoid"</span>) <span class="comment">#计算L层的参数,其对应的激励函数sigmoid</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop from l=L-2 to l=0</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)): </span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class="line">        <span class="comment"># Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l + <span class="number">2</span>)], current_cache, activation = <span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter. Use a for loop.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)]- learning_rate*grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)]- learning_rate*grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>PS：我做作业的时候，遇到最大的麻烦就是几个参数的上标，还有对应那一层的 cache</p><h3 id="1-下面是我整理的，程序对应的框图："><a href="#1-下面是我整理的，程序对应的框图：" class="headerlink" title="1.下面是我整理的，程序对应的框图："></a>1.下面是我整理的，程序对应的框图：</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn1.png" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn2.png" alt=""></p><h3 id="2-参数整理"><a href="#2-参数整理" class="headerlink" title="2.参数整理"></a>2.参数整理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">parameters  字典类型</span><br><span class="line">parameters[&apos;W&apos; + str(l)]</span><br><span class="line">parameters[&apos;b&apos; + str(l)]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">grads　字典类型 </span><br><span class="line">grads[&quot;dA&quot; + str(l)]  前一层的dA_prev</span><br><span class="line">grads[&quot;dW&quot; + str(l)] </span><br><span class="line">grads[&quot;db&quot; + str(l)]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cache 存储当前层的 A_prev，W，b，Z </span><br><span class="line">linear_cache ：A_prev, W, b</span><br><span class="line">activation_cache： Z</span><br><span class="line">cache = (linear_cache, activation_cache)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">caches 存储所有的 A_prev, W, b, Z (caches[l]对应l+1层)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Building-your-Deep-Neural-Network-Step-by-Step&quot;&gt;&lt;a href=&quot;#Building-your-Deep-Neural-Network-Step-by-Step&quot; class=&quot;headerlink&quot; title=&quot;Building your Deep Neural Network: Step by Step&quot;&gt;&lt;/a&gt;Building your Deep Neural Network: Step by Step&lt;/h2&gt;&lt;p&gt;L层神经网络：前L-1层的激励函数是ReLU，输出层激励函数是sigmod&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
