<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>绿小蕤</title>
  <icon>https://www.gravatar.com/avatar/e4d7a8bd1cb84fb3b4123916b4ea2f6b</icon>
  <subtitle>好逸恶劳,贪生怕死</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-06-20T12:19:28.951Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>绿小蕤</name>
    <email>528036346@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Deep Learning 吴恩达 作业（二）</title>
    <link href="http://yoursite.com/2018/06/20/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/20/Deep-Learning-吴恩达-作业（二）/</id>
    <published>2018-06-20T12:01:53.000Z</published>
    <updated>2018-06-20T12:19:28.951Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Building-your-Deep-Neural-Network-Step-by-Step"><a href="#Building-your-Deep-Neural-Network-Step-by-Step" class="headerlink" title="Building your Deep Neural Network: Step by Step"></a>Building your Deep Neural Network: Step by Step</h2><p>L层神经网络：前L-1层的激励函数是ReLU，输出层激励函数是sigmod</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Building your Deep Neural Network: Step by Step</span></span><br><span class="line"><span class="comment">#前L-1层的激励函数是ReLU,输出层激励函数是sigmod</span></span><br><span class="line"><span class="comment">#参数存储需要看清楚</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v4 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化每一层的参数 W、b</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network 每一层神经元的个数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parameters = &#123;&#125;                <span class="comment"># dict类型数据</span></span><br><span class="line">    L = len(layer_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):          <span class="comment"># 0就是输入层</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l<span class="number">-1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 两种激励函数,计算当前层的A,返回cache：A_prev, W, b, Z</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span><br><span class="line"><span class="string">             stored for computing the backward pass efficiently    </span></span><br><span class="line"><span class="string">             dict数据类型</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        <span class="comment">##linear_cache (A_prev, W, b)</span></span><br><span class="line">        <span class="comment">##activation_cache Z</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        Z, linear_cache = np.dot(W,A_prev)+b,(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        Z, linear_cache = np.dot(W,A_prev)+b,(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># L层的前向传播 输出AL(y-hat) caches(A_prev、W、b、Z)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- last post-activation value   最后输出的y-hat </span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []     <span class="comment">#存储A_prev、W、b、Z</span></span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="number">2</span>                  <span class="comment"># number of layers in the neural network //表示整数除</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU]*(L-1). Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L): <span class="comment">#for执行1~L-1</span></span><br><span class="line">        A_prev = A </span><br><span class="line">        A, cache = linear_activation_forward(A,  parameters[<span class="string">'W'</span> + str(l)],  parameters[<span class="string">'b'</span> + str(l)], <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement LINEAR -&gt; SIGMOID. Add "cache" to the "caches" list.</span></span><br><span class="line">    AL, cache = linear_activation_forward(A,  parameters[<span class="string">'W'</span> + str(L)],  parameters[<span class="string">'b'</span> + str(L)], <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># cost function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function defined by equation (7).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from aL and y.</span></span><br><span class="line">    cost = -np.sum(np.multiply(Y,np.log(AL))+ np.multiply(<span class="number">1</span>-Y,np.log(<span class="number">1</span>-AL)))/m</span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据dZ 计算 dA_prev, dW, db</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dW = np.dot(dZ,A_prev.T)/m</span><br><span class="line">    db = np.sum(dZ,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)/m <span class="comment">#同一行的相加 </span></span><br><span class="line">    dA_prev = np.dot(W.T,dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据dA计算dZ,dA_prev, dW, db</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></span><br><span class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    <span class="comment">#linear_cache：A_prev, W, b</span></span><br><span class="line">    <span class="comment">#activation_cache：Z</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#dZ=dA*g`(Z) 不同激励函数的导数不同</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache) <span class="comment">#dZ</span></span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache) <span class="comment">#根据dZ计算参数梯度 dA_prev, dW, db</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># L层反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ... </span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>] <span class="comment">#样本个数</span></span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment">#初始化起始dAL,第L层的dA</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "dAL, current_cache". Outputs: "grads["dAL-1"], grads["dWL"], grads["dbL"]</span></span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]  <span class="comment">#第l层的 A_prev，W，b，Z 放在caches[l-1]</span></span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class="string">"sigmoid"</span>) <span class="comment">#计算L层的参数,其对应的激励函数sigmoid</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop from l=L-2 to l=0</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)): </span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class="line">        <span class="comment"># Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l + <span class="number">2</span>)], current_cache, activation = <span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter. Use a for loop.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)]- learning_rate*grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)]- learning_rate*grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>PS：我做作业的时候，遇到最大的麻烦就是几个参数的上标，还有对应那一层的 cache</p><h3 id="1-下面是我整理的，程序对应的框图："><a href="#1-下面是我整理的，程序对应的框图：" class="headerlink" title="1.下面是我整理的，程序对应的框图："></a>1.下面是我整理的，程序对应的框图：</h3><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn1.png" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dnn2.png" alt=""></p><h3 id="2-参数整理"><a href="#2-参数整理" class="headerlink" title="2.参数整理"></a>2.参数整理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">parameters  字典类型</span><br><span class="line">parameters[&apos;W&apos; + str(l)]</span><br><span class="line">parameters[&apos;b&apos; + str(l)]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">grads　字典类型 </span><br><span class="line">grads[&quot;dA&quot; + str(l)]  前一层的dA_prev</span><br><span class="line">grads[&quot;dW&quot; + str(l)] </span><br><span class="line">grads[&quot;db&quot; + str(l)]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cache 存储当前层的 A_prev，W，b，Z </span><br><span class="line">linear_cache ：A_prev, W, b</span><br><span class="line">activation_cache： Z</span><br><span class="line">cache = (linear_cache, activation_cache)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">caches 存储所有的 A_prev, W, b, Z (caches[l]对应l+1层)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Building-your-Deep-Neural-Network-Step-by-Step&quot;&gt;&lt;a href=&quot;#Building-your-Deep-Neural-Network-Step-by-Step&quot; class=&quot;headerlink&quot; title=&quot;Building your Deep Neural Network: Step by Step&quot;&gt;&lt;/a&gt;Building your Deep Neural Network: Step by Step&lt;/h2&gt;&lt;p&gt;L层神经网络：前L-1层的激励函数是ReLU，输出层激励函数是sigmod&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习，python" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8Cpython/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning 吴恩达 作业（一）</title>
    <link href="http://yoursite.com/2018/06/15/Deep-Learning-%E5%90%B4%E6%81%A9%E8%BE%BE-%E4%BD%9C%E4%B8%9A%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2018/06/15/Deep-Learning-吴恩达-作业（一）/</id>
    <published>2018-06-15T06:11:03.000Z</published>
    <updated>2018-06-20T12:03:49.145Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Planar-data-classification-with-one-hidden-layer"><a href="#Planar-data-classification-with-one-hidden-layer" class="headerlink" title="Planar data classification with one hidden layer"></a>Planar data classification with one hidden layer</h2><p>不废话直接上代码，看神人的代码真的是惊叹！！！膜拜、模仿   不想多说，看代码</p><a id="more"></a><p>单隐层神经网络：输入用tanh激励函数，输出用sigmoid激励函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="comment"># import sklearn.linear_model</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span>  sigmoid, load_planar_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载随机的平面点，画出样本的散点图</span></span><br><span class="line">X, Y = load_planar_dataset()</span><br><span class="line"><span class="string">''' # 画出样本的散点图  作业上画图的函数我这边都会报错</span></span><br><span class="line"><span class="string">blue=['b']*200</span></span><br><span class="line"><span class="string">color=['r']*200</span></span><br><span class="line"><span class="string">color.extend(blue) #append添加单个元素 extend添加多个元素</span></span><br><span class="line"><span class="string">plt.scatter(X[0, :], X[1, :], c=color, s=40) #scatter散点图</span></span><br><span class="line"><span class="string">plt.show()</span></span><br><span class="line"><span class="string">print(np.shape(X))</span></span><br><span class="line"><span class="string">print(np.shape(Y))</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">#1. 逻辑回归</span></span><br><span class="line"><span class="string">lg = sklearn.linear_model.LogisticRegression()</span></span><br><span class="line"><span class="string">lg.fit(X.T, Y.ravel().T)  #训练 #.ravel()将多维数组降位一维</span></span><br><span class="line"><span class="string">LG_predictions = lg.predict(X.T) #将训练数据作为测试数据放入预测结果，与正确标签比对</span></span><br><span class="line"><span class="string">print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LG_predictions) + np.dot(1-Y,1-LG_predictions))/float(Y.size)*100) +'% ') #47%</span></span><br><span class="line"><span class="string">plot_decision_boundary(lambda x: lg.predict(x), X, Y)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#2. 贝叶斯岭回归</span></span><br><span class="line"><span class="string">br = sklearn.linear_model.BayesianRidge()</span></span><br><span class="line"><span class="string">br.fit(X.T, Y.ravel().T) </span></span><br><span class="line"><span class="string">BR_predictions = br.predict(X.T)</span></span><br><span class="line"><span class="string">print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,BR_predictions) + np.dot(1-Y,1-BR_predictions))/float(Y.size)*100) + '% ') #51%</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3. deeplearning</span></span><br><span class="line"><span class="comment"># Defining the neural network structure</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    n_x -- the size of the input layer   输入层</span></span><br><span class="line"><span class="string">    n_h -- the size of the hidden layer  隐藏层</span></span><br><span class="line"><span class="string">    n_y -- the size of the output layer  输出层</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer 输入样本的特征个数</span></span><br><span class="line">    n_h = <span class="number">4</span>          <span class="comment"># 隐藏层的节点个数</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer 输出结果</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the model's parameters 初始化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># np.random.seed(2) </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span>  </span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h,n_x))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y,n_h))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Loop 前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m) X经过转置</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters (output of initialization function)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    Z1 = np.dot(W1,X)+b1    <span class="comment">#.dot矩阵乘法 .multiply对应元素相乘</span></span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2,A1)+b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算cost function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost given in equation (13)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost given equation (13)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    logprobs = np.multiply(Y,np.log(A2))+np.multiply(<span class="number">1</span>-Y,np.log(<span class="number">1</span>-A2))  <span class="comment">#一个样本的loss function</span></span><br><span class="line">    cost = -np.sum(logprobs)/m  <span class="comment">#所有样本的cost function</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect.  #将维度为1的去掉 </span></span><br><span class="line">                                <span class="comment"># E.g., turns [[17]] into 17 </span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float)) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播 计算参数梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation using the instructions above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing our parameters </span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".</span></span><br><span class="line"><span class="string">    X -- input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary "parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary "cache".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span></span><br><span class="line">    dZ2 = A2-Y</span><br><span class="line">    dW2 = np.dot(dZ2,A1.T)/m</span><br><span class="line">    db2 = np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)/m   <span class="comment">#按行相加，并且保持其二维特性</span></span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T,dZ2),<span class="number">1</span>-pow(A1,<span class="number">2</span>)) </span><br><span class="line">    dW1 = np.dot(dZ1,X.T)/m</span><br><span class="line">    db1 = np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)/m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新参数 进行新一轮正向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Updates parameters using the gradient descent update rule given above</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary "grads"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line">    <span class="comment">## END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = W1-learning_rate*dW1</span><br><span class="line">    b1 = b1-learning_rate*db1</span><br><span class="line">    W2 = W2-learning_rate*dW2</span><br><span class="line">    b2 = b2-learning_rate*db2</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 组合前面的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    num_iterations -- Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    !--用到的变量--!</span></span><br><span class="line"><span class="string">    待求参数parameters = &#123;"W1": W1,"b1": b1, "W2": W2, "b2": b2&#125; 先初始化，然后更新</span></span><br><span class="line"><span class="string">    中间变量cache = &#123;"Z1": Z1,"A1": A1, "Z2": Z2,"A2": A2&#125;</span></span><br><span class="line"><span class="string">    代价函数cost</span></span><br><span class="line"><span class="string">    梯度grads = &#123;"dW1": dW1, "db1": db1,"dW2": dW2, "db2": db2&#125; </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#np.random.seed(3)</span></span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>] <span class="comment">#一个样本 输入特征个数</span></span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>] <span class="comment">#一个样本 输出结果</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 5 lines of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y) <span class="comment">#初始化参数</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations): <span class="comment">#梯度下降迭代次数</span></span><br><span class="line">         </span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="comment"># Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost)) <span class="comment">#每迭代1000次 输出cost function</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = np.round(A2) <span class="comment">#四舍五入</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a model with a n_h-dimensional hidden layer</span></span><br><span class="line">n_h = <span class="number">1</span></span><br><span class="line">parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">10000</span>)</span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line">Acc=float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'隐藏层节点：%d'</span>%n_h+<span class="string">'  '</span>+<span class="string">'准确率： %0.3f'</span> %Acc+<span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><p>改变隐藏层的节点数，得到的结果</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dl%E4%BD%9C%E4%B8%9A3.JPG" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dl%E4%BD%9C%E4%B8%9A1.JPG" alt=""></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/dl%E4%BD%9C%E4%B8%9A2.JPG" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Planar-data-classification-with-one-hidden-layer&quot;&gt;&lt;a href=&quot;#Planar-data-classification-with-one-hidden-layer&quot; class=&quot;headerlink&quot; title=&quot;Planar data classification with one hidden layer&quot;&gt;&lt;/a&gt;Planar data classification with one hidden layer&lt;/h2&gt;&lt;p&gt;不废话直接上代码，看神人的代码真的是惊叹！！！膜拜、模仿   不想多说，看代码&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习，python" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8Cpython/"/>
    
  </entry>
  
  <entry>
    <title>anaconda，conda，pip的关系</title>
    <link href="http://yoursite.com/2018/06/13/anaconda%EF%BC%8Cconda%EF%BC%8Cpip%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
    <id>http://yoursite.com/2018/06/13/anaconda，conda，pip的关系/</id>
    <published>2018-06-13T01:31:32.000Z</published>
    <updated>2018-06-13T01:38:12.210Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Anaconda：是一个<strong>python发行版</strong>。软件发行版是在系统上提前编译和配置好的软件包集合， 装好了后就可以直接用。因为包含了大量的科学包，Anaconda 的下载文件比较大（约 515 MB），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用Miniconda这个较小的发行版(仅包含conda和 Python)​</li><li>Conda：是一个通用的包管理器，当初设计来<strong>管理任何语言的包</strong></li><li>pip ：python包的软件包管理器</li></ul><p>Conda 和 pip的区别：pip可以允许你在<strong>任何环境</strong>中安装python包，而conda允许你在<strong>conda环境中安装任何语言包</strong>（包括c语言或者python）</p><p>PS；包管理器是自动化软件安装，更新，卸载的一种工具</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Anaconda：是一个&lt;strong&gt;python发行版&lt;/strong&gt;。软件发行版是在系统上提前编译和配置好的软件包集合， 装好了后就可以直接用。因为包含了大量的科学包，Anaconda 的下载文件比较大（约 515 MB），如果只需要某些包，或者需要节省
      
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="http://yoursite.com/2018/06/11/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>http://yoursite.com/2018/06/11/正则表达式/</id>
    <published>2018-06-11T08:31:41.000Z</published>
    <updated>2018-06-11T08:35:37.805Z</updated>
    
    <content type="html"><![CDATA[<p>PS：暂时没有深入学习的需求，但是老是看到，所有就想了解一下。<strong>正则表达式 (regular expression) 描述了一种字符串匹配的模式(pattern)</strong>。</p><p><a href="http://deerchao.net/tutorials/regex/regex.htm#mission" target="_blank" rel="noopener">http://deerchao.net/tutorials/regex/regex.htm#mission</a></p><p>字符可能是字母，数字，标点符号，空格，换行符，汉字等等。</p><p>字符串是0个或更多个字符的序列。文本也就是字符串。</p><p><strong>字符串匹配正则表达式</strong>，是指这个字符串里有一部分（或几部分）<strong>能满足表达式给出的条件</strong>。</p><p><strong>正则表达式就是记录文本规则的代码，用来进行文本匹配的工具。</strong></p><p>（比如，你可以编写一个正则表达式，用来查找所有以0开头，后面跟着2-3个数字，然后是一个连字号“-”，最后是7或8位数字的字符串像 010-12345678或0376-7654321)。</p><p>re 模块使 Python 语言拥有全部的正则表达式功能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;PS：暂时没有深入学习的需求，但是老是看到，所有就想了解一下。&lt;strong&gt;正则表达式 (regular expression) 描述了一种字符串匹配的模式(pattern)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://deerchao.net/tu
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2018/06/11/Titanic-Machine-Learning-from-Disaster/"/>
    <id>http://yoursite.com/2018/06/11/Titanic-Machine-Learning-from-Disaster/</id>
    <published>2018-06-11T06:36:57.500Z</published>
    <updated>2018-06-20T12:19:49.572Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>opencv Haar特征+Adaboost算法 物体检测</title>
    <link href="http://yoursite.com/2018/06/07/opencv-Haar%E7%89%B9%E5%BE%81-Adaboost%E7%AE%97%E6%B3%95-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2018/06/07/opencv-Haar特征-Adaboost算法-物体检测/</id>
    <published>2018-06-07T09:45:56.000Z</published>
    <updated>2018-06-07T09:48:26.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Haar"><a href="#一、Haar" class="headerlink" title="一、Haar"></a>一、Haar</h2><ol><li>什么是Haar特征</li><li>如何计算Haar特征</li><li>积分图快速计算Haar特征</li><li>Haar特征值标准化</li></ol><a id="more"></a><h2 id="二、级联分类器"><a href="#二、级联分类器" class="headerlink" title="二、级联分类器"></a>二、级联分类器</h2><ol><li>级联分类器结构：弱分类器线性组合成为强分类器，强分类器用决策树的形式级联</li><li>存储分类器的XML中的参数</li></ol><p>强弱分类器<img src="http://p8ge6t5tt.bkt.clouddn.com/xml1.JPG" alt=""></p><p>Haar特征<img src="http://p8ge6t5tt.bkt.clouddn.com/xml2.JPG" alt=""></p><h2 id="三、利用并查集合并窗口和丢弃零散分布的误检窗口"><a href="#三、利用并查集合并窗口和丢弃零散分布的误检窗口" class="headerlink" title="三、利用并查集合并窗口和丢弃零散分布的误检窗口"></a>三、利用并查集合并窗口和丢弃零散分布的误检窗口</h2><h2 id="四、AdaBoost-训练算法"><a href="#四、AdaBoost-训练算法" class="headerlink" title="四、AdaBoost 训练算法"></a>四、AdaBoost 训练算法</h2><ol><li><p>查准率 Precision查全率 Recall，precision和recall都越高越好</p></li><li><p>命中率 hitRate，度量检测器对正样本的通过能力，越接近1越好  </p><p>虚警率 falseAlarm，度量检测器对负样本的通过能力，越接近0越好</p></li></ol><h3 id="AdaBoost-步骤"><a href="#AdaBoost-步骤" class="headerlink" title="AdaBoost 步骤"></a>AdaBoost 步骤</h3><ol><li>寻找TP和FP作为训练样本</li><li>计算每个Haar特征在当前权重下的Best split threshold+leftvalue+rightvalue，组成了一个个弱分类器</li><li>通过WSE寻找最优的弱分类器</li><li>更新权重</li><li>按照minHitRate估计stageThreshold</li><li>重复上述1-5步骤，直到falseAlarmRate到达要求，或弱分类器数量足够，停止循环，输出stage</li><li>进入下一个stage训练</li></ol><h2 id="四、opencv-训练级联分类器"><a href="#四、opencv-训练级联分类器" class="headerlink" title="四、opencv 训练级联分类器"></a>四、opencv 训练级联分类器</h2><ol><li>收集正、负样本，正样本固定大小、负样本任意大小</li><li>生成正、负样本描述文件 .txt</li><li>对正样品归一化处理，负样本不处理</li><li>生成正样本特征文件 .vec</li><li>样本训练级联分类器</li></ol><h2 id="五、HAAR与LBP区别"><a href="#五、HAAR与LBP区别" class="headerlink" title="五、HAAR与LBP区别"></a>五、HAAR与LBP区别</h2><ol><li>HAAR特征是浮点数计算，LBP特征是整数计算；</li><li>LBP训练需要的样本数量比HAAR大 </li><li>LBP的速度一般比HAAR快 </li><li>同样的样本HAAR训练出来的检测结果要比LBP准确</li><li>扩大LBP的样本数据可达到HAAR的训练效果</li></ol><h2 id="六、opencv-提供的模型和工具"><a href="#六、opencv-提供的模型和工具" class="headerlink" title="六、opencv 提供的模型和工具"></a>六、opencv 提供的模型和工具</h2><p>1.opencv 自带的检测模型</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/opencv%E8%87%AA%E5%B8%A6%E7%9A%84xml.JPG" alt="img"></p><p>2.opencv 自带的AdaBoost分类器</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/opencvtrain.JPG" alt="img"></p><p>opencv_annotation        用来在一张大图中标定一个或多个需要检测的目标<br>opencv_createsamples  用来制作positive sample的vec<br>opencv_traincascade     用来训练得到需要的cascade.xml</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、Haar&quot;&gt;&lt;a href=&quot;#一、Haar&quot; class=&quot;headerlink&quot; title=&quot;一、Haar&quot;&gt;&lt;/a&gt;一、Haar&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;什么是Haar特征&lt;/li&gt;
&lt;li&gt;如何计算Haar特征&lt;/li&gt;
&lt;li&gt;积分图快速计算Haar特征&lt;/li&gt;
&lt;li&gt;Haar特征值标准化&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
      <category term="c++" scheme="http://yoursite.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法回顾目录</title>
    <link href="http://yoursite.com/2018/06/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%9B%9E%E9%A1%BE%E7%9B%AE%E5%BD%95/"/>
    <id>http://yoursite.com/2018/06/05/机器学习算法回顾目录/</id>
    <published>2018-06-05T07:32:56.000Z</published>
    <updated>2018-06-05T07:33:50.112Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、回归算法"><a href="#一、回归算法" class="headerlink" title="一、回归算法"></a>一、回归算法</h2><ol><li>线性回归：预测连续值</li><li>逻辑回归：预测离散值或分类</li></ol><h2 id="二、决策树-Decision-Tree-DT"><a href="#二、决策树-Decision-Tree-DT" class="headerlink" title="二、决策树 Decision Tree (DT)"></a>二、决策树 Decision Tree (DT)</h2><p>根据一个准则，选取最优划分属性 (最优划分属性：决策树的分支节点的类别尽可能纯，即尽可能属于同一类别)</p><ol><li>ID3：根据信息增益选取最优划分属性</li><li>C4.5：根据信息增益和增益率选取最优划分属性</li><li>CART (classification and regression tree) ：根据基尼指数选取最优划分属性</li></ol><h2 id="三、集成学习-ensemble-learning"><a href="#三、集成学习-ensemble-learning" class="headerlink" title="三、集成学习 (ensemble learning)"></a>三、集成学习 (ensemble learning)</h2><ol><li>Boosting：个体学习器间有强依赖关系，必须串行生成的序列化方法 (训练集基于上一轮结果进行调整)</li><li>Bagging：个体学习器间没有强依赖关系，可同时生成的并行化方法 (训练集互不相关</li></ol><h2 id="集成学习之-Boosting"><a href="#集成学习之-Boosting" class="headerlink" title="集成学习之 Boosting"></a>集成学习之 Boosting</h2><ol><li><p><strong>弱学习器提升为强学习器的过程称为Boosting</strong>，Boosting产生一系列的学习器，后产生的学习器的训练集取决于之前的产生的学习器，之前被误判的示例在之后占据较大的概率。</p></li><li><p>不同的boosting实现，主要区别：</p><p>(1) 弱学习算法本身不同</p><p>(2) 在每一轮学习之前，改变训练数据的权值分布的方法不同</p><p>(3) 将一组弱分类器组合成一个强分类器的方法不同</p></li><li><p>Boosting 算法总结</p><ul><li><p>AdaBoost</p><p>AdaBoost 算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法的学习方法</p></li></ul></li></ol><ul><li><p>GBDT 梯度提升决策树 </p><p>GBDT = Boosting Decision Tree 提升决策树 (DT)+ Gradient Boosting 梯度提升(GB) </p><p>是一种迭代的决策树算法</p></li></ul><ul><li><p>XGBoost </p><p>XGBoost是在GBDT的基础上对Boosting算法进行的改进，内部决策树使用的是回归树。</p></li></ul><h2 id="集成学习之Bagging"><a href="#集成学习之Bagging" class="headerlink" title="集成学习之Bagging"></a>集成学习之Bagging</h2><p>Bagging是基于bootstrap sampling也称为自助采样法法，是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间。</p><ul><li><p>Random Forest 随机森林 (RF) 就是基于Bagging，进一步在训练过程中引入随机属性。随机森林简单来说就是用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。</p><p>​</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、回归算法&quot;&gt;&lt;a href=&quot;#一、回归算法&quot; class=&quot;headerlink&quot; title=&quot;一、回归算法&quot;&gt;&lt;/a&gt;一、回归算法&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;线性回归：预测连续值&lt;/li&gt;
&lt;li&gt;逻辑回归：预测离散值或分类&lt;/li&gt;
&lt;/ol&gt;
&lt;h2
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Cygwin 安装</title>
    <link href="http://yoursite.com/2018/06/01/Cygwin-%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2018/06/01/Cygwin-安装/</id>
    <published>2018-06-01T09:12:21.000Z</published>
    <updated>2018-06-01T09:17:06.917Z</updated>
    
    <content type="html"><![CDATA[<p>PS：一篇我自己都搞不大明白的博文，咋办呢 … 得慢慢开始学 Linux 了</p><h2 id="Cygwin的安装"><a href="#Cygwin的安装" class="headerlink" title="Cygwin的安装"></a>Cygwin的安装</h2><ul><li>Cygwin 用于各种版本的Microsoft Windows上，运行类UNIX系统。Cygwin的主要目的是通过重新编译，将POSIX系统（例如Linux、BSD，以及其他Unix系统）上的软件移植到Windows上。                     官网下载 <a href="http://www.cygwin.com/" target="_blank" rel="noopener">http://www.cygwin.com/</a></li></ul><p><a href="https://jingyan.baidu.com/article/6b97984d83dfe51ca2b0bf0e.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/6b97984d83dfe51ca2b0bf0e.html</a></p><ul><li>安装vim 之前只安装了gcc， 后面又重新进去安装，在之前那步就把选项给勾了。                        <a href="https://blog.csdn.net/u012247418/article/details/79719679" target="_blank" rel="noopener">https://blog.csdn.net/u012247418/article/details/79719679</a></li></ul><ul><li>helloworld 验证 (难道就我觉得用起来这么不方便的吗)</li></ul><p><a href="https://jingyan.baidu.com/article/a948d6512fb5d70a2ccd2e6f.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/a948d6512fb5d70a2ccd2e6f.html</a></p><a id="more"></a><h2 id="Cygwin-、虚拟机-、Win10-Linux子系统区别"><a href="#Cygwin-、虚拟机-、Win10-Linux子系统区别" class="headerlink" title="Cygwin 、虚拟机 、Win10 Linux子系统区别"></a>Cygwin 、虚拟机 、Win10 Linux子系统区别</h2><p>(1) Cygwin 和虚拟机的区别</p><p>Cygwin 是一个 POSIX 兼容层，就是将linux平台上的系统调用转换成了windows API，使 Linux 的软件能够运行，应该可以称得上一种 Linux 发行版，而虚拟机则是虚拟了一个仿真的计算机环境，让Linux 认为虚拟机是一台机器，从而在上面安装 Linux</p><p><strong>Cygwin并<em>不能直接运行</em>  Linux 中运行的程序，必须在Cygwin中<em>重新编译</em> 该程序源码后才能让该程序在windows中运行</strong>。</p><p><a href="http://www.169it.com/tech-qa-linux/article-8333186796219879155.html" target="_blank" rel="noopener">http://www.169it.com/tech-qa-linux/article-8333186796219879155.html</a></p><p>(2) Win10下的Linux子系统</p><p><a href="http://www.cnblogs.com/micro-chen/p/5437316.html" target="_blank" rel="noopener">http://www.cnblogs.com/micro-chen/p/5437316.html</a></p><p><a href="https://www.jianshu.com/p/bc38ed12da1d" target="_blank" rel="noopener">https://www.jianshu.com/p/bc38ed12da1d</a></p><p>PS：下面是遇到的乱七八糟的总结，因为对OS实在不熟悉，遇到很多英文，真的搞不清楚这些是个啥</p><hr><h2 id="1-Linux-——-redhat、suse、debain、ubuntu、fedora版本区别"><a href="#1-Linux-——-redhat、suse、debain、ubuntu、fedora版本区别" class="headerlink" title="1. Linux —— redhat、suse、debain、ubuntu、fedora版本区别"></a>1. Linux —— <strong>redhat</strong>、suse、debain、<strong>ubuntu</strong>、fedora版本区别</h2><p><a href="https://blog.csdn.net/lzx1104/article/details/41776977" target="_blank" rel="noopener">https://blog.csdn.net/lzx1104/article/details/41776977</a></p><h2 id="2-Linux和windows下的命令行解释器"><a href="#2-Linux和windows下的命令行解释器" class="headerlink" title="2. Linux和windows下的命令行解释器"></a>2. Linux和windows下的命令行解释器</h2><p>命令解释器，处于内核和用户之间，负责把用户的指令传递给内核并且把执行结果回显给用户</p><h3 id="Linux："><a href="#Linux：" class="headerlink" title="Linux："></a>Linux：</h3><p> shell 是一个<strong>命令行解释器</strong>，是终端和Linux内核之间的接口程序，在提示符下输入的每个命令都由 shell 先解释然后传给Linux内核。                                                                               在Linux 和 UNIX系统里可以使用不同的shell，最常用的 Bourne shell (sh), C shell (csh),  Korn shell (ksh)。</p><ul><li><p>bash (Bourne Again shell)： Bourne shell 的扩展</p><p>常见bash命令      <a href="http://blinkfox.com/chang-yong-bashming-ling-zheng-li-yi-cha-kan-wen-jian-he-mu-lu/" target="_blank" rel="noopener">http://blinkfox.com/chang-yong-bashming-ling-zheng-li-yi-cha-kan-wen-jian-he-mu-lu/</a></p></li><li><p>zsh：完美兼容bash，并且有比bash更强大的功能，用起来也比bash更优雅</p></li></ul><h3 id="windows："><a href="#windows：" class="headerlink" title="windows："></a>windows：</h3><p>windows系统中见到的桌面即explorer.exe (文件资源管理器)是<strong>图形</strong>shell，而cmd就是命令行解释器(相当于Linux的bash),，windows也有强大的shell叫windows power shell</p><p>git bash是Windows下的命令行工具，是基于cmd的，在windows下使用git命令的模拟终端，linux、unix可以直接使用git。</p><h2 id="3-版本问题"><a href="#3-版本问题" class="headerlink" title="3. 版本问题"></a>3. 版本问题</h2><ul><li>Alpha：内部测试版。α是希腊字母的第一个，表示最早的版本，这个版本包含很多BUG，功能也不全，主要是给开发人员和测试人员测试和找BUG用的。</li><li>Beta：公开测试版。β是希腊字母的第二个，这个阶段的版本会一直加入新的功能。</li><li>RC：(Release　Candidate) 候选版本，RC版不会再加入新的功能了，主要着重于除错。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：一篇我自己都搞不大明白的博文，咋办呢 … 得慢慢开始学 Linux 了&lt;/p&gt;
&lt;h2 id=&quot;Cygwin的安装&quot;&gt;&lt;a href=&quot;#Cygwin的安装&quot; class=&quot;headerlink&quot; title=&quot;Cygwin的安装&quot;&gt;&lt;/a&gt;Cygwin的安装&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Cygwin 用于各种版本的Microsoft Windows上，运行类UNIX系统。Cygwin的主要目的是通过重新编译，将POSIX系统（例如Linux、BSD，以及其他Unix系统）上的软件移植到Windows上。                     官网下载 &lt;a href=&quot;http://www.cygwin.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.cygwin.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://jingyan.baidu.com/article/6b97984d83dfe51ca2b0bf0e.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://jingyan.baidu.com/article/6b97984d83dfe51ca2b0bf0e.html&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安装vim 之前只安装了gcc， 后面又重新进去安装，在之前那步就把选项给勾了。                        &lt;a href=&quot;https://blog.csdn.net/u012247418/article/details/79719679&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/u012247418/article/details/79719679&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;helloworld 验证 (难道就我觉得用起来这么不方便的吗)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://jingyan.baidu.com/article/a948d6512fb5d70a2ccd2e6f.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://jingyan.baidu.com/article/a948d6512fb5d70a2ccd2e6f.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="装软件" scheme="http://yoursite.com/tags/%E8%A3%85%E8%BD%AF%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Git 学习</title>
    <link href="http://yoursite.com/2018/06/01/Git-%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/06/01/Git-学习/</id>
    <published>2018-06-01T09:05:53.000Z</published>
    <updated>2018-06-01T09:07:33.373Z</updated>
    
    <content type="html"><![CDATA[<p>PS：之前虽然用GitHub搭建博客，但是对于Git的理解非常不透彻，然后最近准备学Linux ，哎，之前挖的坑还是要填起来啊，Linux系统的源码由Git管理，我总要搞清楚Git 是个啥~ 感觉看完廖雪峰老师的教程心里大致有了个数</p><a id="more"></a><h2 id="1-Git-版本控制系统"><a href="#1-Git-版本控制系统" class="headerlink" title="1. Git 版本控制系统"></a>1. Git 版本控制系统</h2><ul><li>集中式的版本控制系统：CVS及SVN                                                                                                                                            版本库是集中存放在中央服务器的，，要先从中央服务器取得最新的版本，然后开始干活，干完活了，再把自己的活推送给中央服务器。最大缺点就是必须联网才能工作。</li></ul><ul><li>分布式版本控制系统：Git                                                                                                                        每个人的电脑上都是一个完整的版本库，就不需要联网，因为版本库就在自己的电脑上。对于多人协作的方式，只需把各自的修改推送给对方，就可以互相看到对方的修改。</li></ul><p>所有的版本控制系统<strong>只能跟踪文本文件的改动</strong>，txt，网页，程序代码等等。而图片、视频这些二进制文件，虽然也能由版本控制系统管理，但没法跟踪文件的变化，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道。而Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的，如果要真正使用版本控制系统，就要以纯文本方式编写文件。</p><p>文本文件与二进制文件的区在编码层次上</p><ul><li>文本文件是基于字符编码的文件，常见的编码有ASCII编码，UNICODE编码</li><li>二进制文件是基于值编码的文件</li></ul><p>文本文件编码基于字符定长，译码容易些；二进制文件编码变长，存储利用率要高，译码难一些</p><p>PS：Git 查看和修改用户名和邮箱 <a href="https://www.jianshu.com/p/d24e791a7679" target="_blank" rel="noopener">https://www.jianshu.com/p/d24e791a7679</a></p><h2 id="2-Git仓库修改版本"><a href="#2-Git仓库修改版本" class="headerlink" title="2. Git仓库修改版本"></a>2. Git仓库修改版本</h2><h3 id="1-工作区"><a href="#1-工作区" class="headerlink" title="1) 工作区"></a>1) 工作区</h3><p>看的见的文件目录，放自己的文件</p><h3 id="2-版本库或仓库-repository"><a href="#2-版本库或仓库-repository" class="headerlink" title="2) 版本库或仓库 (repository)"></a>2) 版本库或仓库 (repository)</h3><p>理解成一个目录，这个目录里每个文件的修改、删除，Git都能跟踪   <u>！划重点！</u>  <strong>Git管理的是修改不是文件</strong></p><h3 id="3-添加文件到仓库的具体步骤"><a href="#3-添加文件到仓库的具体步骤" class="headerlink" title="3) 添加文件到仓库的具体步骤"></a>3) 添加文件到仓库的具体步骤</h3><p>1.创建空的仓库                                                                                                                                                        2.在工作区目录下新建文件      <code>touch create_repository.txt</code>                                                                                                                        3.将这个文件添加到<strong>暂存区</strong>     <code>git add create_repository.txt</code>                                                            4.将<strong>暂存区所有内容提交到当前分支 master</strong>  <code>git commit -m &quot;本次提交说明&quot;</code>  commit就是保存一个文件快照</p><p>文件提交到版本库里分两步，暂存区和分支    <u>！划重点！</u> 所以每次修改，如果不用<code>git add</code>到暂存区，那就不会加到 <code>commit</code> 中</p><h3 id="4）修改仓库版本"><a href="#4）修改仓库版本" class="headerlink" title="4）修改仓库版本"></a>4）修改仓库版本</h3><p>查看版本历史  <code>git log</code>     查看命令修改历史  <code>git reflog</code>   </p><p>回到上一版本   <code>git reset --hard Head^</code>                                                                                         回到上上个版本   <code>git reset --hard Head^^</code>                                                                                            回到往上的100个版本   <code>git reset --hard Head~100</code>        指定到指定的版本   <code>git reset --hard 版本号</code>   </p><h2 id="3-远程仓库"><a href="#3-远程仓库" class="headerlink" title="3.远程仓库"></a>3.远程仓库</h2><p>Git是分布式版本控制系统，同一个Git仓库，可以分布到不同的机器上，且每台机器的版本库其实都是一样的。</p><p>实际情况是找一台电脑当服务器，每天24小时开机，每个人都从这个服务器仓库里克隆一份到自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交。</p><p>只要注册一个GitHub账号，就可以免费获得Git远程仓库，本地Git仓库和GitHub仓库之间的传输是通过SSH加密。</p><p>1) SSH (Secure Shell) 安全外壳协议                                                                                                                     </p><p><a href="https://www.jianshu.com/p/33461b619d53" target="_blank" rel="noopener">https://www.jianshu.com/p/33461b619d53</a> 嗯~看不懂~不想看</p><p>1) 本地仓库同步到远程仓库</p><ul><li><p>GitHub 创建一个新的仓库</p></li><li><p>将本地仓库与远程仓库关联 <code>git remote add origin https://github.com/Sophia0130/LearnGit.git</code></p><p>下面两条tips：</p><p>1&gt; 提示出错信息：fatal: remote origin already exists.      输入  <code>git remote rm origin</code> 再进行关联</p><p>2&gt; 第一次推送master分支时，加上了<code>-u</code>参数，Git不但会把本地的<code>master</code>分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令</p></li></ul><ul><li>把本地库的所有内容推送到远程库上 <code>git push -u origin master</code></li><li>要本地作了提交，通过 <code>git push origin master</code>  把本地master分支的最新修改推送至GitHub</li></ul><p>2) 从远程仓库克隆</p><h2 id="4-分支管理"><a href="#4-分支管理" class="headerlink" title="4. 分支管理"></a>4. 分支管理</h2><p>PS：这部分内容画 branch 分支图比较好理解</p><ul><li><p>创建一个分支，即增加一个分支的指针，并将Head指向该分支</p><p><code>git branch dev</code>  创建了一个分支dev，<code>git checkout dev</code>将Head指向这个分支</p></li></ul><ul><li><p>对工作区的修改和提交就是针对dev分支了，新提交一次后，dev指针往前移动一步，而master指针不变</p><p><code>git add</code> 和 <code>git commit -m &quot;xxxx&quot;</code>提交当前分支</p></li></ul><ul><li><p>在dev上的工作完成后，把master指向当前的dev,完成合并</p><p><code>git merge dev</code></p></li></ul><ul><li><p>删除dev分支，即把dev指针删除</p><p><code>git branch -d dev</code></p></li></ul><p>查看分支：<code>git branch</code>列出所有分支，当前分支前面会标一个<code>*</code>号</p><p>创建分支：<code>git branch</code></p><p>切换分支：<code>git checkout</code></p><p>创建+切换分支：<code>git checkout -b</code></p><p>合并某分支到当前分支：<code>git merge</code></p><p>删除分支：<code>git branch -d</code></p><p>PS：后面还有很多分支管理的内容，但是我觉得我够用了，就不看了，别骂我懒 ~</p><h2 id="5-标签管理"><a href="#5-标签管理" class="headerlink" title="5. 标签管理"></a>5. 标签管理</h2><p>由于版本号那串东西太复杂了，所以打标签可以更容易区分，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。</p><p>切换到需要打标签的分支上</p><p><code>git tag v0.0</code>用于新建一个标签</p><p><code>git tag -a  -m &quot;xxx&quot;</code>可以指定标签信息</p><p><code>git tag</code>查看所有标签</p><p>PS：后面还有操作标签的内容，我也懒得看了 ~</p><h2 id="6-搭建Git服务器"><a href="#6-搭建Git服务器" class="headerlink" title="6. 搭建Git服务器"></a>6. 搭建Git服务器</h2><p>搭建Git服务器需要准备一台运行Linux的机器，需要管理每个人的公钥和控制权限 </p><p> 现在也用不到啊</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：之前虽然用GitHub搭建博客，但是对于Git的理解非常不透彻，然后最近准备学Linux ，哎，之前挖的坑还是要填起来啊，Linux系统的源码由Git管理，我总要搞清楚Git 是个啥~ 感觉看完廖雪峰老师的教程心里大致有了个数&lt;/p&gt;
    
    </summary>
    
    
      <category term="Git" scheme="http://yoursite.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>CryptoKitties</title>
    <link href="http://yoursite.com/2018/05/28/CryptoKitties/"/>
    <id>http://yoursite.com/2018/05/28/CryptoKitties/</id>
    <published>2018-05-28T08:55:11.000Z</published>
    <updated>2018-05-28T08:55:41.159Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CryptoKitties"><a href="#CryptoKitties" class="headerlink" title="CryptoKitties"></a>CryptoKitties</h1><p>PS：区块链的技术发展，颠覆了我对金融的全新认识 ~ CryptoKitties 基于以太坊的虚拟养猫游戏</p><p>PS：花生的回答，让我对以太坊游戏化应用有了透彻心扉的领悟，谁都不会觉得自己成为庞氏骗局的而接盘侠 ~     <a href="https://www.zhihu.com/question/263599712" target="_blank" rel="noopener">https://www.zhihu.com/question/263599712</a></p><p>新手买猫教程，一只猫这么贵，心疼小老百姓的钱，玩不起 <a href="https://zhuanlan.zhihu.com/p/32213293" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32213293</a></p><p>超级6啊，技术流派啊，用爬虫来选猫，哈哈哈 <a href="https://www.zhihu.com/question/263605805" target="_blank" rel="noopener">https://www.zhihu.com/question/263605805</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CryptoKitties&quot;&gt;&lt;a href=&quot;#CryptoKitties&quot; class=&quot;headerlink&quot; title=&quot;CryptoKitties&quot;&gt;&lt;/a&gt;CryptoKitties&lt;/h1&gt;&lt;p&gt;PS：区块链的技术发展，颠覆了我对金融的全新认识 ~
      
    
    </summary>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
  </entry>
  
  <entry>
    <title>2050大会——关于图像处理的收获</title>
    <link href="http://yoursite.com/2018/05/28/2050%E5%A4%A7%E4%BC%9A%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E6%94%B6%E8%8E%B7/"/>
    <id>http://yoursite.com/2018/05/28/2050大会——关于图像处理的收获/</id>
    <published>2018-05-28T07:46:21.000Z</published>
    <updated>2018-05-28T07:49:31.145Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2050"><a href="#2050" class="headerlink" title="2050"></a>2050</h2><p>PS：我也不知道应该咋评价这个大会，可能我实在太菜了，在我眼里更多是企业孵化器的一部分，将创业公司推向大众，吸引融资方和合作者，不是我自以为的知识分享。</p><p>不过从我专业来看，发现两个比较好玩的东西，都是对皮下血液进行图像处理，一个进行健康检测，一个进行身份识别，不过两者的成像原理不同，一个利用可见光，一个利用红外成像。</p><a id="more"></a><h2 id="可见光透视皮肤，查看皮下血液活动"><a href="#可见光透视皮肤，查看皮下血液活动" class="headerlink" title="可见光透视皮肤，查看皮下血液活动"></a>可见光透视皮肤，查看皮下血液活动</h2><p>PS：李康发明一种透皮光学成像技术，可以通过检测面部血液流动的情况，来推测被测试者的情感和感受，检测心理压力和心率。感觉特别神奇之处在于用到的是可见光进行血流检测，他们的产品只需要用手机摄像头就可以检测血压等生理信号。</p><p><strong>透皮光学成像</strong>，就是透过人的皮肤去查看皮肤底下的血液活动。人们在经历不同情感的时候，比如羞愧啊，难过啊，害怕啊，即使表面不露声色，但是面部的血液流动一定会发生变化。比如撒谎的时候，脸颊部分的血液流动就会减少，鼻子附近血流会增加。通过这项技术，观察一个人的鼻子附近是不是血流变多了，就能判断他是不是撒谎了，准确率能上升到85%以上，匹诺曹的故事竟然也是有科学道理的。</p><p>具体成像原理是利用可见光穿过皮肤，光子会被弹来弹去，最后被搅得乱成一团，但这些光子并没有丢失，因此从原则上说，这个混杂光场是可以进行逆向分解的。</p><p>对于利用可见光进行皮下血液成像觉得很神奇吧，不知道这个光场逆向解析散射光具体在光路上怎么做到的，但是准确度啥的，本小白还没考证，希望这项技术能开发更精准的算法，和更多应用吧 ~</p><p>可见光透视人体 ~                                                                                                                                          <a href="http://www.xinhuanet.com/science/2016-11/25/c_135855643.htm" target="_blank" rel="noopener">http://www.xinhuanet.com/science/2016-11/25/c_135855643.htm</a></p><p>这篇是李康的TED演讲 ~                                                                         <a href="https://www.ted.com/talks/kang_lee_can_you_really_tell_if_a_kid_is_lying/transcript?language=zh-cn" target="_blank" rel="noopener">https://www.ted.com/talks/kang_lee_can_you_really_tell_if_a_kid_is_lying/transcript?language=zh-cn</a></p><h2 id="掌静脉身份识别"><a href="#掌静脉身份识别" class="headerlink" title="掌静脉身份识别"></a>掌静脉身份识别</h2><p>掌静脉识别，静脉是导血回心的血管，起于毛细血管，止于心房，表浅静脉在皮下可以看见。掌静脉，顾名思义，就是手掌内静脉。掌静脉识别是静脉识别的一种，属于生物识别，掌静脉识别系统就是首先通过静脉识别仪取得个人掌静脉分布图，从掌静脉分布图依据专用比对算法提取特征值，通过红外线CCD摄像头获取手指、手掌、手背静脉的图像，将静脉的数字图像存贮在计算机系统中，将特征值存储。静脉比对时，实时采取静脉图，提取特征值，运用先进的滤波、图像二值化、细化手段对数字图像提取特征，同存储在主机中静脉特征值比对，采用复杂的匹配算法对静脉特征进行匹配，从而对个人进行身份鉴定，确认身份。</p><p>这项技术已经不是太前沿的了，但是我想聚焦一下它的具体应用，最近比较火的各种网红无人便利店和自动贩卖机，利用掌静脉识别模块及掌静脉认证支付系统，无需投币，无需刷卡，只需刷手便可完成支付，可以实现拿了商品就走的自动结算。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;2050&quot;&gt;&lt;a href=&quot;#2050&quot; class=&quot;headerlink&quot; title=&quot;2050&quot;&gt;&lt;/a&gt;2050&lt;/h2&gt;&lt;p&gt;PS：我也不知道应该咋评价这个大会，可能我实在太菜了，在我眼里更多是企业孵化器的一部分，将创业公司推向大众，吸引融资方和合作者，不是我自以为的知识分享。&lt;/p&gt;
&lt;p&gt;不过从我专业来看，发现两个比较好玩的东西，都是对皮下血液进行图像处理，一个进行健康检测，一个进行身份识别，不过两者的成像原理不同，一个利用可见光，一个利用红外成像。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Advanced Technology" scheme="http://yoursite.com/tags/Advanced-Technology/"/>
    
  </entry>
  
  <entry>
    <title>虚拟机和云(特别简陋的认识)</title>
    <link href="http://yoursite.com/2018/05/18/%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%92%8C%E4%BA%91-%E7%89%B9%E5%88%AB%E7%AE%80%E9%99%8B%E7%9A%84%E8%AE%A4%E8%AF%86/"/>
    <id>http://yoursite.com/2018/05/18/虚拟机和云-特别简陋的认识/</id>
    <published>2018-05-18T09:00:10.000Z</published>
    <updated>2018-05-18T09:00:47.201Z</updated>
    
    <content type="html"><![CDATA[<h2 id="虚拟机"><a href="#虚拟机" class="headerlink" title="虚拟机"></a>虚拟机</h2><p>PS：我不知道我为啥要写这篇博，单纯就想了解吧，所以记录得乱七八糟，因为自己也没接触这块过。</p><ol><li>宿主机：指要安装虚拟机软件的计算机，你花钱买的物理机。</li><li>虚拟机：利用虚拟机工具构造出的一整套硬件设备，有自己操作系统，应用软件。                    </li><li>虚拟机的简单应用</li></ol><p><a href="http://www.pc841.com/article/20120413-5731.html" target="_blank" rel="noopener">http://www.pc841.com/article/20120413-5731.html</a></p><a id="more"></a><p>1) 最简单我们电脑中没有光驱，如果要安装系统我们就可以<strong>使用虚拟机来安装系统，虚拟机内部拥有虚拟光驱，支持直接打开系统镜像文件安装系统</strong>。</p><p>2) 另外虚拟机技术在游戏爱好者朋友眼中也相当实用，很多游戏不支持同时多开，但我们可以在电脑中多创建几个虚拟机，那么在虚拟机系统中即可单独再运行程序了，这样即可实现一台电脑同时多开同一游戏了。</p><h2 id="虚拟机和云"><a href="#虚拟机和云" class="headerlink" title="虚拟机和云"></a>虚拟机和云</h2><p>这篇知乎写的还是很详细的，但是后面术语真的看不懂。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;虚拟机&quot;&gt;&lt;a href=&quot;#虚拟机&quot; class=&quot;headerlink&quot; title=&quot;虚拟机&quot;&gt;&lt;/a&gt;虚拟机&lt;/h2&gt;&lt;p&gt;PS：我不知道我为啥要写这篇博，单纯就想了解吧，所以记录得乱七八糟，因为自己也没接触这块过。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;宿主机：指要安装虚拟机软件的计算机，你花钱买的物理机。&lt;/li&gt;
&lt;li&gt;虚拟机：利用虚拟机工具构造出的一整套硬件设备，有自己操作系统，应用软件。                    &lt;/li&gt;
&lt;li&gt;虚拟机的简单应用&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&quot;http://www.pc841.com/article/20120413-5731.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.pc841.com/article/20120413-5731.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="云" scheme="http://yoursite.com/tags/%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>啥叫运营商</title>
    <link href="http://yoursite.com/2018/05/18/%E5%95%A5%E5%8F%AB%E8%BF%90%E8%90%A5%E5%95%86/"/>
    <id>http://yoursite.com/2018/05/18/啥叫运营商/</id>
    <published>2018-05-18T07:21:14.000Z</published>
    <updated>2018-05-18T07:21:52.920Z</updated>
    
    <content type="html"><![CDATA[<p>运营商是指<strong>提供网络服务的供应商</strong>，如中国联通、中国电信、 中国移动、中国移动这些公司，而诺基亚、三星等这些<strong>通信设备的生产厂家叫生产商</strong>，因为国家在电信管理方面相当严格，只有拥有工信部颁发的运营执照的公司才能架设网络，从通信行业来说，设备生产商和运营商是相互依存的。</p><p>不过今天看到一篇文章讲5G时代的到来对设备商和运营商的影响，感觉还是蛮有趣的，而且对两者会有更深刻的认识吧~                                                                                                            <a href="http://www.sohu.com/a/197243837_100030976" target="_blank" rel="noopener">http://www.sohu.com/a/197243837_100030976</a></p><a id="more"></a><p>随着技术的更新，运营商对于基站的建设可以说是投资巨大，稍有不慎可能全部打水漂，特别是当下技术还不成熟时，这让我想到了当年的液晶和等离子之争，现在的OLED和QLED之争，谁知道几年以后会发生如何的变迁，最重要的是站对阵营啊 ~</p><p>关于为什么要推动5G技术发展，引用文章种中的比喻还是很形象的，设备商是军火商，没有战争，怎么发财。没有新技术，怎么忽悠运营商。而运营商也希望通过网络技术来建筑竞争壁垒，获取技术红利。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;运营商是指&lt;strong&gt;提供网络服务的供应商&lt;/strong&gt;，如中国联通、中国电信、 中国移动、中国移动这些公司，而诺基亚、三星等这些&lt;strong&gt;通信设备的生产厂家叫生产商&lt;/strong&gt;，因为国家在电信管理方面相当严格，只有拥有工信部颁发的运营执照的公司才能架设网络，从通信行业来说，设备生产商和运营商是相互依存的。&lt;/p&gt;
&lt;p&gt;不过今天看到一篇文章讲5G时代的到来对设备商和运营商的影响，感觉还是蛮有趣的，而且对两者会有更深刻的认识吧~                                                                                                            &lt;a href=&quot;http://www.sohu.com/a/197243837_100030976&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.sohu.com/a/197243837_100030976&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>图像处理之坐标系和宽高</title>
    <link href="http://yoursite.com/2018/05/16/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B9%8B%E5%9D%90%E6%A0%87%E7%B3%BB%E5%92%8C%E5%AE%BD%E9%AB%98/"/>
    <id>http://yoursite.com/2018/05/16/图像处理之坐标系和宽高/</id>
    <published>2018-05-16T12:31:13.000Z</published>
    <updated>2018-05-16T12:48:07.284Z</updated>
    
    <content type="html"><![CDATA[<p>PS：为啥写这篇博，因为我傻啊 ~ 每次写程序遇到图像  <strong>坐标系-宽高-行列</strong>，就开始抓狂                        哎呀到底是(i，j)还是(j，i)，这篇博写的很详细啦 ~</p><p><a href="https://blog.csdn.net/oqqenvy12/article/details/71933651" target="_blank" rel="noopener">https://blog.csdn.net/oqqenvy12/article/details/71933651</a></p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/%E5%9D%90%E6%A0%87%E7%B3%BB.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;PS：为啥写这篇博，因为我傻啊 ~ 每次写程序遇到图像  &lt;strong&gt;坐标系-宽高-行列&lt;/strong&gt;，就开始抓狂                        哎呀到底是(i，j)还是(j，i)，这篇博写的很详细啦 ~&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>XML、YAML、JSON</title>
    <link href="http://yoursite.com/2018/05/14/XML%E3%80%81YAML%E3%80%81JSON/"/>
    <id>http://yoursite.com/2018/05/14/XML、YAML、JSON/</id>
    <published>2018-05-14T09:04:32.000Z</published>
    <updated>2018-05-14T09:44:25.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="XML、YML-、JSON-是序列化数据格式"><a href="#XML、YML-、JSON-是序列化数据格式" class="headerlink" title="XML、YML 、JSON 是序列化数据格式"></a>XML、YML 、JSON 是序列化<strong>数据格式</strong></h2><p>PS：为什么要写这个呢，以为今天看opencv的时候讲到了XML、YAML 文件的输入输出，觉得这个经常听到名字但是不知道干啥用的~然后前几天搭博客，又一直出现JSON ~~ 心累，我的理解就是某种数据格式，方便数据的传输吧~~下面这篇篇博客讲了几个的简单区别，了解就可以了                                            <a href="http://rensanning.iteye.com/blog/2379083" target="_blank" rel="noopener">http://rensanning.iteye.com/blog/2379083</a></p><a id="more"></a><h2 id="OpenCV-使用XML和YAML实现文件输入和输出"><a href="#OpenCV-使用XML和YAML实现文件输入和输出" class="headerlink" title="OpenCV 使用XML和YAML实现文件输入和输出"></a>OpenCV 使用XML和YAML实现文件输入和输出</h2><p>为什么需要呢？因为当处理完图像后需要将数据保存到文件上。                                                                举个栗子：我们对一幅图像进行特征提取之后，需要把特征点信息保存到文件上，以供后面的机器学习分类操作。所以我们需要搭建<strong>小型数据库文件</strong>，将数据写到文件上，下次需要时从文件里读出。实现上述方法，需要使用xml和yml，具有可读性。</p><p>PS：opencv 自带的教程实在是太复杂了可以直接看下面这篇的案例，简单好懂~~                                        <a href="http://www.cnblogs.com/skyfsm/p/7182313.html" target="_blank" rel="noopener">http://www.cnblogs.com/skyfsm/p/7182313.html</a></p><p>XML/YAML文件在OpenCV中的数据结构为FileStorage</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;XML、YML-、JSON-是序列化数据格式&quot;&gt;&lt;a href=&quot;#XML、YML-、JSON-是序列化数据格式&quot; class=&quot;headerlink&quot; title=&quot;XML、YML 、JSON 是序列化数据格式&quot;&gt;&lt;/a&gt;XML、YML 、JSON 是序列化&lt;strong&gt;数据格式&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;PS：为什么要写这个呢，以为今天看opencv的时候讲到了XML、YAML 文件的输入输出，觉得这个经常听到名字但是不知道干啥用的~然后前几天搭博客，又一直出现JSON ~~ 心累，我的理解就是某种数据格式，方便数据的传输吧~~下面这篇篇博客讲了几个的简单区别，了解就可以了                                            &lt;a href=&quot;http://rensanning.iteye.com/blog/2379083&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://rensanning.iteye.com/blog/2379083&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>傅里叶变换和图像处理</title>
    <link href="http://yoursite.com/2018/05/14/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%92%8C%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2018/05/14/傅里叶变换和图像处理/</id>
    <published>2018-05-14T07:28:48.000Z</published>
    <updated>2018-05-14T07:30:43.709Z</updated>
    
    <content type="html"><![CDATA[<h2 id="离散傅里叶变换"><a href="#离散傅里叶变换" class="headerlink" title="离散傅里叶变换"></a>离散傅里叶变换</h2><p>原理：傅立叶变换是一个将函数分解的工具，任一函数都可以表示成无数个正弦和余弦函数的和的形式。对一张图像使用傅立叶变换就是将它分解成正弦和余弦两部分，也就是将图像从空间域(spatial domain)转换到频域(frequency domain)</p><h2 id="离散傅里叶变化-DFT"><a href="#离散傅里叶变化-DFT" class="headerlink" title="离散傅里叶变化 DFT"></a>离散傅里叶变化 DFT</h2><p><strong>图像去噪</strong>： 当图像出现的噪声是有规律的，去某个频率的波，比如高斯噪声。但是当出现的噪声是没有规律的，随机出现的一些东西，DFT是没有作用的。</p><h2 id="离散余弦变换-DCT"><a href="#离散余弦变换-DCT" class="headerlink" title="离散余弦变换 DCT"></a>离散余弦变换 DCT</h2><p>图像的余弦波由实偶函数组成</p><p><strong>图像压缩</strong>：JPEG格式的图片就是用Huffman编码方式压缩图片的DCT的系数</p><a id="more"></a><p>PS：这个东西我真的理解了很多遍，还是等要用的时候再说吧，下面这个代码可以看懂，但是原理就…</p><p><a href="http://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/core/discrete_fourier_transform/discrete_fourier_transform.html" target="_blank" rel="noopener">http://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/core/discrete_fourier_transform/discrete_fourier_transform.html</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//离散傅里叶变换 DFT</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/core.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/imgproc.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/imgcodecs.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/highgui.hpp"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">help</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"This program demonstrated the use of the discrete Fourier transform (DFT). "</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"The dft of an image is taken and it's power spectrum is displayed."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> ** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">help();</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* filename = argv[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">Mat I = imread(filename, IMREAD_GRAYSCALE);</span><br><span class="line"><span class="keyword">if</span> (I.empty())&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Error opening image"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//! [expand]  当图像的尺寸是2， 3，5的整数倍时，傅里叶变换的计算速度最快</span></span><br><span class="line">Mat padded; </span><br><span class="line"><span class="keyword">int</span> m = getOptimalDFTSize(I.rows); <span class="comment">//获取最佳尺寸</span></span><br><span class="line"><span class="keyword">int</span> n = getOptimalDFTSize(I.cols);</span><br><span class="line">copyMakeBorder(I, padded, <span class="number">0</span>, m - I.rows, <span class="number">0</span>, n - I.cols, BORDER_CONSTANT, Scalar::all(<span class="number">0</span>)); <span class="comment">//将边缘像素填充为0</span></span><br><span class="line"><span class="comment">//! [expand]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//! [complex_and_real] 为傅里叶变换的实部虚部分配内存</span></span><br><span class="line">Mat planes[] = &#123; Mat_&lt;<span class="keyword">float</span>&gt;(padded), Mat::zeros(padded.size(), CV_32F) &#125;; </span><br><span class="line">    <span class="comment">//在padded基础上加一个初始化为0的通道</span></span><br><span class="line">Mat complexI;</span><br><span class="line">merge(planes, <span class="number">2</span>, complexI);  <span class="comment">//将单通道数组合并成一个多通道的数组，从而创建出一个多通道阵列 </span></span><br><span class="line"><span class="comment">//! [complex_and_real]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//! [dft]</span></span><br><span class="line">dft(complexI, complexI); </span><br><span class="line"><span class="comment">//! [dft]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// compute the magnitude </span></span><br><span class="line"><span class="comment">//! [magnitude]</span></span><br><span class="line">split(complexI, planes);    <span class="comment">// planes[0] = Re(DFT(I), planes[1] = Im(DFT(I))</span></span><br><span class="line">magnitude(planes[<span class="number">0</span>], planes[<span class="number">1</span>], planes[<span class="number">0</span>]);<span class="comment">// 计算实部和虚部的幅值，放在planes[0]</span></span><br><span class="line">Mat magI = planes[<span class="number">0</span>];</span><br><span class="line"><span class="comment">//! [magnitude]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//! [log] switch to logarithmic scale</span></span><br><span class="line"><span class="comment">// =&gt; log(1 + sqrt(Re(DFT(I))^2 + Im(DFT(I))^2))</span></span><br><span class="line">magI += Scalar::all(<span class="number">1</span>); <span class="comment">//傅立叶变换的幅度值范围大到不适合在屏幕上显示，需要做对数尺度缩放</span></span><br><span class="line"><span class="built_in">log</span>(magI, magI);</span><br><span class="line"><span class="comment">//! [log]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//! [crop_rearrange]</span></span><br><span class="line"><span class="comment">// crop the spectrum, if it has an odd number of rows or columns</span></span><br><span class="line">magI = magI(Rect(<span class="number">0</span>, <span class="number">0</span>, magI.cols &amp; <span class="number">-2</span>, magI.rows &amp; <span class="number">-2</span>)); <span class="comment">//剔除添加的像素(和-2按位与) ？？实在不懂是为什么啊</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// rearrange the quadrants of Fourier image  so that the origin is at the image center 重分布幅度图象限</span></span><br><span class="line"><span class="keyword">int</span> cx = magI.cols / <span class="number">2</span>;</span><br><span class="line"><span class="keyword">int</span> cy = magI.rows / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">Mat q0(magI, Rect(0, 0, cx, cy));   // Top-Left - Create a ROI per quadrant</span><br><span class="line">Mat q1(magI, Rect(cx, 0, cx, cy));  // Top-Right</span><br><span class="line">Mat q2(magI, Rect(0, cy, cx, cy));  // Bottom-Left</span><br><span class="line">Mat q3(magI, Rect(cx, cy, cx, cy)); // Bottom-Right</span><br><span class="line"></span><br><span class="line">Mat tmp;                           <span class="comment">// swap quadrants (Top-Left with Bottom-Right)</span></span><br><span class="line">q0.copyTo(tmp);</span><br><span class="line">q3.copyTo(q0);</span><br><span class="line">tmp.copyTo(q3);</span><br><span class="line"></span><br><span class="line">q1.copyTo(tmp);                    <span class="comment">// swap quadrant (Top-Right with Bottom-Left)</span></span><br><span class="line">q2.copyTo(q1);</span><br><span class="line">tmp.copyTo(q2);</span><br><span class="line"><span class="comment">//! [crop_rearrange]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//! [normalize]</span></span><br><span class="line">normalize(magI, magI, <span class="number">0</span>, <span class="number">1</span>, NORM_MINMAX);<span class="comment">//数将幅度归一化到可显示范围//（不能放到0-255）</span></span><br><span class="line"><span class="comment">//! [normalize]</span></span><br><span class="line"></span><br><span class="line">imshow(<span class="string">"Input Image"</span>, I);    </span><br><span class="line">imshow(<span class="string">"spectrum magnitude"</span>, magI);</span><br><span class="line">waitKey();</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;离散傅里叶变换&quot;&gt;&lt;a href=&quot;#离散傅里叶变换&quot; class=&quot;headerlink&quot; title=&quot;离散傅里叶变换&quot;&gt;&lt;/a&gt;离散傅里叶变换&lt;/h2&gt;&lt;p&gt;原理：傅立叶变换是一个将函数分解的工具，任一函数都可以表示成无数个正弦和余弦函数的和的形式。对一张图像使用傅立叶变换就是将它分解成正弦和余弦两部分，也就是将图像从空间域(spatial domain)转换到频域(frequency domain)&lt;/p&gt;
&lt;h2 id=&quot;离散傅里叶变化-DFT&quot;&gt;&lt;a href=&quot;#离散傅里叶变化-DFT&quot; class=&quot;headerlink&quot; title=&quot;离散傅里叶变化 DFT&quot;&gt;&lt;/a&gt;离散傅里叶变化 DFT&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;图像去噪&lt;/strong&gt;： 当图像出现的噪声是有规律的，去某个频率的波，比如高斯噪声。但是当出现的噪声是没有规律的，随机出现的一些东西，DFT是没有作用的。&lt;/p&gt;
&lt;h2 id=&quot;离散余弦变换-DCT&quot;&gt;&lt;a href=&quot;#离散余弦变换-DCT&quot; class=&quot;headerlink&quot; title=&quot;离散余弦变换 DCT&quot;&gt;&lt;/a&gt;离散余弦变换 DCT&lt;/h2&gt;&lt;p&gt;图像的余弦波由实偶函数组成&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图像压缩&lt;/strong&gt;：JPEG格式的图片就是用Huffman编码方式压缩图片的DCT的系数&lt;/p&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>opencv基本绘图</title>
    <link href="http://yoursite.com/2018/05/14/opencv%E5%9F%BA%E6%9C%AC%E7%BB%98%E5%9B%BE/"/>
    <id>http://yoursite.com/2018/05/14/opencv基本绘图/</id>
    <published>2018-05-14T03:37:13.000Z</published>
    <updated>2018-05-14T03:38:53.823Z</updated>
    
    <content type="html"><![CDATA[<p>PS：其实本来不想写这篇的，实在太简单了，但是当做个检索吧，以备不时之需~~</p><p><a href="https://blog.csdn.net/ubunfans/article/details/24421981" target="_blank" rel="noopener">https://blog.csdn.net/ubunfans/article/details/24421981</a></p><a id="more"></a><h3 id="Point"><a href="#Point" class="headerlink" title="Point"></a>Point</h3><h3 id="Scalar"><a href="#Scalar" class="headerlink" title="Scalar"></a>Scalar</h3><h3 id="Rectangle"><a href="#Rectangle" class="headerlink" title="Rectangle"></a>Rectangle</h3><h3 id="Line"><a href="#Line" class="headerlink" title="Line"></a>Line</h3><h3 id="Circle"><a href="#Circle" class="headerlink" title="Circle"></a>Circle</h3><h3 id="Ellipse"><a href="#Ellipse" class="headerlink" title="Ellipse"></a>Ellipse</h3><h3 id="PolyLine-多边形的绘制"><a href="#PolyLine-多边形的绘制" class="headerlink" title="PolyLine   多边形的绘制"></a>PolyLine   多边形的绘制</h3><h3 id="PutText-在窗口显示文本-但是只能显示英文，中文不支持"><a href="#PutText-在窗口显示文本-但是只能显示英文，中文不支持" class="headerlink" title="PutText     在窗口显示文本(但是只能显示英文，中文不支持)"></a>PutText     在窗口显示文本(但是<strong>只能显示英文</strong>，中文不支持)</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PS：其实本来不想写这篇的，实在太简单了，但是当做个检索吧，以备不时之需~~&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/ubunfans/article/details/24421981&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/ubunfans/article/details/24421981&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>图像读取的四种方法</title>
    <link href="http://yoursite.com/2018/05/14/%E5%9B%BE%E5%83%8F%E8%AF%BB%E5%8F%96%E7%9A%84%E5%9B%9B%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/05/14/图像读取的四种方法/</id>
    <published>2018-05-14T03:06:31.000Z</published>
    <updated>2018-05-14T03:09:23.224Z</updated>
    
    <content type="html"><![CDATA[<h2 id="扫描、读取图像的四种方法"><a href="#扫描、读取图像的四种方法" class="headerlink" title="扫描、读取图像的四种方法"></a>扫描、读取图像的四种方法</h2><p>1.C operator [] 指针</p><p>2.iterator 迭代法 用迭代器遍历 [推荐使用]</p><p>3.on-the-fly address generation  .at()函数</p><p>4.LUT function [推荐使用]</p><a id="more"></a><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//扫描图像的四种方法的比较</span></span><br><span class="line"><span class="comment">//输入的三个参数  imageName.jpg  divideWith  intValueToReduce [G](可省略)</span></span><br><span class="line"><span class="comment">//项目属性需要更改：项目属性 -&gt; 配置属性 -&gt; C/C++ -&gt; 代码生成 -&gt; 运行库</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/utility.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/imgcodecs.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">help</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">cout</span></span><br><span class="line">&lt;&lt; <span class="string">"\n--------------------------------------------------------------------------"</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"This program shows how to scan image objects in OpenCV (cv::Mat)."</span></span><br><span class="line">&lt;&lt; <span class="string">" we take an input image and divide the native color palette (255) with the input. "</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"Shows C operator[] method, iterators and at function for on-the-fly item address calculation."</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"./输入三个参数 &lt;imageNameToUse&gt; &lt;divideWith&gt; &lt;G&gt;(可省略)"</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"if you add a G parameter the image is processed in gray scale"</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="string">"--------------------------------------------------------------------------"</span> &lt;&lt; <span class="built_in">endl</span></span><br><span class="line">&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceC</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar* table)</span></span>;</span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceIterator</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar* table)</span></span>;</span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceRandomAccess</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar * table)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">help();</span><br><span class="line"><span class="keyword">if</span> (argc &lt; <span class="number">3</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Not enough parameters"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Mat I, J;</span><br><span class="line"><span class="keyword">if</span> (argc == <span class="number">4</span> &amp;&amp; !<span class="built_in">strcmp</span>(argv[<span class="number">3</span>], <span class="string">"G"</span>))</span><br><span class="line">I = imread(argv[<span class="number">1</span>], IMREAD_GRAYSCALE);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">I = imread(argv[<span class="number">1</span>], IMREAD_COLOR);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (I.empty())</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"The image"</span> &lt;&lt; argv[<span class="number">1</span>] &lt;&lt; <span class="string">" could not be loaded."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//用命令行参数argv[2]给出的整数进行颜色缩减</span></span><br><span class="line"><span class="keyword">int</span> divideWith = <span class="number">0</span>; <span class="comment">// convert our input string to number - C++ style</span></span><br><span class="line"><span class="built_in">stringstream</span> s;</span><br><span class="line">s &lt;&lt; argv[<span class="number">2</span>];</span><br><span class="line">s &gt;&gt; divideWith;</span><br><span class="line"><span class="keyword">if</span> (!s || !divideWith)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Invalid number entered for dividing. "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//查找表</span></span><br><span class="line">uchar table[<span class="number">256</span>]; <span class="comment">//查找表</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; ++i)</span><br><span class="line">table[i] = (uchar)(divideWith * (i / divideWith));</span><br><span class="line"><span class="comment">//divideWith=10时，0到9取为0，10到19取为10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 运行时间-单位毫秒</span></span><br><span class="line"><span class="comment">// getTickCount() 返回CPU自某个事件以来走过的时钟周期数</span></span><br><span class="line"><span class="comment">// getTickFrequency()  返回CPU一秒钟所走的时钟周期数</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> times = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">double</span> t;</span><br><span class="line"><span class="comment">//方法1</span></span><br><span class="line">t = (<span class="keyword">double</span>)getTickCount();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; times; ++i)</span><br><span class="line">&#123;</span><br><span class="line">cv::Mat clone_i = I.clone();</span><br><span class="line">J = ScanImageAndReduceC(clone_i, table);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">t = <span class="number">1000</span> * ((<span class="keyword">double</span>)getTickCount() - t) / getTickFrequency();</span><br><span class="line">t /= times;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Time of reducing with the C operator [] (averaged for "</span></span><br><span class="line">&lt;&lt; times &lt;&lt; <span class="string">" runs): "</span> &lt;&lt; t &lt;&lt; <span class="string">" milliseconds."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">//方法1</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">//方法2</span></span><br><span class="line">t = (<span class="keyword">double</span>)getTickCount();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; times; ++i)</span><br><span class="line">&#123;</span><br><span class="line">cv::Mat clone_i = I.clone();</span><br><span class="line">J = ScanImageAndReduceIterator(clone_i, table);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">t = <span class="number">1000</span> * ((<span class="keyword">double</span>)getTickCount() - t) / getTickFrequency();</span><br><span class="line">t /= times;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Time of reducing with the iterator (averaged for "</span></span><br><span class="line">&lt;&lt; times &lt;&lt; <span class="string">" runs): "</span> &lt;&lt; t &lt;&lt; <span class="string">" milliseconds."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">//方法2</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">//方法3</span></span><br><span class="line">t = (<span class="keyword">double</span>)getTickCount();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; times; ++i)</span><br><span class="line">&#123;</span><br><span class="line">cv::Mat clone_i = I.clone();</span><br><span class="line">ScanImageAndReduceRandomAccess(clone_i, table);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">t = <span class="number">1000</span> * ((<span class="keyword">double</span>)getTickCount() - t) / getTickFrequency();</span><br><span class="line">t /= times;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Time of reducing with the on-the-fly address generation - at function (averaged for "</span></span><br><span class="line">&lt;&lt; times &lt;&lt; <span class="string">" runs): "</span> &lt;&lt; t &lt;&lt; <span class="string">" milliseconds."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">//方法3</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">//方法4</span></span><br><span class="line"><span class="comment">//最被推荐的用于实现批量图像元素查找和更该操作图像方法，在图像处理中，对于一个给定的值，将其替换成其他的值是一个很常见的操作</span></span><br><span class="line"><span class="function">Mat <span class="title">lookUpTable</span><span class="params">(<span class="number">1</span>, <span class="number">256</span>, CV_8U)</span></span>;</span><br><span class="line">uchar* p = lookUpTable.ptr();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; ++i)</span><br><span class="line">p[i] = table[i]; <span class="comment">//table数组的值赋给lookUpTable Mat</span></span><br><span class="line"><span class="comment">//! [table-init]</span></span><br><span class="line"></span><br><span class="line">t = (<span class="keyword">double</span>)getTickCount();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; times; ++i)</span><br><span class="line"><span class="comment">//! [table-use]</span></span><br><span class="line">LUT(I, lookUpTable, J);</span><br><span class="line"><span class="comment">//! [table-use]</span></span><br><span class="line"></span><br><span class="line">t = <span class="number">1000</span> * ((<span class="keyword">double</span>)getTickCount() - t) / getTickFrequency();</span><br><span class="line">t /= times;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Time of reducing with the LUT function (averaged for "</span></span><br><span class="line">&lt;&lt; times &lt;&lt; <span class="string">" runs): "</span> &lt;&lt; t &lt;&lt; <span class="string">" milliseconds."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"> <span class="comment">//方法4</span></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//! [scan-c] c语言风格，用指针</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceC</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar* <span class="keyword">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// accept only char type matrices</span></span><br><span class="line">CV_Assert(I.depth() == CV_8U); <span class="comment">//CV_Assert()若括号中的表达式值为false，则返回一个错误信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> channels = I.channels();</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> nRows = I.rows;</span><br><span class="line"><span class="keyword">int</span> nCols = I.cols * channels;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (I.isContinuous()) <span class="comment">//判断图像在内存中是否连续存储</span></span><br><span class="line">&#123;</span><br><span class="line">nCols *= nRows;   <span class="comment">//若图像连续存储将图像作为一维数组</span></span><br><span class="line">nRows = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//若图像连续存储，则可以将图像的数据看成是一个一维数组</span></span><br><span class="line"><span class="comment">//p（uchar*）就是指向图像数据的第一个字节的，因此可以用data指针访问图像的数据,从而加速Mat图像的访问速度</span></span><br><span class="line"><span class="comment">//一般经过裁剪的Mat图像，都不再连续了</span></span><br><span class="line"><span class="keyword">int</span> i, j;</span><br><span class="line">uchar* p;</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; nRows; ++i)</span><br><span class="line">&#123;</span><br><span class="line">p = I.ptr&lt;uchar&gt;(i);</span><br><span class="line"><span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; nCols; ++j)</span><br><span class="line">&#123;</span><br><span class="line">p[j] = table[p[j]];</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//! [scan-c]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//迭代法</span></span><br><span class="line"><span class="comment">//! [scan-iterator] 获得图像矩阵的begin和end</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceIterator</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar* <span class="keyword">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// accept only char type matrices</span></span><br><span class="line">CV_Assert(I.depth() == CV_8U);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> channels = I.channels();</span><br><span class="line"><span class="keyword">switch</span> (channels)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">&#123;</span><br><span class="line">  MatIterator_&lt;uchar&gt; it, end;</span><br><span class="line">  <span class="keyword">for</span> (it = I.begin&lt;uchar&gt;(), end = I.end&lt;uchar&gt;(); it != end; ++it)</span><br><span class="line">  *it = table[*it];</span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">&#123;</span><br><span class="line">  MatIterator_&lt;Vec3b&gt; it, end;</span><br><span class="line">  <span class="keyword">for</span> (it = I.begin&lt;Vec3b&gt;(), end = I.end&lt;Vec3b&gt;(); it != end; ++it)</span><br><span class="line">  &#123;</span><br><span class="line">  (*it)[<span class="number">0</span>] = table[(*it)[<span class="number">0</span>]];</span><br><span class="line">  (*it)[<span class="number">1</span>] = table[(*it)[<span class="number">1</span>]];</span><br><span class="line">  (*it)[<span class="number">2</span>] = table[(*it)[<span class="number">2</span>]];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//! [scan-iterator]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// On-the-fly 地址计算</span></span><br><span class="line"><span class="comment">//! [scan-random] 这个方法并不推荐被用来进行图像扫描，需要知道元素的所在行数与列数还有数据类型</span></span><br><span class="line"><span class="function">Mat&amp; <span class="title">ScanImageAndReduceRandomAccess</span><span class="params">(Mat&amp; I, <span class="keyword">const</span> uchar* <span class="keyword">const</span> table)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// accept only char type matrices</span></span><br><span class="line">CV_Assert(I.depth() == CV_8U);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> channels = I.channels();</span><br><span class="line"><span class="keyword">switch</span> (channels)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; I.rows; ++i)</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; I.cols; ++j)</span><br><span class="line">  I.at&lt;uchar&gt;(i, j) = table[I.at&lt;uchar&gt;(i, j)];</span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">&#123;</span><br><span class="line">  Mat_&lt;Vec3b&gt; _I = I;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; I.rows; ++i)</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; I.cols; ++j)</span><br><span class="line">  &#123;</span><br><span class="line">  _I(i, j)[<span class="number">0</span>] = table[_I(i, j)[<span class="number">0</span>]];</span><br><span class="line">  _I(i, j)[<span class="number">1</span>] = table[_I(i, j)[<span class="number">1</span>]];</span><br><span class="line">  _I(i, j)[<span class="number">2</span>] = table[_I(i, j)[<span class="number">2</span>]];</span><br><span class="line">  &#125;</span><br><span class="line">  I = _I;</span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> I;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="结果比较"><a href="#结果比较" class="headerlink" title="结果比较"></a>结果比较</h2><p>不同的图片几种方法得到的执行时间差很大。                                                        <strong>尽量使用 OpenCV 内置函数</strong>，调用LUT 函数可以获得最快的速度，这是因为OpenCV库可以通过英特尔线程架构启用多线程。                                                                                        指针法也很快，但是前提是指针连续。                                                                                        .at()函数方法并不推荐被用来进行图像扫描。</p><h2 id="程序解读"><a href="#程序解读" class="headerlink" title="程序解读"></a>程序解读</h2><h3 id="1-计算运算时间的方法"><a href="#1-计算运算时间的方法" class="headerlink" title="1.计算运算时间的方法"></a>1.计算运算时间的方法</h3><p>getTickCount() 返回CPU自某个事件以来走过的时钟周期数<br>getTickFrequency()  返回CPU一秒钟所走的时钟周期数</p><h3 id="2-查找表缩短颜色空间"><a href="#2-查找表缩短颜色空间" class="headerlink" title="2.查找表缩短颜色空间"></a>2.查找表缩短颜色空间</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">uchar table[<span class="number">256</span>]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; ++i)</span><br><span class="line">       table[i] = divideWith* (i/divideWith); <span class="comment">//当divideWith=10时，0到9取为0，10到19取为10</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;扫描、读取图像的四种方法&quot;&gt;&lt;a href=&quot;#扫描、读取图像的四种方法&quot; class=&quot;headerlink&quot; title=&quot;扫描、读取图像的四种方法&quot;&gt;&lt;/a&gt;扫描、读取图像的四种方法&lt;/h2&gt;&lt;p&gt;1.C operator [] 指针&lt;/p&gt;
&lt;p&gt;2.iterator 迭代法 用迭代器遍历 [推荐使用]&lt;/p&gt;
&lt;p&gt;3.on-the-fly address generation  .at()函数&lt;/p&gt;
&lt;p&gt;4.LUT function [推荐使用]&lt;/p&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法大总结</title>
    <link href="http://yoursite.com/2018/05/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%A4%A7%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/05/11/机器学习算法大总结/</id>
    <published>2018-05-11T07:42:09.000Z</published>
    <updated>2018-06-03T08:12:02.106Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习算法分类"><a href="#机器学习算法分类" class="headerlink" title="机器学习算法分类"></a>机器学习算法分类</h1><ul><li><p>监督学习(有标签)      根据预测结果分成两类</p><ul><li>分类问题-离散值</li><li>回归问题-连续值</li></ul></li><li><p>无监督学习(无标签)</p></li><li><p>半监督学习(一半有标签，一般无标签)</p></li><li><p>强化学习</p><p><a id="more"></a>​</p></li></ul><h2 id="回归算法-Regression-Algorithms"><a href="#回归算法-Regression-Algorithms" class="headerlink" title="回归算法(Regression Algorithms)"></a>回归算法(Regression Algorithms)</h2><p>自变量与因变量之间的显著关系，下面列出了七种回归问题</p><p><a href="https://www.jianshu.com/p/15dd20f8d02c" target="_blank" rel="noopener">https://www.jianshu.com/p/15dd20f8d02c</a></p><h2 id="基于实例的算法-Instance-based-Algorithms"><a href="#基于实例的算法-Instance-based-Algorithms" class="headerlink" title="基于实例的算法(Instance-based Algorithms)"></a>基于实例的算法(Instance-based Algorithms)</h2><p><strong>最后建成的模型，对原始数据样本实例依旧有很强的依赖性</strong>。这类算法在做预测决策时，一般都是使用某类相似度准则，去<strong>比对待预测的样本和原始样本的相近度</strong>，再给出相应的预测结果。</p><h2 id="决策树类算法-Decision-Tree-Algorithms"><a href="#决策树类算法-Decision-Tree-Algorithms" class="headerlink" title="决策树类算法(Decision Tree Algorithms)"></a>决策树类算法(Decision Tree Algorithms)</h2><p>决策树是一个树结构，每个非叶节点表示一个<strong>特征属性</strong>上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个<strong>类别</strong>。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，<strong>将叶子节点存放的类别作为决策结果</strong>。                    PS：西瓜书里的这张图很好解释啥叫决策树，还好学了数据结构里的树结构，现在感觉学的东西都是相通的~~</p><p><img src="https://pic3.zhimg.com/v2-39d109b46ea4f34d5efbf67edc11d57d_r.jpg" alt="决策树"></p><h2 id="贝叶斯类算法-Bayesian-Algorithms"><a href="#贝叶斯类算法-Bayesian-Algorithms" class="headerlink" title="贝叶斯类算法(Bayesian Algorithms)"></a>贝叶斯类算法(Bayesian Algorithms)</h2><p>在分类和回归问题中，隐含使用了贝叶斯原理的算法</p><p>PS：都是概率统计里的数学知识啊~概率服从一定的分布( y随x的变化规律)    </p><p>下面这篇博客写的简直太好了，用了一个栗子讲解朴素贝叶斯~~</p><p><a href="https://blog.csdn.net/amds123/article/details/70173402" target="_blank" rel="noopener">https://blog.csdn.net/amds123/article/details/70173402</a></p><p>用上面这篇博客里的栗子解释朴素贝叶斯：如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？                                                                这是一个典型的分类问题，<strong>转为数学问题就是比较p(嫁|(不帅、性格不好、身高矮、不上进))与p(不嫁|(不帅、性格不好、身高矮、不上进))的概率</strong>，谁的概率大，我就能给出嫁或者不嫁的答案！    </p><h2 id="聚类算法-Clustering-Algorithms"><a href="#聚类算法-Clustering-Algorithms" class="headerlink" title="聚类算法(Clustering Algorithms)"></a>聚类算法(Clustering Algorithms)</h2><p>用于进行数据分类，把不同的数据分到不同的群组</p><p><a href="https://miketech.it/k-means/" target="_blank" rel="noopener">https://miketech.it/k-means/</a>        </p><h2 id="关联规则算法-Association-Rule-Learning-Algorithms"><a href="#关联规则算法-Association-Rule-Learning-Algorithms" class="headerlink" title="关联规则算法(Association Rule Learning Algorithms)"></a>关联规则算法(Association Rule Learning Algorithms)</h2><p>抽取出最能解释观察到的训练样本之间关联关系的规则，也就是获取一个事件和其他事件之间依赖或关的知识                利用一些度量指标来分辨数据库中存在的强规则，也就是说<strong>关联规则挖掘是用于知识发现</strong>，而<strong>非预测</strong>，所以是属于<strong>无监督</strong>的机器学习方法                                                                                        <a href="https://www.jianshu.com/p/7d459ace31ab" target="_blank" rel="noopener">https://www.jianshu.com/p/7d459ace31ab</a></p><h2 id="人工神经网络类算法-Artificial-Neural-Network-Algorithms"><a href="#人工神经网络类算法-Artificial-Neural-Network-Algorithms" class="headerlink" title="人工神经网络类算法(Artificial Neural Network Algorithms)"></a>人工神经网络类算法(Artificial Neural Network Algorithms)</h2><p>人工神经网络(ANN) ，简称神经网络(缩写)，模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题，其中深度学习就是其中的一类算法</p><h2 id="深度学习-Deep-Learning-Algorithms"><a href="#深度学习-Deep-Learning-Algorithms" class="headerlink" title="深度学习(Deep Learning Algorithms)"></a>深度学习(Deep Learning Algorithms)</h2><p>PS：Deap Learning和NN的区别，说实在的除了网络层级不同，下面的那个特征挑选，真没懂。</p><p><a href="https://zhidao.baidu.com/question/554111494800207612.html" target="_blank" rel="noopener">https://zhidao.baidu.com/question/554111494800207612.html</a></p><ul><li>神经网络只有输入层、隐藏层、输出层<br>深度学习 输入层 - 卷积层 -降维层 -卷积层 - 降维层 – …. – 隐藏层 -输出层</li><li>多层神经网络做的步骤是：特征-&gt;值，<strong>特征是人工挑选</strong>。<br>深度学习做的步骤是：信号-&gt;特征-&gt;值，<strong>特征是网络自己选择</strong>。</li></ul><h2 id="降维算法-Dimensionality-Reduction-Algorithms"><a href="#降维算法-Dimensionality-Reduction-Algorithms" class="headerlink" title="降维算法(Dimensionality Reduction Algorithms)"></a>降维算法(Dimensionality Reduction Algorithms)</h2><p>降维就是指采用某种映射方法，<strong>将原高维空间中的数据点映射到低维度的空间</strong>， 降维算法一般在数据的<strong>可视化</strong>，或者是降低数据计算空间有很大的作用。它作为一种机器学习的算法，很多时候用它先<strong>处理数据</strong>，再灌入别的机器学习算法学习。</p><h2 id="模型融合算法-Ensemble-Algorithms"><a href="#模型融合算法-Ensemble-Algorithms" class="headerlink" title="模型融合算法(Ensemble Algorithms)"></a>模型融合算法(Ensemble Algorithms)</h2><p>融合不算是一种机器学习算法，而更像是一种<strong>优化手段</strong>，它通常是结合多个简单的弱机器学习算法，去做更可靠的决策。                                                                                                                                            拿分类问题举个例，直观的理解，就是单个分类器的分类是可能出错，不可靠的，但是如果<strong>多个分类器投票，那可靠度就会高很多</strong>。</p><h1 id="机器学习算法使用图谱"><a href="#机器学习算法使用图谱" class="headerlink" title="机器学习算法使用图谱"></a>机器学习算法使用图谱</h1><p>scikit-learn作为一个丰富的python机器学习库，实现了绝大多数机器学习的算法。</p><p><img src="https://img-blog.csdn.net/20160930051801241?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><p>上图是scikit-learn的路径图，分成了分类、聚类、回归和降维四个方法</p><hr><h2 id="最常用的算法"><a href="#最常用的算法" class="headerlink" title="最常用的算法"></a>最常用的算法</h2><p>1.线性回归 (Linear Regression)</p><p>2.逻辑回归 (Logistic Regression)</p><p>3.决策树 (Decision Tree)</p><p>4.支持向量机（SVM）</p><p>5.朴素贝叶斯 (Naive Bayes)</p><p>6.K邻近算法（KNN）</p><p>7.K-均值算法（K-means）</p><p>8.随机森林 (Random Forest)</p><p>9.降低维度算法（Dimensionality Reduction Algorithms）</p><p>10.Gradient Boost和Adaboost算法</p><p>这篇总结的很全面了 <a href="https://blog.csdn.net/han_xiaoyang/article/details/51191386" target="_blank" rel="noopener">https://blog.csdn.net/han_xiaoyang/article/details/51191386</a></p><hr><h2 id="机器学习工作流程"><a href="#机器学习工作流程" class="headerlink" title="机器学习工作流程"></a>机器学习工作流程</h2><h3 id="1-抽象成数学问题"><a href="#1-抽象成数学问题" class="headerlink" title="1 抽象成数学问题"></a>1 <strong>抽象成数学问题</strong></h3><p><strong>明确问题是进行机器学习的第一步</strong>。这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。</p><h3 id="2-获取数据"><a href="#2-获取数据" class="headerlink" title="2 获取数据"></a>2 <strong>获取数据</strong></h3><ul><li><strong>数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。</strong>数据要有<strong>代表性</strong>，否则必然会过拟合。</li><li>而且对于分类问题，<strong>数据偏斜不能过于严重</strong>，不同类别的数据数量不要有数个数量级的差距。</li><li>而且还要对<strong>数据的量级有一个评估</strong>，多少个样本，多少个特征，可以估算出其<strong>对内存的消耗程度</strong>，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。</li></ul><h3 id="特征预处理与特征选择"><a href="#特征预处理与特征选择" class="headerlink" title="特征预处理与特征选择"></a><strong>特征预处理与特征选择</strong></h3><ul><li>良好的数据要能够提取出<strong>良好的特征才能真正发挥效力</strong>。</li><li>特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。<strong>归一化、离散化、因子化、缺失值处理、去除共线性等</strong>，数据挖掘过程中很多时间就花在它们上面。</li><li>筛选出显著特征、摒弃非显著特征，这对很多结果有决定性的影响。这需要运用<strong>特征有效性分析</strong>的相关技术，如<strong>相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重</strong>等方法。</li></ul><h3 id="4-训练模型与调优"><a href="#4-训练模型与调优" class="headerlink" title="4 训练模型与调优"></a>4 <strong>训练模型与调优</strong></h3><p>算法训练，现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。</p><h3 id="5-模型诊断"><a href="#5-模型诊断" class="headerlink" title="5 模型诊断"></a>5 <strong>模型诊断</strong></h3><p>这就需要对模型进行诊断的技术。</p><ul><li><strong>过拟合、欠拟合</strong> 判断是模型诊断中至关重要的一步。常见的方法如<strong>交叉验证，绘制学习曲线</strong>等。过拟合的基本调优思路是<strong>增加数据量，降低模型复杂度</strong>。欠拟合的基本调优思路是<strong>提高特征数量和质量，增加模型复杂度</strong>。</li><li><strong>误差分析</strong> 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因，<strong>是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题</strong>……</li><li>诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，<strong>这是一个反复迭代不断逼近的过程，需要不断地尝试，</strong> 进而达到最优状态。</li></ul><h3 id="6-模型融合"><a href="#6-模型融合" class="headerlink" title="6 模型融合"></a>6 <strong>模型融合</strong></h3><ul><li>一般来说，<strong>模型融合后都能使得效果有一定提升</strong>。</li><li><strong>工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫</strong>。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习算法分类&quot;&gt;&lt;a href=&quot;#机器学习算法分类&quot; class=&quot;headerlink&quot; title=&quot;机器学习算法分类&quot;&gt;&lt;/a&gt;机器学习算法分类&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;监督学习(有标签)      根据预测结果分成两类&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分类问题-离散值&lt;/li&gt;
&lt;li&gt;回归问题-连续值&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;无监督学习(无标签)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;半监督学习(一半有标签，一般无标签)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;强化学习&lt;/p&gt;
&lt;p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>opencv(二) imgproc 模块</title>
    <link href="http://yoursite.com/2018/05/09/opencv-%E4%BA%8C-imgproc-%E6%A8%A1%E5%9D%97/"/>
    <id>http://yoursite.com/2018/05/09/opencv-二-imgproc-模块/</id>
    <published>2018-05-09T13:37:55.000Z</published>
    <updated>2018-05-12T12:58:47.744Z</updated>
    
    <content type="html"><![CDATA[<h2 id="imgproc模块"><a href="#imgproc模块" class="headerlink" title="imgproc模块"></a>imgproc模块</h2><ul><li><p>PS：具体代码参考opencv /samples里的源码，这里不放了</p><h2 id="1-平滑图像-模糊"><a href="#1-平滑图像-模糊" class="headerlink" title="1. 平滑图像/模糊"></a>1. 平滑图像/模糊</h2><p>(1)模糊 cv :: blur                                                                            (2)高斯平滑 cv :: GaussianBlur                                                                                                                                            (3)中值滤波 cv :: medianBlur                                                                                                                                                            (4)双边滤波 cv :: bilateralFilter</p><p>​</p><h2 id="2-腐蚀和膨胀"><a href="#2-腐蚀和膨胀" class="headerlink" title="2. 腐蚀和膨胀"></a>2. 腐蚀和膨胀</h2><p>看图理解膨胀腐蚀                                                                                                                                        (1)腐蚀 cv::erode  输出像素的值是原图被掩膜所覆盖的所有像素中取<strong>最小像素</strong>                                                                                                                                            <img src="https://img.w3cschool.cn/attachments/image/20170901/1504236679969494.png" alt="腐蚀">                                                                                                                                                            (2)膨胀 cv::dilate   输出像素的值是原图被掩膜所覆盖的所有像素中取<strong>最大像素</strong>                                                                                                    <img src="https://img.w3cschool.cn/attachments/image/20170901/1504236632842236.gif" alt="膨胀"></p><p>​</p><h2 id="3-更多形态变化"><a href="#3-更多形态变化" class="headerlink" title="3. 更多形态变化"></a>3. 更多形态变化</h2><p>cv :: morphologyEx<br>开运算 、闭运算 、形态梯度、 顶帽 、黑帽</p><a id="more"></a><p>​</p><h2 id="4-Hit-or-Miss-击中击不中"><a href="#4-Hit-or-Miss-击中击不中" class="headerlink" title="4. Hit-or-Miss 击中击不中"></a>4. Hit-or-Miss 击中击不中</h2><p>cv :: morphologyEx 函数有一个参数 MORPH_HITMISS 实现击中击不中，不过是比较<strong>新的opencv版本</strong>才有</p><p>PS：知乎上的大神说就是一个完全的<strong>模板匹配过程</strong>                                                                                                                                                        原理看这篇比较简单  <a href="https://blog.csdn.net/horseinch/article/details/50127955" target="_blank" rel="noopener">https://blog.csdn.net/horseinch/article/details/50127955</a>                                                        但是结合具体找钥匙的应用，一下子就明白啥叫击中击不中，猛推这篇博文，简直一语戳醒我~~                                            <strong>击中击不中变换是形态学中用来检测特定形状所处位置的一个基本工具</strong>                                                                                                                                                                        <a href="https://blog.csdn.net/jinshengtao/article/details/20707711" target="_blank" rel="noopener">https://blog.csdn.net/jinshengtao/article/details/20707711</a></p><p>​</p><h2 id="5-使用形态学操作来提取水平和垂直线，加水印的方法"><a href="#5-使用形态学操作来提取水平和垂直线，加水印的方法" class="headerlink" title="5. 使用形态学操作来提取水平和垂直线，加水印的方法"></a>5. 使用形态学操作来提取水平和垂直线，加水印的方法</h2><p>(1) 形态学是一组图像处理操作，其基于预定义的也称为内核的结构元素来处理图像，两个最基本的形态操作是腐蚀和膨胀。                                                                                                获得水平线，根据结构元素<img src="https://img.w3cschool.cn/attachments/image/20170901/1504236903564726.png" alt="水平">先腐蚀得到水平线，再膨胀加粗</p><p>同理，获得竖直线，只不过结构元素需要改变<img src="https://img.w3cschool.cn/attachments/image/20170901/1504236942451433.png" alt="竖直">                                                        这种方法结构元素的大小和形状很关键，不然找直线还是用霍夫变换吧</p><p>​</p><p>(2) PS：这个程序里用到了 copyTo 加了个mask参数，查了一下，竟然是一种<strong>做水印的方法</strong>！！神奇~~                                                                                                                        src.copyTo(dst)     src.copyTo(dst, mask)                                                                                                            原理：<strong>src为水印图片，mask作为一个掩模板，mask在(i, j)其值为1，则把src.at(i, j)处的值直接赋给dst.at(i, j)                                        mask在(i, j)其值为0，dst.at(i, j)处保留其原始像素值</strong>                                                            (mask 可以由src 经过阈值化得到，mask为1的地方就是水印src需要加上去的地方，mask为0的地方，就是水印src，即原图dst不会被掩盖)                                                                                                                水印案例：可以参考下面这个博文~~                                                                                                                                                                                                                                <a href="http://www.cnblogs.com/xianglan/archive/2011/07/30/2122186.html" target="_blank" rel="noopener">http://www.cnblogs.com/xianglan/archive/2011/07/30/2122186.html</a></p><p>​</p><h2 id="6-图像金字塔"><a href="#6-图像金字塔" class="headerlink" title="6. 图像金字塔"></a>6. 图像金字塔</h2><p>可以看浅墨大神写的 ~ ~ <a href="https://blog.csdn.net/poem_qianmo/article/details/26157633" target="_blank" rel="noopener">https://blog.csdn.net/poem_qianmo/article/details/26157633</a></p><p>图像金字塔是图像中多尺度表达的一种，是来源于同一张原始图的图像集合。                                                        越向上层级越大，图像越小，分辨率越低，金字塔<strong>底部是高分辨率图像</strong>，而<strong>顶部是低分辨率</strong>的近似。</p><h3 id="1-图像金字塔和采样"><a href="#1-图像金字塔和采样" class="headerlink" title="(1)图像金字塔和采样"></a>(1)图像金字塔和采样</h3><ul><li>高斯金字塔: 用来向下采样，从低层向上(这句话一定要看下面这张图理解)</li><li>拉普拉斯金字塔: <strong>将拉普拉斯金字塔理解为高斯金字塔的逆形式</strong></li></ul></li></ul><ul><li><p>向上采样：pyrUp                                                                                                                                                                                        </p><ul><li>向下采样：pyrDown                                                                    </li></ul><p>PS：<strong>采样方向和金字塔方向相反</strong>(重要的事情不说三遍) 在这里卡了好久，<strong>金字塔的上下按照层级来，采样的上下按照尺寸来</strong>，直接上图吧，好理解~~</p><p><img src="http://p8ge6t5tt.bkt.clouddn.com/pyramid.png" alt=""></p><p><strong>pryUp不是PryDown的逆操作</strong>。图像首先在每个维度上扩大为原来的两倍，新增的行（偶数行）以0填充，用指定的滤波器卷积，估计“丢失”像素的近似值。</p><p>pryDown( )是一个会丢失信息的函数。为了恢复原来更高的分辨率的图像，要获得由降采样操作丢失的信息，这些数据就和拉普拉斯金字塔有关系。</p><h3 id="2-图像金字塔的作用"><a href="#2-图像金字塔的作用" class="headerlink" title="(2) 图像金字塔的作用"></a>(2) 图像金字塔的作用</h3></li><li><p>图像压缩</p></li><li><p>图像金字塔<strong>构建尺度空间</strong>，用于<strong>目标检测</strong></p><p>下面这篇博文提到了SIFT特征，没想到图像金字塔引出的尺寸空间这么有用~~</p><p><a href="https://blog.csdn.net/xiaowei_cqu/article/details/8069548" target="_blank" rel="noopener">https://blog.csdn.net/xiaowei_cqu/article/details/8069548</a></p><h3 id="3-SIFT"><a href="#3-SIFT" class="headerlink" title="(3) SIFT"></a>(3) SIFT</h3><p>SIFT特征对旋转、<strong>尺度缩放</strong>、亮度变化等保持不变性，是非常稳定的局部特征                                    下面这篇博文能够快速理解<strong>SIFT特征用于目标检测</strong>                                                        <a href="https://blog.csdn.net/haluoluo211/article/details/52767143" target="_blank" rel="noopener">https://blog.csdn.net/haluoluo211/article/details/52767143</a></p><p>​</p><h2 id="7-阈值操作"><a href="#7-阈值操作" class="headerlink" title="7. 阈值操作"></a>7. 阈值操作</h2></li><li><p>cv :: threshold</p></li><li><p>cv :: inRange</p><p>区别：两个函数都能实现二值化                                                                                                                      但是 <strong>inRange()可以同时针对多通道操作</strong>，使用起来非常方便</p><p>​</p><h2 id="8-线性滤波器"><a href="#8-线性滤波器" class="headerlink" title="8. 线性滤波器"></a>8. 线性滤波器</h2><p>cv::filter2D</p><p>​</p><h2 id="9-设置边框"><a href="#9-设置边框" class="headerlink" title="9. 设置边框"></a>9. 设置边框</h2><p>cv::copyMakeBorder</p><p>除了字面上的添加边框外，还有一个实际用处，如果<strong>评估点位于图像的边缘，如何卷积图像</strong>？                                                                                                            大多数OpenCV功能是将给定的图像复制到另一个稍大的图像上，然后用上式自动填充边界</p><p><strong>随机数生成器RNG</strong>                                                                             uniform函数可以返回指定范围的随机数，gaussian函数返回一个高斯随机数，fill则用随机数填充矩阵                <a href="https://blog.csdn.net/zyttae/article/details/41719349" target="_blank" rel="noopener">https://blog.csdn.net/zyttae/article/details/41719349</a></p><p>​</p><h2 id="10-霍夫变换"><a href="#10-霍夫变换" class="headerlink" title="10. 霍夫变换"></a>10. 霍夫变换</h2><h3 id="1-Hough-Line变换-找直线"><a href="#1-Hough-Line变换-找直线" class="headerlink" title="1.Hough Line变换 找直线"></a>1.Hough Line变换 找直线</h3><p>cv::HoughLines<br>cv::HoughLinesP</p><h3 id="2-Hough-Circle变换-找圆-圆心和半径"><a href="#2-Hough-Circle变换-找圆-圆心和半径" class="headerlink" title="2.Hough Circle变换 找圆(圆心和半径)"></a>2.Hough Circle变换 找圆(圆心和半径)</h3><p>cv::HoughCircles</p><p>​</p><h2 id="11-重映射（Remapping）"><a href="#11-重映射（Remapping）" class="headerlink" title="11.重映射（Remapping）"></a>11.重映射（Remapping）</h2><p>cv::remap</p><p>重映射，重新映射，就是从图像中的一个位置获取像素并将它们定位在新图像中的另一位置的过程。为了完成映射过程，可能需要对非整数像素位置进行一些<strong>插值</strong>，因为在源图像和目的图像之间不一定存在一对一像素的对应关系。</p><p><a href="https://blog.csdn.net/poem_qianmo/article/details/30974513" target="_blank" rel="noopener">https://blog.csdn.net/poem_qianmo/article/details/30974513</a></p><p>​</p><h2 id="12-仿射变换（Affine-Transformations）"><a href="#12-仿射变换（Affine-Transformations）" class="headerlink" title="12. 仿射变换（Affine Transformations）"></a>12. 仿射变换（Affine Transformations）</h2><p>实现仿射：                                                                                                                cv::warpAffine   可以实现平移(Translation)、缩放(Scale)、旋转(Rotation)</p><p>计算仿射矩阵的两种方法    ：                                                          cv::getRotationMatrix2D  根据三个系数<strong>center旋转中心 、angle旋转角度、 scale、缩放系数</strong>计算仿射矩阵，用                                                                                        cv::getAffineTransform  根据三个点的映射计算仿射矩阵</p><p>2 x 3的矩阵M来表示仿射变换        <img src="http://p8ge6t5tt.bkt.clouddn.com/affine.png" alt=""></p><p>再上浅墨大神的教程~~ <a href="https://blog.csdn.net/poem_qianmo/article/details/33320997" target="_blank" rel="noopener">https://blog.csdn.net/poem_qianmo/article/details/33320997</a></p><p>​</p><h2 id="13-直方图"><a href="#13-直方图" class="headerlink" title="13. 直方图"></a>13. 直方图</h2><h3 id="1-计算直方图"><a href="#1-计算直方图" class="headerlink" title="1.计算直方图"></a>1.计算直方图</h3><p>cv::split 将图像分割成对应的平面，将图像分解为R，G和B平面<br>cv::calcHist 来计算图像数组的直方图<br>cv::normalize 对数组进行归一化</p><h3 id="2-直方图均衡化"><a href="#2-直方图均衡化" class="headerlink" title="2.直方图均衡化"></a>2.直方图均衡化</h3><p>对图像中像素个数多的灰度级进行展宽，像素个数少的灰度进行压缩，从而提高了对比度和灰度色调的变化，使图像更加清晰)                                                                                                                                                cv::equalizeHis</p><h3 id="3-直方图比较"><a href="#3-直方图比较" class="headerlink" title="3.直方图比较"></a>3.直方图比较</h3><p>使用不同的指标来比较直方图，两个直方图相互匹配的程度，提供了四种比较方法                            cv::compareHist</p><h3 id="4-反向投影"><a href="#4-反向投影" class="headerlink" title="4.反向投影"></a>4.反向投影</h3><p><strong>反向投影图是指图像的某一位置上像素值用对应在直方图的所属于的bin上的值来代替该像素值</strong>，不想看文字，可以看下面这篇博文的数学描述                                                                                            <a href="https://blog.csdn.net/chenjiazhou12/article/details/22150421" target="_blank" rel="noopener">https://blog.csdn.net/chenjiazhou12/article/details/22150421</a>    </p><p>PS：开始觉得这个反向投影能有啥用啊，如果你和我一样，看看下面，反向投影在<strong>定位</strong>上可是很有用~~            反向投影用于在输入图像 (通常较大) 中<strong>查找特定的模板图像</strong> (通常较小) 最匹配的点或者区域最亮。                                    cv::calcBackProject </p><p>​</p><h2 id="15-模板匹配"><a href="#15-模板匹配" class="headerlink" title="15. 模板匹配"></a>15. 模板匹配</h2><p>通过滑动，从左到右，从上到下，在各个位置，比较模板和源图的匹配程度<br><strong>矩阵R用于存放metric</strong>，即匹配好坏程度的值 ，R中(x,y)存放对应位置的匹配度量</p><p>cv::matchTemplate() 用来搜索模板和输入图像之间的匹配<br>cv::minMaxLoc() 用来查找R矩阵，即匹配好坏程度中的最大值和最小值(以及位置)</p><p>​</p><h2 id="16-轮廓查找"><a href="#16-轮廓查找" class="headerlink" title="16. 轮廓查找"></a>16. 轮廓查找</h2><p>cv::findContours      查找轮廓<br>cv::drawContours     画轮廓</p><p>​</p><h2 id="17-凸包"><a href="#17-凸包" class="headerlink" title="17. 凸包"></a>17. 凸包</h2><p>凸包又叫凸壳，凸包能包含点集中所有的点，如果在集合A内连接任意两个点的直线段都在A的内部，则称集合A是凸形的。字面意思，就是一个多边型，没有凹的地方。</p><p><a href="https://blog.csdn.net/keith_bb/article/details/70194073" target="_blank" rel="noopener">https://blog.csdn.net/keith_bb/article/details/70194073</a></p><p>​</p><h2 id="18-为轮廓创建边界框和圆"><a href="#18-为轮廓创建边界框和圆" class="headerlink" title="18. 为轮廓创建边界框和圆"></a>18. 为轮廓创建边界框和圆</h2><p>cv::approxPolyDP  对边缘轮廓进行多边形拟合                                                                                                            对于拟合后的轮廓求包围盒</p><p><a href="http://www.cnblogs.com/mikewolf2002/p/3427079.html" target="_blank" rel="noopener">http://www.cnblogs.com/mikewolf2002/p/3427079.html</a></p><p>​</p><h2 id="19-空间矩，中心矩，归一化中心矩，Hu矩"><a href="#19-空间矩，中心矩，归一化中心矩，Hu矩" class="headerlink" title="19. 空间矩，中心矩，归一化中心矩，Hu矩"></a>19. 空间矩，中心矩，归一化中心矩，Hu矩</h2><p>PS：关于矩的概念，若不想深究，只看公式，可以看下面这篇~~<br>中心矩：<strong>平移不变性</strong><br>归一化中心矩：<strong>平移不变性，比例不变性</strong><br>Hu矩：利用二阶和三阶中心矩构造<strong>七个不变矩</strong>，具有<strong>平移、缩放、旋转不变性</strong><br><a href="https://blog.csdn.net/kuweicai/article/details/79027388" target="_blank" rel="noopener">https://blog.csdn.net/kuweicai/article/details/79027388</a></p><p>cv::moments 计算图像的中心矩<br>cv::HuMoments  由中心矩计算Hu矩<br>cv::contourArea 计算轮廓面积<br>cv::arcLength  计算封闭轮廓或曲线长度</p><p>​</p><h2 id="20-判断点在多边形内部还是外部"><a href="#20-判断点在多边形内部还是外部" class="headerlink" title="20. 判断点在多边形内部还是外部"></a>20. 判断点在多边形内部还是外部</h2><p>cv::pointPolygonTest  返回值是图像中的该点到某轮廓的最短距离，通过返回值的正负，判断该点在这个轮廓里面还是外面</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;imgproc模块&quot;&gt;&lt;a href=&quot;#imgproc模块&quot; class=&quot;headerlink&quot; title=&quot;imgproc模块&quot;&gt;&lt;/a&gt;imgproc模块&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PS：具体代码参考opencv /samples里的源码，这里不放了&lt;/p&gt;
&lt;h2 id=&quot;1-平滑图像-模糊&quot;&gt;&lt;a href=&quot;#1-平滑图像-模糊&quot; class=&quot;headerlink&quot; title=&quot;1. 平滑图像/模糊&quot;&gt;&lt;/a&gt;1. 平滑图像/模糊&lt;/h2&gt;&lt;p&gt;(1)模糊 cv :: blur                                                                            (2)高斯平滑 cv :: GaussianBlur                                                                                                                                            (3)中值滤波 cv :: medianBlur                                                                                                                                                            (4)双边滤波 cv :: bilateralFilter&lt;/p&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2 id=&quot;2-腐蚀和膨胀&quot;&gt;&lt;a href=&quot;#2-腐蚀和膨胀&quot; class=&quot;headerlink&quot; title=&quot;2. 腐蚀和膨胀&quot;&gt;&lt;/a&gt;2. 腐蚀和膨胀&lt;/h2&gt;&lt;p&gt;看图理解膨胀腐蚀                                                                                                                                        (1)腐蚀 cv::erode  输出像素的值是原图被掩膜所覆盖的所有像素中取&lt;strong&gt;最小像素&lt;/strong&gt;                                                                                                                                            &lt;img src=&quot;https://img.w3cschool.cn/attachments/image/20170901/1504236679969494.png&quot; alt=&quot;腐蚀&quot;&gt;                                                                                                                                                            (2)膨胀 cv::dilate   输出像素的值是原图被掩膜所覆盖的所有像素中取&lt;strong&gt;最大像素&lt;/strong&gt;                                                                                                    &lt;img src=&quot;https://img.w3cschool.cn/attachments/image/20170901/1504236632842236.gif&quot; alt=&quot;膨胀&quot;&gt;&lt;/p&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2 id=&quot;3-更多形态变化&quot;&gt;&lt;a href=&quot;#3-更多形态变化&quot; class=&quot;headerlink&quot; title=&quot;3. 更多形态变化&quot;&gt;&lt;/a&gt;3. 更多形态变化&lt;/h2&gt;&lt;p&gt;cv :: morphologyEx&lt;br&gt;开运算 、闭运算 、形态梯度、 顶帽 、黑帽&lt;/p&gt;
    
    </summary>
    
    
      <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
</feed>
